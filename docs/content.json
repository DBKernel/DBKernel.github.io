{"meta":{"title":"DBKernel","subtitle":"","description":"专注于数据库技术分享","author":"DBKernel","url":"http://dbkernel.github.io","root":"/"},"pages":[{"title":"分类","date":"2021-07-10T09:34:18.000Z","updated":"2021-07-10T10:49:25.987Z","comments":false,"path":"categories/index.html","permalink":"http://dbkernel.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-05-27T05:47:40.000Z","updated":"2021-07-10T13:50:19.870Z","comments":false,"path":"tags/index.html","permalink":"http://dbkernel.github.io/tags/index.html","excerpt":"","text":"-"},{"title":"关于我","date":"2021-07-10T05:47:55.000Z","updated":"2021-09-23T14:57:33.617Z","comments":false,"path":"about/index.html","permalink":"http://dbkernel.github.io/about/index.html","excerpt":"","text":"简介TODO…"},{"title":"友情链接","date":"2021-07-11T13:25:13.262Z","updated":"2021-07-11T13:25:13.213Z","comments":true,"path":"links/index.html","permalink":"http://dbkernel.github.io/links/index.html","excerpt":"","text":"微信公众号：MySQL数据库技术"},{"title":"项目","date":"2021-07-11T13:27:01.021Z","updated":"2021-07-11T13:27:00.986Z","comments":true,"path":"repository/index.html","permalink":"http://dbkernel.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"技术分享 | 如何为你的代码选择一个合适的开源协议？","slug":"how-to-choose-open-source-licence","date":"2021-08-18T16:37:15.000Z","updated":"2021-09-24T04:07:20.435Z","comments":true,"path":"2021/08/19/how-to-choose-open-source-licence/","link":"","permalink":"http://dbkernel.github.io/2021/08/19/how-to-choose-open-source-licence/","excerpt":"近期公司全面拥抱开源，在选择开源协议方面遇到了一些问题，查阅了很多资料，特此总结~~","text":"近期公司全面拥抱开源，在选择开源协议方面遇到了一些问题，查阅了很多资料，特此总结~~ 本文首发于 2021-08-19 00:37:15 前言对于很多刚踏入开源软件这个行业的小伙伴来说，在编码过程中难免会用到其他人的成果，如果你足够细心，很容易注意到即使是一小段代码，优秀的作者都在文件开头附上一段关于版权的声明，比如 Licensed under the MIT license。同时，一些博客也会标明”此文章采用 CC BY 4.0 CN 协议“。 如果我们拷贝了别人的代码或文章却没注意版权问题，在国外法律意识特别强的环境下（国内版权意识也在逐步加强），那么我们的作品会因触犯别人的权益而违法。即使是最开放的开源协议，最低要求也是保留原作者对代码的声明，所以开源不等于免费，也不等于没有约束。 何为 LICENCE？ LICENCE 是软件的授权许可，详细说明了获得代码后拥有的权利，哪些操作是允许的，哪些操作是禁止的。软件的版权许可证可有很多方式，本文仅限于讨论开源软件协议 Open Source License。 对于大多数人来说，没必要花大把时间去写许可协议，选择一种比较流行的开源协议就足够了，省时省力，更便于自己作品的传播，于人于己都有利。 PS： 说句题外话，很多国外开发者在尊重他人劳动成果方面做得很好，如果A的作品是因为B的作品的启发而来，A甚至都没有使用B任何一句代码，但A会在他的作品里面指明是受到了B的启发：Inspired by XXX link: http://www.xxxx.com。 快速选择开源协议如果你不想了解太多，只是想要一个简直直接的答案，下面给出的建议或许适合你。本小节关于协议地址来自于 GitHub choosealicence 。 简单宽松的协议： 如果你只想要一个简单点的协议不想太麻烦的话。 MIT协议相对宽松，此协议允许别人以任何方式使用你的代码同时署名原作者，但原作者不承担代码使用后的风险，当然也没有技术支持的义务。 考虑有专利的情况： 如果你的作品中涉及到专利相关。 Apache协议也是个相对宽松的协议，与MIT类似，但它指明了作者对用户专利上的一些授权（我的理解是软件作品中含有专利，但它授权你可以免费使用）。 促进代码分享： 如果你在乎作品的传播和别人的修改，希望别人也以相同的协议分享出来。 GPL（V2或V3）协议要求代码分发者或者以此代码为基础开发出来的衍生作品需要以同样的协议来发布，也必须开源，因此，该协议具有”传染性“。 乌克兰程序员Paul Bagwell，画了一张分析图，说明应该怎么选择。只用两分钟，你就能搞清楚这六种开源协议之间的最大区别。 国内大神阮一峰的汉化版本： 主流开源许可协议（Open Source License）世界上的开源许可协议（Open Source License）大概有上百种，常用的开源软件协议大致有： GPL LGPL BSD MIT Mozilla Apache 由宽松到严紧排序，常用的开源协议有： MIT BSD Apache LGPL GPL 主要区别： MIT、BSD 开源协议都源自大学，体现了简单、开放和包容的特点。 MIT、BSD、Apache 三者都支持闭源的后续开发。 GPL、LGPL 传染性开源，编译的代码里用了这里的代码，都必须开源。 MIT来源于大学，MIT 开源协议是史上最为简洁、慷慨的开源协议之一。作者只想保留版权，而无任何其他了限制。也就是说，你必须在你的发行版里包含原许可协议的声明，无论你是以二进制发布的还是以源代码发布的。 特点： 用户可以拿你的代码做任何想做的事情。 用户在项目副本中要包含版权声明和许可声明。 你无需承担任何责任。 代表作品： jQuery Rails 等。 BSD BSD-2-Clause BSD-3-Clause BSD可证也来源于大学，与MIT差不多，也非常简单、慷慨。 BSD开源协议是一个给于使用者很大自由的协议。基本上使用者可以”为所欲为”,可以自由的使用、修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。前提是当你发布使用了BSD协议的代码，或者以BSD协议代码为基础开发自己的产品时，需要满足三个条件： 如果再发布的产品中包含源代码，则在源代码中必须带有原代码中的BSD协议。 如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议。 不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。 BSD 开源协议鼓励代码共享，但需要尊重代码作者的著作权。BSD 开源协议允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布、销售，是对商业集成很友好的协议。因此，很多公司在选用开源产品的时候都首选BSD协议。 Apache Licence Apache License, Version 2.0 Apache License, Version 1.1 Apache License, Version 1.0 来自 Apache，类似 MIT 开源协议，但它重视专利权。 Apache Licence 是著名的非盈利开源组织 Apache 采用的协议。该协议和BSD类似，同样鼓励代码共享和尊重原作者的著作权，同样允许修改代码、再发布（作为开源或商业软件）。需要满足的条件也和BSD类似： 需要为使用代码的用户提供一份 Apache Licence 。 如果你修改了代码，需要在被修改的文件中说明。 在延伸的代码中（修改和由源代码衍生的代码中）需要带有原来代码中的协议、商标、专利声明和其他原作者规定需要包含的说明。 如果再发布的产品中包含一个Notice文件，则在Notice文件中需要带有 Apache Licence 。你可以在Notice中增加自己的许可，但不可对 Apache Licence 构成更改。 Apache Licence 也是对商业应用友好的许可，使用者也可以在需要的时候修改代码来满足需要并作为开源或商业产品发布/销售。 代表作品： echarts superset dubbo spark LGPLLGPL（GNU LESSER GENERAL PUBLIC LICENSE）来自于自由软件联盟GNU，可以翻译为更宽松的GPL协议，也属于传染性开源协议。 LGPL是GPL的一个主要为类库使用设计的开源协议。和GPL要求任何使用/修改/衍生之GPL类库的的软件必须采用GPL协议不同，LGPL 允许商业软件通过类库引用(link)方式使用LGPL类库而不需要开源商业软件的代码。这使得采用LGPL协议的开源代码可以被商业软件作为类库引用并发布和销售。 但是如果修改LGPL协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL协议，因此，LGPL协议的开源代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。 GPL/LGPL都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品。 GPLGPL（GNU GENERAL PUBLIC LICENSE）来源于自由软件联盟GNU，GPL/LGPL侧重于代码及衍生代码的开源与免费使用。 GPL协议的主要内容是只要在一个软件中使用（”使用”指类库引用，修改后的代码或者衍生代码）GPL 协议的产品，则该软件产品必须也采用GPL协议，既必须也是开源和免费。这就是所谓的”传染性”。 由于GPL严格要求使用了GPL类库的软件产品必须使用GPL协议，对于使用GPL协议的开源代码，商业软件或者对代码有保密要求的部门就不适合集成/采用作为类库和二次开发的基础。 我们很熟悉的Linux就是采用了GPL。GPL协议和BSD, Apache Licence等鼓励代码重用的许可很不一样。GPL的出发点是代码的开源/免费使用/引用/修改和衍生代码的开源/免费使用，但不允许修改后和衍生的代码做为闭源的商业软件发布和销售。 其它细节和BSD/Apache等协议类似。 代表作品： Linux 更多开源协议对比下方表格中出现的用词的解释： 协议和版权信息(License and copyright notice)：在代码中保留作者提供的协议和版权信息。 声明变更(State Changes)：在代码中声明对原来代码的重大修改及变更。 公开源码(Disclose Source)：代码必需公开。 库引用(Library usage)：该库可以用于商业软件中。 责任承担(Hold Liable)：代码的作者承担代码使用后的风险及产生的后果。如果禁止，那么作者将不会承担责任，可以理解为免责条款。 商标使用(Use Trademark)：可以使用作者的姓名，作品的Logo，或商标。 附加协议(Sublicensing)：允许在软件分发传播过程中附加上原来没有的协议条款等。 协议 描述 要求 允许 禁止 Apache 一个比较宽松且简明地指出了专利授权的协议。 1. 协议和版权信息2. 声明变更 1. 商用2. 分发3. 修改4. 专利授权5. 私用6. 附加协议 1. 责任承担（作者免责）2. 商标使用 GPL 应用最广泛的开源协议，拥有较强的版权自由（copyleft）要求。衍生代码的分发需开源并且也要遵守此协议。此协议有许多变种，不同变种的要求略有不同。 1. 公开源码2. 协议和版权信息3. 声明变更 1. 商用2. 分发3. 修改4. 专利授权5. 私用 1. 责任承担2. 附加协议 MIT 此协议宽松简单。在适当标明来源及免责的情况下，它允许你对代码进行任何形式的使用。 1. 协议和版权信息 1. 商用2. 分发3. 修改4. 私用5. 附加协议 1. 责任承担 Artistic Perl社区最钟爱此协议。要求更改后的软件不能影响原软件的使用。 1. 协议和版权信息2. 声明变更 1. 商用2. 分发3. 修改4. 私用5. 附加协议 1. 责任承担2. 商标使用 BSD 较为宽松的协议，有两个变种BSD 2-Clause 和BSD 3-Clause，两者都与MIT协议只存在细微差异。 1. 协议和版权信息 1. 商用2. 分发3. 修改4. 私用5. 附加协议 1. 责任承担 Eclipse 对商用非常友好的协议，可以用于软件的商业授权。包含对专利的优雅授权，也可以对相关代码应用商业协议。 1. 公开源码2. 协议和版权信息 1. 商用2. 分发3. 修改4. 专利授权5. 私用6. 附加协议 1. 责任承担 LGPL 主要用于一些代码库。衍生代码可以以此协议发布（也可以用其他协议），但与此协议相关的代码必需遵循此协议。 1. 公开源码2. 库引用3. 协议和版权信息 1. 商用2. 分发3. 修改4. 专利授权5. 私用6. 附加协议 1. 责任承担 Mozilla Mozilla Public License(MPL 2.0)是由Mozilla基金创建维护的，旨在较为宽松的BSD协议和更加互惠的GPL协议中找一个折衷点。 1. 公开源码2. 协议和版权信息 1. 商用2. 分发3. 修改4. 专利授权5. 私用6. 附加协议 1. 责任承担2. 商标使用 No license 作者保留所有权利，不允许他人分发，复制或者创造衍生物。当你将代码发表在一些网站上时需要遵守该网站的协议，此协议可能包含了一些对你劳动成果的授权许可。比如将代码发布到GitHub，那么就必须同意别人查看和fork。 1. 协议和版权信息 1. 商用2. 私用 1. 分发2. 修改3. 附加协议 Public domain dedication 在许多国家，默认版权归作者自动拥有，所以Unlicense协议提供了一种通用的模板。此协议表明作者放弃版权，将劳动成果无私贡献出来，会丧失作品全部权利，包括在MIT/X11中定义的无担保权利。 1. N/A 1. 商用2. 分发3. 修改4. 私用 1. 责任承担 参考链接 https://github.com/github/choosealicense.com https://opensource.org/licenses https://www.cnblogs.com/Wayou/p/how_to_choose_a_license.html https://zhuanlan.zhihu.com/p/87855729 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"通用","slug":"通用","permalink":"http://dbkernel.github.io/categories/%E9%80%9A%E7%94%A8/"}],"tags":[{"name":"开源协议","slug":"开源协议","permalink":"http://dbkernel.github.io/tags/%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE/"},{"name":"开源许可证","slug":"开源许可证","permalink":"http://dbkernel.github.io/tags/%E5%BC%80%E6%BA%90%E8%AE%B8%E5%8F%AF%E8%AF%81/"},{"name":"LICENCE","slug":"LICENCE","permalink":"http://dbkernel.github.io/tags/LICENCE/"},{"name":"github","slug":"github","permalink":"http://dbkernel.github.io/tags/github/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（12）神奇的物化视图(Materialized View)与原理","slug":"clickhouse-and-friends-12-materialized-view","date":"2020-09-03T13:22:14.000Z","updated":"2021-10-10T13:13:57.643Z","comments":true,"path":"2020/09/03/clickhouse-and-friends-12-materialized-view/","link":"","permalink":"http://dbkernel.github.io/2020/09/03/clickhouse-and-friends-12-materialized-view/","excerpt":"","text":"本文首发于 2020-09-03 21:22:14 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/08/31/clickhouse-and-friends-materialized-view/以下为正文。 在 ClickHouse 里，物化视图(Materialized View)可以说是一个神奇且强大的东西，用途别具一格。 本文从底层机制进行分析，看看 ClickHouse 的 Materalized View 是怎么工作的，以方便更好的使用它。 什么是物化视图对大部分人来说，物化视图这个概念会比较抽象，物化？视图？。。。 为了更好的理解它，我们先看一个场景。 假设你是 *hub 一个“幸福”的小程序员，某天产品经理有个需求：实时统计每小时视频下载量。 用户下载明细表： 123456789101112131415clickhouse&gt; SELECT * FROM download LIMIT 10;+---------------------+--------+--------+| when | userid | bytes |+---------------------+--------+--------+| 2020-08-31 18:22:06 | 19 | 530314 || 2020-08-31 18:22:06 | 19 | 872957 || 2020-08-31 18:22:06 | 19 | 107047 || 2020-08-31 18:22:07 | 19 | 214876 || 2020-08-31 18:22:07 | 19 | 820943 || 2020-08-31 18:22:07 | 19 | 693959 || 2020-08-31 18:22:08 | 19 | 882151 || 2020-08-31 18:22:08 | 19 | 644223 || 2020-08-31 18:22:08 | 19 | 199800 || 2020-08-31 18:22:09 | 19 | 511439 |... .... 计算每小时下载量： 12345678910111213141516clickhouse&gt; SELECT toStartOfHour(when) AS hour, userid, count() as downloads, sum(bytes) AS bytes FROM download GROUP BY userid, hour ORDER BY userid, hour;+---------------------+--------+-----------+------------+| hour | userid | downloads | bytes |+---------------------+--------+-----------+------------+| 2020-08-31 18:00:00 | 19 | 6822 | 3378623036 || 2020-08-31 19:00:00 | 19 | 10800 | 5424173178 || 2020-08-31 20:00:00 | 19 | 10800 | 5418656068 || 2020-08-31 21:00:00 | 19 | 10800 | 5404309443 || 2020-08-31 22:00:00 | 19 | 10800 | 5354077456 || 2020-08-31 23:00:00 | 19 | 10800 | 5390852563 || 2020-09-01 00:00:00 | 19 | 10800 | 5369839540 || 2020-09-01 01:00:00 | 19 | 10800 | 5384161012 || 2020-09-01 02:00:00 | 19 | 10800 | 5404581759 || 2020-09-01 03:00:00 | 19 | 6778 | 3399557322 |+---------------------+--------+-----------+------------+10 rows in set (0.13 sec) 很容易嘛，不过有个问题：每次都要以 download 表为基础数据进行计算，*hub 数据量太大，无法忍受。 想到一个办法：如果对 download 进行预聚合，把结果保存到一个新表 download_hour_mv，并随着 download 增量实时更新，每次去查询download_hour_mv 不就可以了。 这个新表可以看做是一个物化视图，它在 ClickHouse 是一个普通表。 创建物化视图12345678910clickhouse&gt; CREATE MATERIALIZED VIEW download_hour_mvENGINE = SummingMergeTreePARTITION BY toYYYYMM(hour) ORDER BY (userid, hour)AS SELECT toStartOfHour(when) AS hour, userid, count() as downloads, sum(bytes) AS bytesFROM download WHERE when &gt;= toDateTime(&#x27;2020-09-01 04:00:00&#x27;)GROUP BY userid, hour 这个语句主要做了： 创建一个引擎为 SummingMergeTree 的物化视图 download_hour_mv 物化视图的数据来源于 download 表，并根据 select 语句中的表达式进行相应“物化”操作 选取一个未来时间(当前时间是 2020-08-31 18:00:00)作为开始点 WHERE when &gt;= toDateTime(&#39;2020-09-01 04:00:00&#39;)，表示在2020-09-01 04:00:00 之后的数据才会被同步到 download_hour_mv 这样，目前 download_hour_mv 是一个空表： 12clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY userid, hour;Empty set (0.02 sec) 注意：官方有 POPULATE 关键字，但是不建议使用，因为视图创建期间 download 如果有写入数据会丢失，这也是我们加一个 WHERE 作为数据同步点的原因。 那么，我们如何让源表数据可以一致性的同步到 download_hour_mv 呢？ 物化全量数据在2020-09-01 04:00:00之后，我们可以通过一个带 WHERE 快照的INSERT INTO SELECT... 对 download 历史数据进行物化： 12345678clickhouse&gt; INSERT INTO download_hour_mvSELECT toStartOfHour(when) AS hour, userid, count() as downloads, sum(bytes) AS bytesFROM download WHERE when &lt; toDateTime(&#x27;2020-09-01 04:00:00&#x27;)GROUP BY userid, hour 查询物化视图： 12345678910111213141516clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY hour, userid, downloads DESC;+---------------------+--------+-----------+------------+| hour | userid | downloads | bytes |+---------------------+--------+-----------+------------+| 2020-08-31 18:00:00 | 19 | 6822 | 3378623036 || 2020-08-31 19:00:00 | 19 | 10800 | 5424173178 || 2020-08-31 20:00:00 | 19 | 10800 | 5418656068 || 2020-08-31 21:00:00 | 19 | 10800 | 5404309443 || 2020-08-31 22:00:00 | 19 | 10800 | 5354077456 || 2020-08-31 23:00:00 | 19 | 10800 | 5390852563 || 2020-09-01 00:00:00 | 19 | 10800 | 5369839540 || 2020-09-01 01:00:00 | 19 | 10800 | 5384161012 || 2020-09-01 02:00:00 | 19 | 10800 | 5404581759 || 2020-09-01 03:00:00 | 19 | 6778 | 3399557322 |+---------------------+--------+-----------+------------+10 rows in set (0.05 sec) 可以看到数据已经“物化”到 download_hour_mv。 物化增量数据写一些数据到 download表: 1234567clickhouse&gt; INSERT INTO download SELECT toDateTime(&#x27;2020-09-01 04:00:00&#x27;) + number*(1/3) as when, 19, rand() % 1000000 FROM system.numbers LIMIT 10; 查询物化视图 download_hour_mv: 1234567891011121314151617clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY hour, userid, downloads;+---------------------+--------+-----------+------------+| hour | userid | downloads | bytes |+---------------------+--------+-----------+------------+| 2020-08-31 18:00:00 | 19 | 6822 | 3378623036 || 2020-08-31 19:00:00 | 19 | 10800 | 5424173178 || 2020-08-31 20:00:00 | 19 | 10800 | 5418656068 || 2020-08-31 21:00:00 | 19 | 10800 | 5404309443 || 2020-08-31 22:00:00 | 19 | 10800 | 5354077456 || 2020-08-31 23:00:00 | 19 | 10800 | 5390852563 || 2020-09-01 00:00:00 | 19 | 10800 | 5369839540 || 2020-09-01 01:00:00 | 19 | 10800 | 5384161012 || 2020-09-01 02:00:00 | 19 | 10800 | 5404581759 || 2020-09-01 03:00:00 | 19 | 6778 | 3399557322 || 2020-09-01 04:00:00 | 19 | 10 | 5732600 |+---------------------+--------+-----------+------------+11 rows in set (0.00 sec) 可以看到最后一条数据就是我们增量的一个物化聚合，已经实时同步，这是如何做到的呢？ 物化视图原理ClickHouse 的物化视图原理并不复杂，在 download 表有新的数据写入时，如果检测到有物化视图跟它关联，会针对这批写入的数据进行物化操作。 比如上面新增数据是通过以下 SQL 生成的： 123456789101112131415161718192021clickhouse&gt; SELECT -&gt; toDateTime(&#x27;2020-09-01 04:00:00&#x27;) + number*(1/3) as when, -&gt; 19, -&gt; rand() % 1000000 -&gt; FROM system.numbers -&gt; LIMIT 10;+---------------------+------+-------------------------+| when | 19 | modulo(rand(), 1000000) |+---------------------+------+-------------------------+| 2020-09-01 04:00:00 | 19 | 870495 || 2020-09-01 04:00:00 | 19 | 322270 || 2020-09-01 04:00:00 | 19 | 983422 || 2020-09-01 04:00:01 | 19 | 759708 || 2020-09-01 04:00:01 | 19 | 975636 || 2020-09-01 04:00:01 | 19 | 365507 || 2020-09-01 04:00:02 | 19 | 865569 || 2020-09-01 04:00:02 | 19 | 975742 || 2020-09-01 04:00:02 | 19 | 85827 || 2020-09-01 04:00:03 | 19 | 992779 |+---------------------+------+-------------------------+10 rows in set (0.02 sec) 物化视图执行的语句类似： 12345678INSERT INTO download_hour_mvSELECT toStartOfHour(when) AS hour, userid, count() as downloads, sum(bytes) AS bytesFROM [新增的10条数据] WHERE when &gt;= toDateTime(&#x27;2020-09-01 04:00:00&#x27;)GROUP BY userid, hour 代码导航： 添加视图 OutputStream， InterpreterInsertQuery.cpp 1234if (table-&gt;noPushingToViews() &amp;&amp; !no_destination) out = table-&gt;write(query_ptr, metadata_snapshot, context);else out = std::make_shared&lt;PushingToViewsBlockOutputStream&gt;(table, metadata_snapshot, context, query_ptr, no_destination); 构造 Insert ， PushingToViewsBlockOutputStream.cpp 1234ASTPtr insert_query_ptr(insert.release());InterpreterInsertQuery interpreter(insert_query_ptr, *insert_context);BlockIO io = interpreter.execute();out = io.out; 物化新增数据：PushingToViewsBlockOutputStream.cpp 123456Context local_context = *select_context;local_context.addViewSource( StorageValues::create( storage-&gt;getStorageID(), metadata_snapshot-&gt;getColumns(), block, storage-&gt;getVirtuals()));select.emplace(view.query, local_context, SelectQueryOptions());in = std::make_shared&lt;MaterializingBlockInputStream&gt;(select-&gt;execute().getInputStream() 总结物化视图的用途较多。 比如可以解决表索引问题，我们可以用物化视图创建另外一种物理序，来满足某些条件下的查询问题。 还有就是通过物化视图的实时同步数据能力，我们可以做到更加灵活的表结构变更。 更强大的地方是它可以借助 MergeTree 家族引擎(SummingMergeTree、Aggregatingmergetree等)，得到一个实时的预聚合，满足快速查询。 原理是把增量的数据根据 AS SELECT ... 对其进行处理并写入到物化视图表，物化视图是一种普通表，可以直接读取和写入。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Materialized View","slug":"Materialized-View","permalink":"http://dbkernel.github.io/tags/Materialized-View/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（11）MySQL实时复制之GTID模式","slug":"clickhouse-and-friends-11-mysql-gtid-replication","date":"2020-08-28T12:40:14.000Z","updated":"2021-10-10T13:01:07.842Z","comments":true,"path":"2020/08/28/clickhouse-and-friends-11-mysql-gtid-replication/","link":"","permalink":"http://dbkernel.github.io/2020/08/28/clickhouse-and-friends-11-mysql-gtid-replication/","excerpt":"","text":"本文首发于 2020-08-28 20:40:14 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/08/26/clickhouse-and-friends-mysql-gtid-replication/以下为正文。 MySQL实时复制原理篇 几天前 ClickHouse 官方发布了 v20.8.1.4447-testing，这个版本已经包含了 MaterializeMySQL 引擎，实现了 ClickHouse 实时复制 MySQL 数据的能力，感兴趣的朋友可以通过官方安装包来做体验，安装方式参考 https://clickhouse.tech/#quick-start，需要注意的是要选择 testing 分支。 基于位点同步MaterializeMySQL 在 v20.8.1.4447-testing 版本是基于 binlog 位点模式进行同步的。 每次消费完一批 binlog event，就会记录 event 的位点信息到 .metadata 文件: 1234Version: 1Binlog File: mysql-bin.000002Binlog Position: 328Data Version: 1 这样当 ClickHouse 再次启动时，它会把 &#123;‘mysql-bin.000002’, 328&#125; 二元组通过协议告知 MySQL Server，MySQL 从这个位点开始发送数据： 123s1&gt; ClickHouse 发送 &#123;&#x27;mysql-bin.000002&#x27;, 328&#125; 位点信息给 MySQLs2&gt; MySQL 找到本地 mysql-bin.000002 文件并定位到 328 偏移位置，读取下一个 event 发送给 ClickHouses3&gt; ClickHouse 接收 binlog event 并更新 .metadata位点 看起来不错哦，但是有个问题：如果 MySQL Server 是一个集群(比如１主２从)，通过 VIP 对外服务，MaterializeMySQL 的 host 指向的是这个 vip。当集群主从发生切换后，&#123;binlog-name, binlog-position&#125; 二元组其实是不准确的，因为集群里主从 binlog 不一定是完全一致的(binlog 可以做 reset 操作)。 123s1&gt; ClickHouse 发送 &#123;&#x27;mysql-bin.000002&#x27;, 328&#125; 给集群新主 MySQLs2&gt; 新主 MySQL 发现本地没有 mysql-bin.000002 文件，因为它做过 reset master 操作，binlog 文件是 mysql-bin.000001... oops ... 为了解决这个问题，我们开发了 GTID 同步模式，废弃了不安全的位点同步模式，目前已被 upstream merged #PR13820，下一个 testing 版本即可体验。 着急的话可以自己编译或通过 ClickHouse Build Check for master-20.9.1 下载安装。 基于GTID同步GTID 是 MySQL 复制增强版，从 MySQL 5.6 版本开始支持，目前已经是 MySQL 主流复制模式。 它为每个 event 分配一个全局唯一ID和序号，我们可以不用关心 MySQL 集群主从拓扑结构，直接告知 MySQL 这个 GTID 即可，.metadata变为: 123Version: 2Executed GTID: f4aee41e-e36f-11ea-8b37-0242ac110002:1-5Data Version: 1 f4aee41e-e36f-11ea-8b37-0242ac110002 是生成 event的主机UUID，1-5是已经同步的event区间。 这样流程就变为: 123s1&gt; ClickHouse 发送 GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5 给 MySQLs2&gt; MySQL 根据 GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5 找到本地位点，读取下一个 event 发送给 ClickHouses3&gt; ClickHouse 接收 binlog event 并更新 .metadata GTID信息 MySQL开启GTID那么，MySQL 侧怎么开启 GTID 呢？增加以下两个参数即可: 1--gtid-mode=ON --enforce-gtid-consistency 比如启动一个启用 GTID 的 MySQL docker： 1docker run -d -e MYSQL_ROOT_PASSWORD=123 mysql:5.7 mysqld --datadir=/var/lib/mysql --server-id=1 --log-bin=/var/lib/mysql/mysql-bin.log --gtid-mode=ON --enforce-gtid-consistency 注意事项启用 GTID 复制模式后，metadata Version 会变为 2，也就是老版本启动时会直接报错，database 需要重建。 总结MaterializeMySQL 引擎还处于不停迭代中，对于它我们有一个初步的规划： 稳定性保证这块需要更多测试，更多试用反馈 索引优化OLTP 索引一般不是为 OLAP 设计，目前索引转换还是依赖 MySQL 表结构，需要更加智能化 可观测性在 ClickHouse 侧可以方便的查看当前同步信息，类似 MySQL show slave status 数据一致性校验需要提供方式可以校验 MySQL 和 ClickHouse 数据一致性 MaterializeMySQL 已经是社区功能，仍然有不少的工作要做。期待更多的力量加入，我们的征途不止星辰大海。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"问题定位 | Peronca Xtrabackup 8.0近日踩坑总结 - xtrabackup 2.4和8.0区别","slug":"percona-xtrabackup-2.4-vs-8.0","date":"2020-08-27T05:46:15.000Z","updated":"2021-09-24T04:11:55.890Z","comments":true,"path":"2020/08/27/percona-xtrabackup-2.4-vs-8.0/","link":"","permalink":"http://dbkernel.github.io/2020/08/27/percona-xtrabackup-2.4-vs-8.0/","excerpt":"","text":"本文首发于 2020-08-27 13:46:15 前言近期在给 radondb/xenon 适配 percona xtrabackup 8.0时，遇到了一些问题，经过多日调研、尝试终于解决，特此分享。 版本信息： 12Percona-Server 8.0.19-10Percona-Xtrabackup 8.0.13 版本各字段含义参考 https://www.percona.com/blog/2020/08/18/aligning-percona-xtrabackup-versions-with-percona-server-for-mysql/ 适配过程中遇到的坑一、MySQL 8.0 + Semi-Sync + 持续写入数据期间执行重建后，change master to &amp;&amp; start slave 报错： 1Last_Error: Could not execute Write_rows event on table db1.t1; Duplicate entry &#x27;28646&#x27; for key &#x27;t1.PRIMARY&#x27;, Error_code: 1062; handler error HA_ERR_FOUND_DUPP_KEY; the event&#x27;s master log mysql-bin.000052, end_log_pos 437 二、MySQL 8.0 + Group Replication + 持续写入数据期间执行重建后，change master to &amp;&amp; start group_replication 报错： 123452020-08-21T14:51:09.977606+08:00 61 [System] [MY-010597] [Repl] &#x27;CHANGE MASTER TO FOR CHANNEL &#x27;group_replication_applier&#x27; executed&#x27;. Previous state master_host=&#x27;&lt;NULL&gt;&#x27;, master_port= 0, master_log_file=&#x27;&#x27;, master_log_pos= 4, master_bind=&#x27;&#x27;. New state master_host=&#x27;&lt;NULL&gt;&#x27;, master_port= 0, master_log_file=&#x27;&#x27;, master_log_pos= 4, master_bind=&#x27;&#x27;.2020-08-21T14:51:09.987494+08:00 61 [ERROR] [MY-013124] [Repl] Slave SQL for channel &#x27;group_replication_applier&#x27;: Slave failed to initialize relay log info structure from the repository, Error_code: MY-0131242020-08-21T14:51:09.987542+08:00 61 [ERROR] [MY-011534] [Repl] Plugin group_replication reported: &#x27;Error while starting the group replication applier thread&#x27;2020-08-21T14:51:09.987651+08:00 7 [ERROR] [MY-011669] [Repl] Plugin group_replication reported: &#x27;Unable to initialize the Group Replication applier module.&#x27;2020-08-21T14:51:09.987831+08:00 7 [ERROR] [MY-011735] [Repl] Plugin group_replication reported: &#x27;[GCS] The member is leaving a group without being on one.&#x27; 要解释这个问题，首先要弄清楚xtrabackup 2.4和8.0的区别。 xtrabackup 2.4和8.0区别google查到xtrabackup 8.0与2.4版本行为有所不同： Xtrabackup 2.4 备份后生成的 xtrabackup_binlog_info 文件记录的 GTID 信息是准确的，但是备份恢复后 show master status 显示的 GTID 是不准确的。 Xtrabackup 8.0 在备份只有 InnoDB 表的实例时，xtrabackup_binlog_info 文件记录的 GTID 信息不一定是准确的，但是备份恢复后 show master status 显示的 GTID 是准确的。 Xtrabackup 8.0 在备份有非 InnoDB 表格的实例时，xtrabackup_binlog_info 文件记录的 GTID 信息是准确的，备份恢复后 show master status 显示的 GTID 也是准确的。 之前研究过 xtrabackup 2.4 ，其过程大致如下： start backup copy ibdata1 / copy .ibd file excuted FTWRL backup non-InnoDB tables and files writing xtrabackup_binlog_info executed FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS executed UNLOCK TABLES copying ib_buffer_pool completed OK! 问题1：xtrabackup 8.0 的执行过程是什么样？ 首先，查看重建期间的general log： 1234567891011121314151617181920212020-08-26T16:20:18.136376+08:00 170 Query SET SESSION wait_timeout=21474832020-08-26T16:20:18.136439+08:00 170 Query SET SESSION autocommit=12020-08-26T16:20:18.136523+08:00 170 Query SET NAMES utf82020-08-26T16:20:18.136595+08:00 170 Query SHOW VARIABLES2020-08-26T16:20:18.138840+08:00 170 Query SELECT COUNT(*) FROM information_schema.tables WHERE engine = &#x27;MyISAM&#x27; OR engine = &#x27;RocksDB&#x27;2020-08-26T16:20:18.140203+08:00 170 Query SHOW ENGINES2020-08-26T16:20:18.140407+08:00 170 Query SHOW ENGINE INNODB STATUS2020-08-26T16:20:18.141570+08:00 170 Query SELECT PLUGIN_NAME, PLUGIN_LIBRARY FROM information_schema.plugins WHERE PLUGIN_STATUS = &#x27;ACTIVE&#x27; AND PLUGIN_TYPE = &#x27;KEYRING&#x27;2020-08-26T16:20:18.142140+08:00 170 Query SELECT CONCAT(table_schema, &#x27;/&#x27;, table_name), engine FROM information_schema.tables WHERE engine NOT IN (&#x27;MyISAM&#x27;, &#x27;InnoDB&#x27;, &#x27;CSV&#x27;, &#x27;MRG_MYISAM&#x27;, &#x27;ROCKSDB&#x27;) AND table_schema NOT IN ( &#x27;performance_schema&#x27;, &#x27;information_schema&#x27;, &#x27;mysql&#x27;)2020-08-26T16:20:18.209819+08:00 171 Query SET SESSION wait_timeout=21474832020-08-26T16:20:18.209879+08:00 171 Query SET SESSION autocommit=12020-08-26T16:20:18.209950+08:00 171 Query SET NAMES utf82020-08-26T16:20:18.210015+08:00 171 Query SHOW VARIABLES2020-08-26T16:20:18.214030+08:00 170 Query SELECT T2.PATH, T2.NAME, T1.SPACE_TYPE FROM INFORMATION_SCHEMA.INNODB_TABLESPACES T1 JOIN INFORMATION_SCHEMA.INNODB_TABLESPACES_BRIEF T2 USING (SPACE) WHERE T1.SPACE_TYPE = &#x27;Single&#x27; &amp;&amp; T1.ROW_FORMAT != &#x27;Undo&#x27;UNION SELECT T2.PATH, SUBSTRING_INDEX(SUBSTRING_INDEX(T2.PATH, &#x27;/&#x27;, -1), &#x27;.&#x27;, 1) NAME, T1.SPACE_TYPE FROM INFORMATION_SCHEMA .INNODB_TABLESPACES T1 JOIN INFORMATION_SCHEMA .INNODB_TABLESPACES_BRIEF T2 USING (SPACE) WHERE T1.SPACE_TYPE = &#x27;General&#x27; &amp;&amp; T1.ROW_FORMAT != &#x27;Undo&#x27;2020-08-26T16:20:19.533904+08:00 170 Query FLUSH NO_WRITE_TO_BINLOG BINARY LOGS2020-08-26T16:20:19.543095+08:00 170 Query SELECT server_uuid, local, replication, storage_engines FROM performance_schema.log_status2020-08-26T16:20:19.543418+08:00 170 Query SHOW VARIABLES2020-08-26T16:20:19.545383+08:00 170 Query SHOW VARIABLES2020-08-26T16:20:19.550641+08:00 170 Query FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS2020-08-26T16:20:20.556885+08:00 170 Query SELECT UUID()2020-08-26T16:20:20.557118+08:00 170 Query SELECT VERSION() 可见，xtrabackup 8.0默认情况下大致过程如下： start backup copy .ibd file backup non-InnoDB tables and files executed FLUSH NO_WRITE_TO_BINLOG BINARY LOGS selecting LSN and binary log position from p_s.log_status copy last binlog file writing /mysql/backup/backup/binlog.index writing xtrabackup_binlog_info executing FLUSH NO_WRITE_TO_BINLOG ENGINE LOGS copy ib_buffer_pool completed OK! 注意： 当存在非InnoDB表时，xtrabackup 8.0会执行FTWRL。 从上述步骤可知，xtrabackup 8.0与2.4的步骤主要区别为： 当只存在InnoDB引擎的表时，不再执行FTWRL，而是通过 上述第5步（SELECT server_uuid, local, replication, storage_engines FROM performance_schema.log_status ）来获取LSN、binlog position、GTID 。 手册中对于表 log_status 的描述如下： The log_status table provides information that enables an online backup tool to copy the required log files without locking those resources for the duration of the copy process. When the log_status table is queried, the server blocks logging and related administrative changes for just long enough to populate the table, then releases the resources. The log_status table informs the online backup which point it should copy up to in the source’s binary log and gtid_executed record, and the relay log for each replication channel. It also provides relevant information for individual storage engines, such as the last log sequence number (LSN) and the LSN of the last checkpoint taken for the InnoDB storage engine. 从上述手册描述可知，performance_schema.log_status是MySQL 8.0提供给在线备份工具获取复制信息的表格，查询该表时，mysql server将阻止日志的记录和相关的更改来获取足够的时间以填充该表，然后释放资源。 log_status 表通知在线备份工具当前主库的 binlog 的位点和 gtid_executed 的值以及每个复制通道的 relay log。另外，它还提供了各个存储引擎的相关信息，比如，提供了 InnoDB 引擎使用的最后一个日志序列号（LSN）和最后一个检查点的 LSN。 performance_schema.log_status表定义为： 1234567891011121314151617-- Semi-Syncmysql&gt; select * from performance_schema.log_status\\G*************************** 1. row *************************** SERVER_UUID: 6b437e80-e5d5-11ea-88e3-52549922fdbb LOCAL: &#123;&quot;gtid_executed&quot;: &quot;6b437e80-e5d5-11ea-88e3-52549922fdbb:1-201094&quot;, &quot;binary_log_file&quot;: &quot;mysql-bin.000079&quot;, &quot;binary_log_position&quot;: 195&#125; REPLICATION: &#123;&quot;channels&quot;: []&#125;STORAGE_ENGINES: &#123;&quot;InnoDB&quot;: &#123;&quot;LSN&quot;: 23711425885, &quot;LSN_checkpoint&quot;: 23711425885&#125;&#125;1 row in set (0.00 sec)-- Group Replicationmysql&gt; select * from performance_schema.log_status\\G*************************** 1. row *************************** SERVER_UUID: 7bd32480-e5d5-11ea-8f8a-525499cfbb7d LOCAL: &#123;&quot;gtid_executed&quot;: &quot;aaaaaaaa-aaaa-aaaa-aaaa-53ab6ea1210a:1-11&quot;, &quot;binary_log_file&quot;: &quot;mysql-bin.000003&quot;, &quot;binary_log_position&quot;: 1274&#125; REPLICATION: &#123;&quot;channels&quot;: [&#123;&quot;channel_name&quot;: &quot;group_replication_applier&quot;, &quot;relay_log_file&quot;: &quot;mysql-relay-bin-group_replication_applier.000004&quot;, &quot;relay_log_position&quot;: 311, &quot;relay_master_log_file&quot;: &quot;&quot;, &quot;exec_master_log_position&quot;: 0&#125;, &#123;&quot;channel_name&quot;: &quot;group_replication_recovery&quot;, &quot;relay_log_file&quot;: &quot;mysql-relay-bin-group_replication_recovery.000003&quot;, &quot;relay_log_position&quot;: 151, &quot;relay_master_log_file&quot;: &quot;&quot;, &quot;exec_master_log_position&quot;: 0&#125;]&#125;STORAGE_ENGINES: &#123;&quot;InnoDB&quot;: &#123;&quot;LSN&quot;: 20257208, &quot;LSN_checkpoint&quot;: 20257208&#125;&#125;1 row in set (0.00 sec) 问题2：performance_schema.log_status提供的信息是否准确呢？ 当写入压力大时，该表中的binlog position与GTID信息不一致。 1234567891011121314mysql&gt; select * from performance_schema.log_status\\G show master status;*************************** 1. row *************************** SERVER_UUID: 6b437e80-e5d5-11ea-88e3-52549922fdbb LOCAL: &#123;&quot;gtid_executed&quot;: &quot;6b437e80-e5d5-11ea-88e3-52549922fdbb:1-448709&quot;, &quot;binary_log_file&quot;: &quot;mysql-bin.000087&quot;, &quot;binary_log_position&quot;: 341265185&#125; REPLICATION: &#123;&quot;channels&quot;: []&#125;STORAGE_ENGINES: &#123;&quot;InnoDB&quot;: &#123;&quot;LSN&quot;: 33797305275, &quot;LSN_checkpoint&quot;: 33433316246&#125;&#125;1 row in set (0.11 sec)+------------------+-----------+--------------+------------------+-----------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+-----------+--------------+------------------+-----------------------------------------------+| mysql-bin.000087 | 343317905 | | | 6b437e80-e5d5-11ea-88e3-52549922fdbb:1-448709 |+------------------+-----------+--------------+------------------+-----------------------------------------------+1 row in set (0.01 sec) 问题3：既然log_status中的binlog position不准确，为什么备份恢复后GTID并没有缺失，数据也没问题？ 原因是xtrabackup 8.0在第4步FLUSH NO_WRITE_TO_BINLOG BINARY LOGS之后，在第6步copy last binlog file，这样备份恢复出的新实例在启动后不仅会读取 gtid_executed 表，还会读取拷贝的那个binlog文件来更新GTID。 1234567891011121314151617181920$ mysqlbinlog -vv /data/mysql/mysql-bin.000096/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 4#200827 11:26:47 server id 575010000 end_log_pos 124 CRC32 0xb026e372 Start: binlog v 4, server v 8.0.19-10 created 200827 11:26:47# Warning: this binlog is either in use or was not closed properly.BINLOG &#x27;9ydHXw/Q9EUieAAAAHwAAAABAAQAOC4wLjE5LTEwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEwANAAgAAAAABAAEAAAAYAAEGggAAAAICAgCAAAACgoKKioAEjQACgFy4yaw&#x27;/*!*/;# at 124#200827 11:26:47 server id 575010000 end_log_pos 195 CRC32 0xad060415 Previous-GTIDs# 6b437e80-e5d5-11ea-88e3-52549922fdbb:1-465503SET @@SESSION.GTID_NEXT= &#x27;AUTOMATIC&#x27; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 问题定位坑一：MySQL 8.0 + Semi-Sync 重建问题xenon原有的重建逻辑是适配于MySQL 5.6、5.7的（重建过程中xenon进程存活），一直无问题： 禁用raft，将xenon状态设为LEARNER ； 如mysql进程存在，则stop mysql； 清空MySQL数据目录； 执行xtrabackup --backup以xbstream方式获取对端数据； 执行xtrabackup --prepare应用redo log； 启动mysql； 执行stop slave; reset slave all； 执行reset master，以xtrabackup_binlog_info文件中的GTID为准设置gtid_purged； 启用raft，将xenon状态设为FOLLOWER或IDLE； 等待xenon自动change master to到主节点。 执行start slave。 问题1：为什么在 MySQL 8.0 + Semi-Sync 组合下会出现 Duplicate entry ？ 跟踪重建过程中的general log，发现在第6和第7步中间，也就是设置gtid_purged之前凭空多出了 change master to 和 start slave 操作： 1234567891011121314151617181920212223242526272829303132333435363738394041422020-08-24T21:55:22.817859+08:00 8 Query SET GLOBAL rpl_semi_sync_master_enabled=OFF2020-08-24T21:55:22.818025+08:00 8 Query SET GLOBAL read_only = 12020-08-24T21:55:22.818143+08:00 8 Query SET GLOBAL super_read_only = 12020-08-24T21:55:22.818323+08:00 8 Query START SLAVE2020-08-24T21:55:22.824449+08:00 8 Query STOP SLAVE2020-08-24T21:55:22.824610+08:00 8 Query CHANGE MASTER TO MASTER_HOST = &#x27;192.168.0.3&#x27;, MASTER_USER = &#x27;qc_repl&#x27;, MASTER_PASSWORD = &lt;secret&gt;, MASTER_PORT = 3306, MASTER_AUTO_POSITION = 12020-08-24T21:55:22.833710+08:00 8 Query START SLAVE2020-08-24T21:55:22.935973+08:00 10 Query BEGIN2020-08-24T21:55:22.936084+08:00 10 Query COMMIT /* implicit, from Xid_log_event */......2020-08-24T21:55:24.701711+08:00 10 Query BEGIN2020-08-24T21:55:24.701901+08:00 10 Query COMMIT /* implicit, from Xid_log_event */2020-08-24T21:55:24.816571+08:00 8 Query SET GLOBAL rpl_semi_sync_master_enabled=OFF2020-08-24T21:55:24.816886+08:00 8 Query SET GLOBAL read_only = 12020-08-24T21:55:24.817177+08:00 8 Query SET GLOBAL super_read_only = 12020-08-24T21:55:24.817281+08:00 8 Query START SLAVE2020-08-24T21:55:25.039581+08:00 10 Query BEGIN2020-08-24T21:55:25.039749+08:00 10 Query COMMIT /* implicit, from Xid_log_event */......2020-08-24T21:55:25.152919+08:00 10 Query BEGIN2020-08-24T21:55:25.153082+08:00 10 Query COMMIT /* implicit, from Xid_log_event */2020-08-24T21:55:25.389776+08:00 8 Query STOP SLAVE2020-08-24T21:55:25.392581+08:00 8 Query RESET SLAVE ALL2020-08-24T21:55:25.407434+08:00 8 Query RESET MASTER2020-08-24T21:55:25.417292+08:00 8 Query SET GLOBAL gtid_purged=&#x27;6b437e80-e5d5-11ea-88e3-52549922fdbb:1-102610&#x27;2020-08-24T21:55:25.419835+08:00 8 Query START SLAVE2020-08-24T21:55:25.427071+08:00 8 Query SET GLOBAL read_only = 12020-08-24T21:55:25.427178+08:00 8 Query SET GLOBAL super_read_only = 12020-08-24T21:55:25.427271+08:00 8 Query SET GLOBAL sync_binlog=10002020-08-24T21:55:25.427339+08:00 8 Query SET GLOBAL innodb_flush_log_at_trx_commit=12020-08-24T21:55:25.427423+08:00 8 Query SHOW SLAVE STATUS2020-08-24T21:55:25.427600+08:00 8 Query SHOW MASTER STATUS2020-08-24T21:55:26.817622+08:00 8 Query SET GLOBAL rpl_semi_sync_master_enabled=OFF2020-08-24T21:55:26.817794+08:00 8 Query SET GLOBAL read_only = 12020-08-24T21:55:26.817897+08:00 8 Query SET GLOBAL super_read_only = 12020-08-24T21:55:26.817988+08:00 8 Query START SLAVE2020-08-24T21:55:26.818381+08:00 8 Query SHOW SLAVE STATUS2020-08-24T21:55:26.818570+08:00 8 Query SHOW MASTER STATUS2020-08-24T21:55:26.818715+08:00 8 Query STOP SLAVE2020-08-24T21:55:26.818823+08:00 8 Query CHANGE MASTER TO MASTER_HOST = &#x27;192.168.0.3&#x27;, MASTER_USER = &#x27;qc_repl&#x27;, MASTER_PASSWORD = &lt;secret&gt;, MASTER_PORT = 3306, MASTER_AUTO_POSITION = 12020-08-24T21:55:26.832164+08:00 8 Query START SLAVE 这就是说在设置gtid_purged之前已经启用复制获取了一部分数据，那么 xtrabackup_binlog_info 中的内容就不再准确，之后设置的GTID与实际数据就不一致，实际的数据比设置的GTID要多，引起主键冲突。 问题2：为什么之前MySQL 5.6、5.7从没遇到过这个问题呢？ 测试了很多次，发现在 MySQL 5.6 &amp; 5.7 在set gtid_purged 前执行 change master to &amp; start slave 后会报复制错误 Slave failed to initialize relay log info structure from the repository ，而在reset slave all; reset master、set gtid_purged后再执行 change master to &amp; start slave 就可以正常复制，数据无误。 问题3：xenon中哪块逻辑引起的额外的 change master to 和 start slave ？ 问题根源在重建期间 xenon 会设为 LEARNER 角色，而该角色在探测到MySQL Alive后，会 change master 到主节点。正常来说，要等raft状态设为 FOLLOWER 后由 FOLLOWER 的监听线程 change master 到主节点。（代码见 pr104 、pr102 ） 坑二：MySQL 8.0 + Group-Replication 重建后无法启动MGR根据报错信息Slave failed to initialize relay log info structure from the repository看，应该是xtrabackup重建后的数据目录保留了slave复制信息导致的，尝试在启动组复制前执行reset slave或reset slave all即可解决。 总结 Xtrabackup 2.4 备份后生成的 xtrabackup_binlog_info 文件记录的 GTID 信息是准确的，但是备份恢复后 show master status 显示的 GTID 是不准确的。 Xtrabackup 8.0 在备份只有 InnoDB 表的实例时，xtrabackup_binlog_info 文件记录的 GTID 信息不一定是准确的，但是备份恢复后 show master status 显示的 GTID 是准确的。 Xtrabackup 8.0 在备份有非 InnoDB 表格的实例时，xtrabackup_binlog_info 文件记录的 GTID 信息是准确的，备份恢复后 show master status 显示的 GTID 也是准确的。 使用 Xtrabackup 8.0 重建集群节点后，无需执行 reset master &amp; set gtid_purged 操作。 使用 Xtrabackup 8.0 重建 Group-Replication 集群节点后，启动组复制前需要先执行reset slave或reset slave all清除slave信息，否则 start group_replication 会失败。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"问题定位","slug":"问题定位","permalink":"http://dbkernel.github.io/tags/%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"name":"Percona","slug":"Percona","permalink":"http://dbkernel.github.io/tags/Percona/"},{"name":"Xtrabackup","slug":"Xtrabackup","permalink":"http://dbkernel.github.io/tags/Xtrabackup/"},{"name":"RadonDB","slug":"RadonDB","permalink":"http://dbkernel.github.io/tags/RadonDB/"},{"name":"Xenon","slug":"Xenon","permalink":"http://dbkernel.github.io/tags/Xenon/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（10）MergeTree Write-Ahead Log","slug":"clickhouse-and-friends-10-merge-tree-wal","date":"2020-08-20T11:55:14.000Z","updated":"2021-10-10T12:50:17.778Z","comments":true,"path":"2020/08/20/clickhouse-and-friends-10-merge-tree-wal/","link":"","permalink":"http://dbkernel.github.io/2020/08/20/clickhouse-and-friends-10-merge-tree-wal/","excerpt":"","text":"本文首发于 2020-08-20 19:55:14 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/08/18/clickhouse-and-friends-merge-tree-wal/以下为正文。 数据库系统为了提高写入性能，会把数据先写到内存，等“攒”到一定程度后再回写到磁盘，比如 MySQL 的 buffer pool 机制。 因为数据先写到内存，为了数据的安全性，我们需要一个 Write-Ahead Log (WAL) 来保证内存数据的安全性。 今天我们来看看 ClickHouse 新增的 MergeTreeWriteAheadLog 模块，它到底解决了什么问题。 高频写问题对于 ClickHouse MergeTree 引擎，每次写入(即使１条数据)都会在磁盘生成一个分区目录(part)，等着 merge 线程合并。 如果有多个客户端，每个客户端写入的数据量较少、次数较频繁的情况下，就会引发 DB::Exception: Too many parts 错误。 这样就对客户端有一定的要求，比如需要做 batch 写入。 或者，写入到 Buffer 引擎，定时的刷回 MergeTree，缺点是在宕机时可能会丢失数据。 MergeTree WAL1. 默认模式我们先看看在没有 WAL 情况下，MergeTree 是如何写入的： 每次写入 MergeTree 都会直接在磁盘上创建分区目录，并生成分区数据，这种模式其实就是 WAL + 数据的融合。 很显然，这种模式不适合频繁写操作的情况，否则会生成非常多的分区目录和文件，引发 Too many parts 错误。 2. WAL模式设置SETTINGS: min_rows_for_compact_part=2，分别执行２条写 SQL，数据会先写到 wal.bin 文件： 当满足 min_rows_for_compact_part=2 后，merger 线程触发合并操作，生成 1_1_2_1 分区，也就是完成了 wal.bin 里的 1_1_1_0 和 1_2_2_0 两个分区的合并操作。当我们执行第三条 SQL 写入: 1insert into default.mt(a,b,c) values(1,3,3) 数据块(分区)会继续追加到 wal.bin 尾部： 此时，3 条数据分布在两个地方：分区 1_1_2_1， wal.bin 里的 1_3_3_0。 这样就有一个问题：当我们执行查询的时候，数据是怎么合并的呢？ MergeTree 使用全局结构 data_parts_indexes 维护分区信息，当服务启动的时候，MergeTreeData::loadDataParts方法： data_parts_indexes.insert(1_1_2_1) 读取 wal.bin，通过 getActiveContainingPart 判断分区是否已经 merge 到磁盘：1_1_1_0 已经存在, 1_2_2_0 已经存在，data_parts_indexes.insert(1_3_3_0) data_parts_indexes:&#123;1_1_2_1,1_3_3_0&#125; 这样，它总是能维护全局的分区信息。 总结WAL 功能在 PR＃8290 实现，master 分支已经默认开启。 MergeTree 通过 WAL 来保护客户端的高频、少量写机制，减少服务端目录和文件数量，让客户端操作尽可能简单、高效。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"WAL","slug":"WAL","permalink":"http://dbkernel.github.io/tags/WAL/"},{"name":"MergeTree","slug":"MergeTree","permalink":"http://dbkernel.github.io/tags/MergeTree/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（9）MySQL实时复制与实现","slug":"clickhouse-and-friends-09-mysql-replication","date":"2020-07-28T13:50:10.000Z","updated":"2021-10-10T12:31:06.598Z","comments":true,"path":"2020/07/28/clickhouse-and-friends-09-mysql-replication/","link":"","permalink":"http://dbkernel.github.io/2020/07/28/clickhouse-and-friends-09-mysql-replication/","excerpt":"","text":"本文首发于 2020-07-28 21:50:10 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/07/25/clickhouse-and-friends-parser/以下为正文。 很多人看到标题还以为自己走错了夜场，其实没有。 ClickHouse 可以挂载为 MySQL 的一个从库 ，先全量再增量的实时同步 MySQL 数据，这个功能可以说是今年最亮眼、最刚需的功能，基于它我们可以轻松的打造一套企业级解决方案，让 OLTP 和 OLAP 的融合从此不再头疼。 目前支持 MySQL 5.6/5.7/8.0 版本，兼容 Delete/Update 语句，及大部分常用的 DDL 操作。 代码已经合并到 upstream master 分支，预计在20.8版本作为experimental 功能发布。 毕竟是两个异构生态的融合，仍然有不少的工作要做，同时也期待着社区用户的反馈，以加速迭代。 代码获取获取 clickhouse/master 代码编译即可，方法见 ClickHouse和他的朋友们（1）编译、开发、测试… MySQL Master我们需要一个开启 binlog 的 MySQL 作为 master: 1docker run -d -e MYSQL_ROOT_PASSWORD=123 mysql:5.7 mysqld --datadir=/var/lib/mysql --server-id=1 --log-bin=/var/lib/mysql/mysql-bin.log --gtid-mode=ON --enforce-gtid-consistency 创建数据库和表，并写入数据: 123456789101112mysql&gt; create database ckdb;mysql&gt; use ckdb;mysql&gt; create table t1(a int not null primary key, b int);mysql&gt; insert into t1 values(1,1),(2,2);mysql&gt; select * from t1;+---+------+| a | b |+---+------+| 1 | 1 || 2 | 2 |+---+------+2 rows in set (0.00 sec) ClickHouse Slave目前以 database 为单位进行复制，不同的 database 可以来自不同的 MySQL master，这样就可以实现多个 MySQL 源数据同步到一个 ClickHouse 做 OLAP 分析功能。 首先开启体验开关: 1clickhouse :) SET allow_experimental_database_materialize_mysql=1; 创建一个复制通道： 123456789101112131415clickhouse :) CREATE DATABASE ckdb ENGINE = MaterializeMySQL(&#x27;172.17.0.2:3306&#x27;, &#x27;ckdb&#x27;, &#x27;root&#x27;, &#x27;123&#x27;);clickhouse :) use ckdb;clickhouse :) show tables;┌─name─┐│ t1 │└──────┘clickhouse :) select * from t1;┌─a─┬─b─┐│ 1 │ 1 │└───┴───┘┌─a─┬─b─┐│ 2 │ 2 │└───┴───┘2 rows in set. Elapsed: 0.017 sec. 看下 ClickHouse 的同步位点： 12345$ cat ckdatas/metadata/ckdb/.metadataVersion: 1Binlog File: mysql-bin.000001Binlog Position: 913Data Version: 0 Delete首先在 MySQL Master 上执行一个删除操作： 12mysql&gt; delete from t1 where a=1;Query OK, 1 row affected (0.01 sec) 然后在 ClickHouse Slave 侧查看记录： 12345678910clickhouse :) select * from t1;SELECT *FROM t1┌─a─┬─b─┐│ 2 │ 2 │└───┴───┘1 rows in set. Elapsed: 0.032 sec. 此时的 metadata 里 Data Version 已经递增到 2: 12345cat ckdatas/metadata/ckdb/.metadataVersion: 1Binlog File: mysql-bin.000001Binlog Position: 1171Data Version: 2 UpdateMySQL Master: 1234567891011121314151617mysql&gt; select * from t1;+---+------+| a | b |+---+------+| 2 | 2 |+---+------+1 row in set (0.00 sec)mysql&gt; update t1 set b=b+1;mysql&gt; select * from t1;+---+------+| a | b |+---+------+| 2 | 3 |+---+------+1 row in set (0.00 sec) ClickHouse Slave: 12345678910clickhouse :) select * from t1;SELECT *FROM t1┌─a─┬─b─┐│ 2 │ 3 │└───┴───┘1 rows in set. Elapsed: 0.023 sec. 性能测试测试环境123MySQL 8C16G 云主机, 192.168.0.3，基础数据 10188183 条记录ClickHouse 8C16G 云主机, 192.168.0.4benchyou 8C8G 云主机, 192.168.0.5, 256并发写, https://github.com/xelabs/benchyou 性能测试跟硬件环境有较大关系，这里使用的是云主机模式，数据供参考。 全量性能123456789101112131415161718192021222324252627282930313233343536373839408c16G-vm :) create database sbtest engine=MaterializeMySQL(&#x27;192.168.0.3:3306&#x27;, &#x27;sbtest&#x27;, &#x27;test&#x27;, &#x27;123&#x27;);8c16G-vm :) watch lv1;WATCH lv1┌─count()─┬───────────────now()─┬─_version─┐│ 0 │ 2020-07-29 06:36:04 │ 1 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 1113585 │ 2020-07-29 06:36:05 │ 2 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 2227170 │ 2020-07-29 06:36:07 │ 3 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 3340755 │ 2020-07-29 06:36:10 │ 4 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 4454340 │ 2020-07-29 06:36:13 │ 5 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 5567925 │ 2020-07-29 06:36:16 │ 6 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 6681510 │ 2020-07-29 06:36:18 │ 7 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 7795095 │ 2020-07-29 06:36:22 │ 8 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 8908680 │ 2020-07-29 06:36:25 │ 9 │└─────────┴─────────────────────┴──────────┘┌──count()─┬───────────────now()─┬─_version─┐│ 10022265 │ 2020-07-29 06:36:28 │ 10 │└──────────┴─────────────────────┴──────────┘┌──count()─┬───────────────now()─┬─_version─┐│ 10188183 │ 2020-07-29 06:36:28 │ 11 │└──────────┴─────────────────────┴──────────┘← Progress: 11.00 rows, 220.00 B (0.16 rows/s., 3.17 B/s.) 在这个硬件环境下，全量同步性能大概是 424507/s，42w 事务每秒。 因为全量的数据之间没有依赖关系，可以进一步优化成并行，加速同步。 全量的性能直接决定 ClickHouse slave 坏掉后重建的速度，如果你的 MySQL 有 10 亿条数据，大概 40 分钟就可以重建完成。 增量性能(实时同步)在当前配置下，ClickHouse slave 单线程回放消费能力大于 MySQL master 256 并发下生产能力，通过测试可以看到它们保持实时同步。 benchyou 压测数据，2.1w 事务/秒(MySQL 在当前环境下TPS上不去): 12345678910111213141516./bin/benchyou --mysql-host=192.168.0.3 --mysql-user=test --mysql-password=123 --oltp-tables-count=1 --write-threads=256 --read-threads=0time thds tps wtps rtps[13s] [r:0,w:256,u:0,d:0] 19962 19962 0time thds tps wtps rtps[14s] [r:0,w:256,u:0,d:0] 20415 20415 0time thds tps wtps rtps[15s] [r:0,w:256,u:0,d:0] 21131 21131 0time thds tps wtps rtps[16s] [r:0,w:256,u:0,d:0] 21606 21606 0time thds tps wtps rtps[17s] [r:0,w:256,u:0,d:0] 22505 22505 0 ClickHouse 侧单线程回放能力，2.1w 事务/秒，实时同步： 123456789101112131415161718192021222324252627282930313233┌─count()─┬───────────────now()─┬─_version─┐│ 150732 │ 2020-07-30 05:17:15 │ 17 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 155477 │ 2020-07-30 05:17:16 │ 18 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 160222 │ 2020-07-30 05:17:16 │ 19 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 164967 │ 2020-07-30 05:17:16 │ 20 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 169712 │ 2020-07-30 05:17:16 │ 21 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 174457 │ 2020-07-30 05:17:16 │ 22 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 179202 │ 2020-07-30 05:17:17 │ 23 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 183947 │ 2020-07-30 05:17:17 │ 24 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 188692 │ 2020-07-30 05:17:17 │ 25 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 193437 │ 2020-07-30 05:17:17 │ 26 │└─────────┴─────────────────────┴──────────┘┌─count()─┬───────────────now()─┬─_version─┐│ 198182 │ 2020-07-30 05:17:17 │ 27 │└─────────┴─────────────────────┴──────────┘ 实现机制在探讨机制之前，首先需要了解下 MySQL 的 binlog event ，主要有以下几种类型： 12341. MYSQL_QUERY_EVENT -- DDL2. MYSQL_WRITE_ROWS_EVENT -- insert数据3. MYSQL_UPDATE_ROWS_EVENT -- update数据4. MYSQL_DELETE_ROWS_EVENT -- delete数据 当一个事务提交后，MySQL 会把执行的 SQL 处理成相应的 binlog event，并持久化到 binlog 文件。 binlog 是 MySQL 对外输出的重要途径，只要你实现 MySQL Replication Protocol，就可以流式的消费MySQL 生产的 binlog event，具体协议见 Replication Protocol。 由于历史原因，协议繁琐而诡异，这不是本文重点。 对于 ClickHouse 消费 MySQL binlog 来说，主要有以下３个难点： DDL 兼容 Delete/Update 支持 Query 过滤 DDLDDL 兼容花费了大量的代码去实现。 首先，我们看看 MySQL 的表复制到 ClickHouse 后会变成什么样子。 MySQL master: 12345678mysql&gt; show create table t1\\G;*************************** 1. row *************************** Table: t1Create Table: CREATE TABLE `t1` ( `a` int(11) NOT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB DEFAULT CHARSET=latin1 ClickHouse slave: 1234567891011ATTACH TABLE t1( `a` Int32, `b` Nullable(Int32), `_sign` Int8, `_version` UInt64)ENGINE = ReplacingMergeTree(_version)PARTITION BY intDiv(a, 4294967)ORDER BY tuple(a)SETTINGS index_granularity = 8192 可以看到： 默认增加了 2 个隐藏字段：_sign(-1删除, 1写入) 和 _version(数据版本) 引擎转换成了 ReplacingMergeTree，以 _version 作为 column version 原主键字段 a 作为排序和分区键 这只是一个表的复制，其他还有非常多的DDL处理，比如增加列、索引等，感兴趣可以观摩 Parsers/MySQL 下代码。 Update和Delete当我们在 MySQL master 执行： 12mysql&gt; delete from t1 where a=1;mysql&gt; update t1 set b=b+1; ClickHouse t1数据（把 _sign 和 _version 一并查询）： 12345678910111213141516171819clickhouse :) select a,b,_sign, _version from t1;SELECT a, b, _sign, _versionFROM t1┌─a─┬─b─┬─_sign─┬─_version─┐│ 1 │ 1 │ 1 │ 1 ││ 2 │ 2 │ 1 │ 1 │└───┴───┴───────┴──────────┘┌─a─┬─b─┬─_sign─┬─_version─┐│ 1 │ 1 │ -1 │ 2 │└───┴───┴───────┴──────────┘┌─a─┬─b─┬─_sign─┬─_version─┐│ 2 │ 3 │ 1 │ 3 │└───┴───┴───────┴──────────┘ 根据返回结果，可以看到是由 3 个 part 组成。 part1 由 mysql&gt; insert into t1 values(1,1),(2,2) 生成： 1234┌─a─┬─b─┬─_sign─┬─_version─┐│ 1 │ 1 │ 1 │ 1 ││ 2 │ 2 │ 1 │ 1 │└───┴───┴───────┴──────────┘ part2 由 mysql&gt; delete from t1 where a=1 生成： 12345┌─a─┬─b─┬─_sign─┬─_version─┐│ 1 │ 1 │ -1 │ 2 │└───┴───┴───────┴──────────┘说明：_sign = -1表明处于删除状态 part3 由 update t1 set b=b+1 生成： 123┌─a─┬─b─┬─_sign─┬─_version─┐│ 2 │ 3 │ 1 │ 3 │└───┴───┴───────┴──────────┘ 使用 final 查询： 123456789101112131415161718clickhouse :) select a,b,_sign,_version from t1 final;SELECT a, b, _sign, _versionFROM t1FINAL┌─a─┬─b─┬─_sign─┬─_version─┐│ 1 │ 1 │ -1 │ 2 │└───┴───┴───────┴──────────┘┌─a─┬─b─┬─_sign─┬─_version─┐│ 2 │ 3 │ 1 │ 3 │└───┴───┴───────┴──────────┘2 rows in set. Elapsed: 0.016 sec. 可以看到 ReplacingMergeTree 已经根据 _version 和 OrderBy 对记录进行去重。 QueryMySQL master: 1234567mysql&gt; select * from t1;+---+------+| a | b |+---+------+| 2 | 3 |+---+------+1 row in set (0.00 sec) ClickHouse slave: 12345678910111213141516171819202122clickhouse :) select * from t1;SELECT *FROM t1┌─a─┬─b─┐│ 2 │ 3 │└───┴───┘clickhouse :) select *,_sign,_version from t1;SELECT *, _sign, _versionFROM t1┌─a─┬─b─┬─_sign─┬─_version─┐│ 1 │ 1 │ -1 │ 2 ││ 2 │ 3 │ 1 │ 3 │└───┴───┴───────┴──────────┘说明：这里还有一条删除记录，_sign为-1 MaterializeMySQL 被定义成一种存储引擎，所以在读取的时候，会根据 _sign 状态进行判断，如果是-1则是已经删除，进行过滤。 并行回放为什么 MySQL 需要并行回放？ 假设 MySQL master 有 1024 个并发同时写入、更新数据，瞬间产生大量的 binlog event ，MySQL slave 上只有一个线程一个 event 接着一个 event 式回放，于是 MySQL 实现了并行回放功能！ 那么，MySQL slave 回放时能否完全(或接近)模拟出 master 当时的 1024 并发行为呢？ 要想并行首先要解决的就是依赖问题：我们需要 master 标记出哪些 event 可以并行，哪些 event 有先后关系，因为它是第一现场。 MySQL 通过在 binlog 里增加: last_committed，相同则可以并行 sequece_number，较小先执行，描述先后依赖 1234last_committed=3 sequece_number=4 -- event1last_committed=4 sequece_number=5 -- event2last_committed=4 sequece_number=6 -- event3last_committed=5 sequece_number=7 -- event4 event2 和 event3 则可以并行，event4 需要等待前面 event 完成才可以回放。 以上只是一个大体原理，目前 MySQL 有３种并行模式可以选择： 基于 database 并行 基于 group commit 并行 基于主键不冲突的 write set 并行 最大程度上让 MySQL slave加速回放，整套机制还是异常复杂的。 回到 ClickHouse slave 问题，我们采用的单线程回放，延迟已经不是主要问题，这是由它们的机制决定的： MySQL slave 回放时，需要把 binlog event 转换成 SQL，然后模拟 master 的写入，这种逻辑复制是导致性能低下的最重要原因。 而 ClickHouse 在回放上，直接把 binlog event 转换成 底层 block 结构，然后直接写入底层的存储引擎，接近于物理复制，可以理解为把 binlog event 直接回放到 InnoDB 的 page。 读取最新虽然 ClickHouse slave 回放非常快，接近于实时，如何在ClickHouse slave上总是读取到最新的数据呢？ 其实非常简单，借助 MySQL binlog GTID 特性，每次读的时候，我们跟 ｍaster 做一次 executed_gtid 同步，然后等待这些 executed_gtid 回放完毕即可。 数据一致性对一致性要求较高的场景，我们怎么验证 MySQL master 的数据和 ClickHouse slave 的数据一致性呢？ 这块初步想法是提供一个兼容 MySQL checksum 算法的函数，我们只需对比两边的 checksum 值即可。 总结ClickHouse 实时复制同步 MySQL 数据是 upstream 2020 的一个 roadmap，在整体构架上比较有挑战一直无人接单，挑战主要来自两方面： 对 MySQL 复制通道与协议非常熟悉 对 ClickHouse 整体机制非常熟悉 这样，在两个本来有点遥远的山头中间架起了一座高速，这条 10851号 高速由 zhang1024(ClickHouse侧) 和 BohuTANG(MySQL复制) 两个修路工联合承建，目前已经合并到 upstream 分支。 关于同步 MySQL 的数据，目前大家的方案基本都是在中间安置一个 binlog 消费工具，这个工具对 event 进行解析，然后再转换成 ClickHouse 的 SQL 语句，写到 ClickHouse server，链路较长，性能损耗较大。 10851号 高速是在 ClickHouse 内部实现一套 binlog 消费方案，然后根据 event 解析成 ClickHouse 内部的 block 结构，再直接回写到底层存储引擎，几乎是最高效的一种实现方式，实现与 MySQL 实时同步的能力，让分析更接近现实。 基于 database 级的复制，实现了多源复制的功能，如果复制通道坏掉，我们只需在 ClickHouse 侧删掉 database 再重建一次即可，非常快速、方便，OLTP+OLAP 就是这么简单！ 要想富，先修路！ 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"源码分析 | ClickHouse和他的朋友们 (８) 纯手工打造的SQL解析器","slug":"clickhouse-and-friends-08-parser","date":"2020-07-26T13:55:10.000Z","updated":"2021-10-10T12:11:17.257Z","comments":true,"path":"2020/07/26/clickhouse-and-friends-08-parser/","link":"","permalink":"http://dbkernel.github.io/2020/07/26/clickhouse-and-friends-08-parser/","excerpt":"","text":"本文首发于 2020-07-26 21:55:10 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/07/25/clickhouse-and-friends-parser/以下为正文。 现实生活中的物品一旦被标记为“纯手工打造”，给人的第一感觉就是“上乘之品”，一个字“贵”，比如北京老布鞋。 但是在计算机世界里，如果有人告诉你 ClickHouse 的 SQL 解析器是纯手工打造的，是不是很惊讶！ 这个问题引起了不少网友的关注，所以本篇聊聊 ClickHouse 的纯手工解析器，看看它们的底层工作机制及优缺点。 枯燥先从一个 SQL 开始： 1EXPLAIN SELECT a,b FROM t1 token首先对 SQL 里的字符逐个做判断，然后根据其关联性做 token 分割： 比如连续的 WordChar，那它就是 BareWord，解析函数在 Lexer::nextTokenImpl()，解析调用栈： 123456789101112DB::Lexer::nextTokenImpl() Lexer.cpp:63DB::Lexer::nextToken() Lexer.cpp:52DB::Tokens::operator[](unsigned long) TokenIterator.h:36DB::TokenIterator::get() TokenIterator.h:62DB::TokenIterator::operator-&gt;() TokenIterator.h:64DB::tryParseQuery(DB::IParser&amp;, char const*&amp;, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;&amp;, bool, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, bool, unsigned long, unsigned long) parseQuery.cpp:224DB::parseQueryAndMovePosition(DB::IParser&amp;, char const*&amp;, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, bool, unsigned long, unsigned long) parseQuery.cpp:314DB::parseQuery(DB::IParser&amp;, char const*, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, unsigned long, unsigned long) parseQuery.cpp:332DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:272DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:731DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313DB::MySQLHandler::run() MySQLHandler.cpp:150 asttoken 是最基础的元组，他们之间没有任何关联，只是一堆生冷的词组与符号，所以我们还需对其进行语法解析，让这些 token 之间建立一定的关系，达到一个可描述的活力。 ClickHouse 在解每一个 token 的时候，会根据当前的 token 进行状态空间进行预判（parse 返回 true 则进入子状态空间继续），然后决定状态跳转，比如： 1EXPLAIN -- TokenType::BareWord 逻辑首先会进入Parsers/ParserQuery.cpp 的 ParserQuery::parseImpl 方法： 12345678910111213bool res = query_with_output_p.parse(pos, node, expected) || insert_p.parse(pos, node, expected) || use_p.parse(pos, node, expected) || set_role_p.parse(pos, node, expected) || set_p.parse(pos, node, expected) || system_p.parse(pos, node, expected) || create_user_p.parse(pos, node, expected) || create_role_p.parse(pos, node, expected) || create_quota_p.parse(pos, node, expected) || create_row_policy_p.parse(pos, node, expected) || create_settings_profile_p.parse(pos, node, expected) || drop_access_entity_p.parse(pos, node, expected) || grant_p.parse(pos, node, expected); 这里会对所有 query 类型进行 parse 方法的调用，直到有分支返回 true。 我们来看第一层 query_with_output_p.parse Parsers/ParserQueryWithOutput.cpp： 1234567891011121314151617181920bool parsed = explain_p.parse(pos, query, expected) || select_p.parse(pos, query, expected) || show_create_access_entity_p.parse(pos, query, expected) || show_tables_p.parse(pos, query, expected) || table_p.parse(pos, query, expected) || describe_table_p.parse(pos, query, expected) || show_processlist_p.parse(pos, query, expected) || create_p.parse(pos, query, expected) || alter_p.parse(pos, query, expected) || rename_p.parse(pos, query, expected) || drop_p.parse(pos, query, expected) || check_p.parse(pos, query, expected) || kill_query_p.parse(pos, query, expected) || optimize_p.parse(pos, query, expected) || watch_p.parse(pos, query, expected) || show_access_p.parse(pos, query, expected) || show_access_entities_p.parse(pos, query, expected) || show_grants_p.parse(pos, query, expected) || show_privileges_p.parse(pos, query, expected 跳进第二层 explain_p.parse ParserExplainQuery::parseImpl状态空间： 12345678910111213141516171819202122232425bool ParserExplainQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)&#123; ASTExplainQuery::ExplainKind kind; bool old_syntax = false; ParserKeyword s_ast(&quot;AST&quot;); ParserKeyword s_analyze(&quot;ANALYZE&quot;); ParserKeyword s_explain(&quot;EXPLAIN&quot;); ParserKeyword s_syntax(&quot;SYNTAX&quot;); ParserKeyword s_pipeline(&quot;PIPELINE&quot;); ParserKeyword s_plan(&quot;PLAN&quot;); ... ... else if (s_explain.ignore(pos, expected)) &#123; ... ... &#125; ... ... ParserSelectWithUnionQuery select_p; ASTPtr query; if (!select_p.parse(pos, query, expected)) return false; ... ... s_explain.ignore 方法会进行一个 keyword 解析，解析出 ast node: 1EXPLAIN -- keyword 跃进第三层 select_p.parse ParserSelectWithUnionQuery::parseImpl状态空间： 12345678bool ParserSelectWithUnionQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)&#123; ASTPtr list_node; ParserList parser(std::make_unique&lt;ParserUnionQueryElement&gt;(), std::make_unique&lt;ParserKeyword&gt;(&quot;UNION ALL&quot;), false); if (!parser.parse(pos, list_node, expected)) return false;... parser.parse 里又调用第四层 ParserSelectQuery::parseImpl 状态空间： 12345678910111213141516171819202122232425262728293031bool ParserSelectQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)&#123; auto select_query = std::make_shared&lt;ASTSelectQuery&gt;(); node = select_query; ParserKeyword s_select(&quot;SELECT&quot;); ParserKeyword s_distinct(&quot;DISTINCT&quot;); ParserKeyword s_from(&quot;FROM&quot;); ParserKeyword s_prewhere(&quot;PREWHERE&quot;); ParserKeyword s_where(&quot;WHERE&quot;); ParserKeyword s_group_by(&quot;GROUP BY&quot;); ParserKeyword s_with(&quot;WITH&quot;); ParserKeyword s_totals(&quot;TOTALS&quot;); ParserKeyword s_having(&quot;HAVING&quot;); ParserKeyword s_order_by(&quot;ORDER BY&quot;); ParserKeyword s_limit(&quot;LIMIT&quot;); ParserKeyword s_settings(&quot;SETTINGS&quot;); ParserKeyword s_by(&quot;BY&quot;); ParserKeyword s_rollup(&quot;ROLLUP&quot;); ParserKeyword s_cube(&quot;CUBE&quot;); ParserKeyword s_top(&quot;TOP&quot;); ParserKeyword s_with_ties(&quot;WITH TIES&quot;); ParserKeyword s_offset(&quot;OFFSET&quot;); ParserNotEmptyExpressionList exp_list(false); ParserNotEmptyExpressionList exp_list_for_with_clause(false); ParserNotEmptyExpressionList exp_list_for_select_clause(true); ... if (!exp_list_for_select_clause.parse(pos, select_expression_list, expected)) return false; 第五层 exp_list_for_select_clause.parse ParserExpressionList::parseImpl状态空间继续： 1234567bool ParserExpressionList::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)&#123; return ParserList( std::make_unique&lt;ParserExpressionWithOptionalAlias&gt;(allow_alias_without_as_keyword), std::make_unique&lt;ParserToken&gt;(TokenType::Comma)) .parse(pos, node, expected);&#125; … … 写不下去个鸟！ 可以发现，ast parser 的时候，预先构造好状态空间，比如 select 的状态空间: expression list from tables where group by with … order by limit 在一个状态空间內，还可以根据 parse 返回的 bool 判断是否继续进入子状态空间，一直递归解析出整个 ast。 总结手工 parser 的好处是代码清晰简洁，每个细节可防可控，以及友好的错误处理，改动起来不会一发动全身。 缺点是手工成本太高，需要大量的测试来保证其正确性，还需要一些fuzz来保证可靠性。 好在ClickHouse 已经实现的比较全面，即使有新的需求，在现有基础上修修补补即可。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Parser","slug":"Parser","permalink":"http://dbkernel.github.io/tags/Parser/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（6）MergeTree存储结构","slug":"clickhouse-and-friends-06-merge-tree-disk-layout","date":"2020-06-30T13:41:12.000Z","updated":"2021-10-10T11:02:02.185Z","comments":true,"path":"2020/06/30/clickhouse-and-friends-06-merge-tree-disk-layout/","link":"","permalink":"http://dbkernel.github.io/2020/06/30/clickhouse-and-friends-06-merge-tree-disk-layout/","excerpt":"","text":"本文首发于 2020-06-30 21:41:12 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/06/26/clickhouse-and-friends-merge-tree-disk-layout/以下为正文。 上篇的 存储引擎技术进化与MergeTree 介绍了存储算法的演进。 存储引擎是一个数据库的底盘，一定要稳和动力澎湃。 接下来我们将一起来探索下 ClickHouse MergeTree 列式存储引擎，解构下这台“跑车”最重要的部件。 所有的存储引擎，无论精良与粗制滥造，最终都是要把数据回写到磁盘，来满足存储和索引目的。 磁盘文件的构造可以说是算法的物理体现，我们甚至可以通过这些存储结构反推出其算法实现。 所以，要想深入了解一个存储引擎，最好的入手点是它的磁盘存储结构，然后再反观它的读、写机制就会有一种水到渠成的感觉。 如果这个分析顺序搞反了，会有一种生硬的感觉，网上大部分教程都是这种“生硬”式教学，本文将直击灵魂从最底层谈起，彻底搞明白４个问题： MergeTree 有哪些文件？ MergeTree 数据如何分布？ MergeTree 索引如何组织？ MergeTree 如何利用索引加速？ 话不多说，上表： 1234567891011CREATE TABLE default.mt( `a` Int32, `b` Int32, `c` Int32, INDEX `idx_c` (c) TYPE minmax GRANULARITY 1)ENGINE = MergeTreePARTITION BY aORDER BY bSETTINGS index_granularity=3 造点数据： 123insert into default.mt(a,b,c) values(1,1,1);insert into default.mt(a,b,c) values(5,2,2),(5,3,3);insert into default.mt(a,b,c) values(3,10,4),(3,9,5),(3,8,6),(3,7,7),(3,6,8),(3,5,9),(3,4,10); 磁盘文件12ls ckdatas/data/default/mt/1_4_4_0 3_6_6_0 5_5_5_0 detached format_version.txt 可以看到，生成了 3 个数据目录，每个目录在 ClickHouse 里称作一个分区(part)，目录名的前缀正是我们写入时字段 a 的值: 1,3,5，因为表分区是这样定位的：PARTITION BY a。 现在我们看看 a=3 分区： 12ls ckdatas/data/default/mt/3_6_6_0/a.bin a.mrk2 b.bin b.mrk2 c.bin checksums.txt c.mrk2 columns.txt count.txt minmax_a.idx partition.dat primary.idx skp_idx_idx_c.idx skp_idx_idx_c.mrk2 *.bin 是列数据文件，按主键排序(ORDER BY)，这里是按照字段 b 进行排序 *.mrk2 mark 文件，目的是快速定位 bin 文件数据位置 minmax_a.idx 分区键 min-max 索引文件，目的是加速分区键 a 查找 primay.idx 主键索引文件，目的是加速主键 b 查找 skp_idx_idx_c.* 字段 c 索引文件，目的是加速 c 的查找 在磁盘上，MergeTree 只有一种物理排序，就是 ORDER BY 的主键序，其他文件(比如 .mrk/.idx)是一种逻辑加速，围绕仅有的一份物理排序，要解决的问题是： 在以字段 b 物理排序上，如何实现字段 a、字段 c 的快速查找？ MergeTree 引擎概括起来很简单：整个数据集通过分区字段被划分为多个物理分区，每个分区內又通过逻辑文件围绕仅有的一种物理排序进行加速查找。 存储结构数据文件对于单个物理分区內的存储结构，首先要明确一点，MergeTree 的数据只有一份：*.bin。 a.bin 是字段 a 的数据，b.bin 是字段 b 的数据，c.bin 是字段 c 的数据，也就是大家熟悉的列存储。 各个 bin 文件以 b.bin排序对齐（b 是排序键），如图： 这样会有一个比较严重的问题：如果 *.bin 文件较大，即使读取一行数据，也要加载整个 bin 文件，浪费了大量的 IO，没法忍。 granule高、黑科技来了，ClickHouse MergeTree 把 bin 文件根据颗粒度(GRANULARITY)划分为多个颗粒(granule)，每个 granule 单独压缩存储。 SETTINGS index_granularity=3 表示每 ３ 行数据为一个 granule，分区目前只有 ７ 条数据，所以被划分成 3 个 granule(三个色块)： 为方便读取某个 granule，使用 *.mrk 文件记录每个 granule 的 offset，每个 granule 的 header 里会记录一些元信息，用于读取解析: 这样，我们就可以根据 ｍark 文件，直接定位到想要的 granule，然后对这个单独的 granule 进行读取、校验。 目前，我们还有缺少一种映射：每个 mark 与字段值之间的对应，哪些值区间落在 mark0，哪些落在 mark1 …？ 有了这个映射，就可以实现最小化读取 granule 来加速查询： 根据查询条件确定需要哪些 mark 根据 mark 读取相应的 granule 存储排序在了解 MergeTree 索引机制之前，需要明白以下两点： 只有一份全量数据，存储在 *.bin 文件 *.bin 按照 ORDER BY 字段降序存储 稀疏索引因为数据只有一份且只有一种物理排序，MergeTree在索引设计上选择了简单、高效的稀疏索引模式。 什么是稀疏索引呢？就是从已经排序的全量数据里，间隔性的选取一些点，并记录这些点属于哪个 mark。 1. primary index主键索引，可通过[PRIMARY KEY expr]指定，默认是 ORDER BY 字段值。 注意 ClickHouse primary index 跟 MySQL primary key 不是一个概念。 在稀疏点的选择上，取每个 granule 最小值： 2. skipping index普通索引。 INDEX idx_c(c) TYPE minmax GRANULARITY 1 针对字段 c 创建一个 minmax 模式索引。 GRANULARITY 是稀疏点选择上的 granule 颗粒度，GRANULARITY 1 表示每 1 个 granule 选取一个： 如果定义为GRANULARITY 2 ，则 2 个 granule 选取一个： 3. partition minmax index针对分区键，MergeTree 还会创建一个 min/max 索引，来加速分区选择。 4. 全景图 查询优化现在熟悉了 MergeTree 的存储结构，我们通过几个查询来体验下。 1. 分区键查询语句： 1select * from default.mt where a=3 查询会直接根据 a=3 定位到单个分区: 12345678910111213&lt;Debug&gt; InterpreterSelectQuery: MergeTreeWhereOptimizer: condition &quot;a = 3&quot; moved to PREWHERE&lt;Debug&gt; default.mt (SelectExecutor): Key condition: unknown&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: (column 0 in [3, 3])&lt;Debug&gt; default.mt (SelectExecutor): Selected 1 parts by a, 1 parts by key, 3 marks by primary key, 3 marks to read from 1 ranges┌─a─┬──b─┬──c─┐│ 3 │ 4 │ 10 ││ 3 │ 5 │ 9 ││ 3 │ 6 │ 8 ││ 3 │ 7 │ 7 ││ 3 │ 8 │ 6 ││ 3 │ 9 │ 5 ││ 3 │ 10 │ 4 │└───┴────┴────┘ 2. 主键索引查询语句： 1select * from default.mt where b=5 查询会先从 3 个分区读取 prmary.idx，然后定位到只有一个分区符合条件，找到要读取的 mark: 123456&lt;Debug&gt; default.mt (SelectExecutor): Key condition: (column 0 in [5, 5])&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: unknown&lt;Debug&gt; default.mt (SelectExecutor): Selected 3 parts by a, 1 parts by key, 1 marks by primary key, 1 marks to read from 1 ranges┌─a─┬─b─┬─c─┐│ 3 │ 5 │ 9 │└───┴───┴───┘ 3. 索引查询语句： 1select * from default.mt where c=5 查询会先从 3 个分区读取 prmary.idx 和 skp_idx_idx_c.idx 进行 granule 过滤（没用的 drop 掉），然后定位到只有 3_x_x_x 分区的一个 granule 符合条件: 12345678910&lt;Debug&gt; InterpreterSelectQuery: MergeTreeWhereOptimizer: condition &quot;b = 5&quot; moved to PREWHERE&lt;Debug&gt; default.mt (SelectExecutor): Key condition: unknown&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: unknown&lt;Debug&gt; default.mt (SelectExecutor): Index `idx_c` has dropped 1 / 1 granules.&lt;Debug&gt; default.mt (SelectExecutor): Index `idx_c` has dropped 1 / 1 granules.&lt;Debug&gt; default.mt (SelectExecutor): Index `idx_c` has dropped 2 / 3 granules.&lt;Debug&gt; default.mt (SelectExecutor): Selected 3 parts by a, 1 parts by key, 5 marks by primary key, 1 marks to read from 1 ranges┌─a─┬─b─┬─c─┐│ 3 │ 9 │ 5 │└───┴───┴───┘ 总结本文从磁盘存储结构入手，分析 ClickHouse MergeTree 的存储、索引设计。 只有了解了这些底层机制，我们才好对自己的 SQL 和表结构进行优化，使其执行更加高效。 ClickHouse MergeTree 设计简单、高效，它首要解决的问题是：在一种物理排序上，如何实现快速查找。 针对这个问题，ClickHouse使用稀疏索引来解决。 在官方 roadmap 上，列举了一个有意思的索引方向：Z-Order Indexing，目的是把多个维度编码到一维存储，当我们给出多维度条件的时候，可以快速定位到这个条件点集的空间位置，目前 ClickHouse 针对这个索引设计暂无进展。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"MergeTree","slug":"MergeTree","permalink":"http://dbkernel.github.io/tags/MergeTree/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（5）存储引擎技术进化与MergeTree","slug":"clickhouse-and-friends-05-merge-tree-algo","date":"2020-06-22T13:55:10.000Z","updated":"2021-09-24T04:02:24.553Z","comments":true,"path":"2020/06/22/clickhouse-and-friends-05-merge-tree-algo/","link":"","permalink":"http://dbkernel.github.io/2020/06/22/clickhouse-and-friends-05-merge-tree-algo/","excerpt":"","text":"本文首发于 2020-06-22 21:55:10 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/06/20/clickhouse-and-friends-merge-tree-algo/以下为正文。 21 世纪的第二个 10 年，虎哥已经在存储引擎一线奋战近 10 年，由于强大的兴趣驱动，这么多年来几乎不放过 arXiv 上与存储相关的每一篇 paper。 尤其是看到带有 draft 的 paper 时，有一种乞丐听到“叮当”响时的愉悦。 看paper这玩意就像鉴宝，多数是“赝品”，需要你有“鉴真”的本领，否则今天是张三的算法超越xx，明儿又是王二的硬件提升了yy，让你永远跟不上节奏zz，湮灭在这些没有营养的技术垃圾中，浪费大好青春。 言归正传，接下来的3篇，跟 ClickHouse 的 MergeTree 引擎有关： 上篇介绍存储引擎的技术演进史，从”远古”的 B-tree 出发推演到目前主流的技术架构。 中篇会从存储结构介绍 MergeTree 原理 ，对 ClickHouse MergeTree 有一个深入的认识，如何合理设计来进行科学加速。 下篇会从MergeTree代码出发，看看 ClickHouse MergeTree 如何实现读、写。 本文为上篇，先来个热身，相信本篇大部分内容对大家来说都比较陌生，很少人写过。 地位存储引擎(事务型)在一个数据库(DBMS)中的地位如何呢？ MySQL 的商业成功可以说大部分来自于 InnoDB 引擎，Oracle 收购 InnoDB 比 MySQL 早好几年呢！ 20年前，能亲手撸一套 ARIES (Algorithms for Recovery and Isolation Exploiting Semantics) 规范引擎，实力还是相当震撼的，相信 Oracle 收购的不仅是 InnoDB 这个引擎，更重要的是人， InnoDB 作者在哪里，在干什么？！ Fork 出来的 MariaDB 这么多年一直找不到自己的灵魂，在 Server 层磨磨蹭蹭可谓是江河日下，只能四处收购碰碰运气，当年 TokuDB 战斗过的 commit 依在，但这些已经是历史了。 另，WiredTiger 被 MongoDB 收购并使用，对整个生态所起的作用也是无可估量的，这些发动机引擎对于一辆汽车是非常重要的。 有人问道，都已经 2020 年了，开发一个存储引擎还这么难吗？不难，但是造出来的未必有 RocksDB 好用？！ 如大家所见，很多的分布式存储引擎都是基于 RocksDB 研发，可谓短期内还算明智的选择。 从工程角度来看，一个 ACID 引擎要打磨的东西非常之多，到处充斥着人力、钱力、耐心的消耗，一种可能是写到一半就停滞了(如 nessDB)，还有一种可能是写着写着发现跟xx很像，沃茨法克。 当然，这里并不是鼓励大家都去基于 RocksDB 去构建自己的产品，而是要根据自己的情况去做选择。 B-tree首先要尊称一声大爷，这个大爷年方 50，目前支撑着数据库产业的半壁江山。 50 年来不变而且人们还没有改变它的意向，这个大爷厉害的很！ 鉴定一个算法的优劣，有一个学派叫 IO复杂度分析，简单推演真假便知。 下面就用此法分析下 B-tree(traditional b-tree) 的 IO 复杂度，对读、写 IO 一目了然，真正明白读为什么快，写为什么慢，如何优化。 为了可以愉快的阅读，本文不会做任何公式推导，复杂度分析怎么可能没有公式呢！ 读IO分析这里有一个 3-level 的 B-tree，每个方块代表一个 page，数字代表 page ID。 上图 B-tree 结构是内存的一个表现形式，如果我们要读取的记录在 leaf-8上，read-path 如蓝色箭头所示: root-9 –&gt; branch-6 –&gt; leaf-8 下图是 B-tree 在磁盘上的存储形式，meta page 是起点: 这样读取的随机 IO (假设内存里没有 page 缓存且 page 存储是随机的)总数就是(蓝色箭头): 1(meta-10)IO + 1(root-9)IO + 1(branch-6)IO + 1(leaf-8)IO = 4次 IO，这里忽略一直缓存的 meta 和 root，就是 2 次随机 IO。如果磁盘 seek 是 1ms，读取延迟就是 2ms。 通过推演就会发现，B-tree 是一种读优化(Read-Optimized)的数据结构，无论 LSM-tree 还是 Fractal-tree 等在读上只能比它慢，因为读放大(Read Amplification)问题。 存储引擎算法可谓日新月异，但是大部分都是在跟写优化(Write-Optimized)做斗争，那怕是一个常数项的优化那就是突破，自从 Fractal-tree 突破后再无来者了！ 写IO分析现在写一条记录到 leaf-8。 可以发现，每次写都需要先读取一遍，如上图蓝色路径所示。 假设这次写入导致 root, branch 都发生了变化，这种 in-place 的更新反映到磁盘上就是： 基本是 2 次读 IO和写 2 次写 IO+WAL fsync，粗略为 4 次随机 IO。 通过分析发现，B-tree 对写操作不太友好，随机 IO 次数较多，而且 in-place 更新必须增加一个 page 级的 WAL 保证失败回滚，简直是要命。 Write-Optimized B-tree说到写优化，在机械盘的年代，大家的方向基本是把随机 IO 转换为顺序 IO，充分发挥磁盘的机械优势，于是出现一种 Append-only B-tree： 更新生成新的 page(蓝色) page 回写磁盘时 append only 到文件末尾 无需 page WAL，数据不 overwrite，有写放大(Write Amplification)问题，需要做空洞重利用机制 Append-only B-tree 节省了回写时的 2 次随机 IO，转换为常数级(constant)的1次顺序 IO，写性能大幅提升，总结起来就是： 随机变顺序，空间换时间 LSM-tree, Fractal-tree 等写优化算法的核心思想也是这个，只不过其实现机制不同。 LSM-trees随着 LevelDB 的问世，LSM-tree 逐渐被大家所熟知。 LSM-tree 更像一种思想，模糊了 B-tree 里 tree 的严肃性，通过文件组织成一个更加松散的 tree。 这里不谈一个具体的 LSM-tree 是 Leveled 还是 Size-tiered，只谈大体思想。 写入 先写入内存的 C0 后台线程根据规则(Leveled/Sized)进行 merge，C0 –&gt; C1, C1 –&gt; C2 … CL 写入 C0 即可返回，IO 放到后台的 Merge 过程 每次 Merge 是硬伤，动作大就抖，动作小性能不好，每次 Merge 的数据流向不明确 写放大问题 读取 读取 C0 读取 C1 .. CL 合并记录返回 读放大问题 Fractal-tree终于发展到了“终极”优化(目前最先进的索引算法)，Fractal-tree。 它是在 Append-only B-tree 的基础上，对每个 branch 节点增加了一个 message buffer 作为缓冲，可以看做是 LSM-tree 和 Append-only B-tree 完美合体。 相对于 LSM-tree 它的优势非常明显:Merge 更加有序，数据流向非常分明，消除了 Merge 的抖动问题，大家一直寻找的 compaction 防抖方案一直存在的！ 这个高科技目前只有 TokuDB 在使用，这个算法可以开篇新介，这里不做累述，感兴趣的可以参考原型实现 nessDB。 Cache-oblivious这个词对于大部分人都是陌生的，不过别怕。 在存储引擎里，有一个数据结构非常非常重要，它负责 page 数据有序性维护，比如在一个 page 里怎么快速定位到我要的记录。 在 LevelDB 里使用 skiplist，但大部分引擎使用的是一个有序数组来表示，比如 [1, 2, 3, … 100]，然后使用二分查找。 大概 10 年前一位内核开发者发表了一篇 &lt;You’re Doing It Wrong&gt;，这个小文讲了一个很有意思的事情： 数据的组织形式对性能有很大的影响，因为 CPU有 cache line。 抛开这篇文章不谈，咱们来看一张“神仙”图： 这是一个 binary-tree 的 4 种 layout 表示形式，那么哪种 layout 对 CPU cache line 最友好？ 也许你已经猜对了，那就是 van Emde Boas，简称 vEB。 因为它的相邻数据“扎堆”存储，point-query 和 range-query 的 cache line 可以最大化共享，skiplist 对 cache line 是非常不友好的，还可以更快！ 对于 cache oblivious 数据结构，这里有一个简单的原型实现: omt B-tree优化魔力象限写优化算法从原生的 B-tree 到 Append-only B-tree(代表作 LMDB)，又到 LSM-tree(LevelDB/RocksDB 等)，最后进化到目前最先进的 Fractal-tree (TokuDB)。 这些算法耗费了很多年才在工程上实现并被认可，研发一款存储引擎缺的不是算法而是“鉴宝”的能力，这个“宝”可能已经躺了几十年了。 其实，”科学家”们已经总结出一个 B-tree 优化魔力象限: 横坐标是写性能，纵坐标是读性能，B-tree 和 Logging 数据结构分布在曲线的两个极端。 B-tree 的读性能非常好，但是写性能差。 Logging 的写性能非常好，但是读性能差(想想我们每次写都把数据追加到文件末尾，是不是很快？但是读…)。 在它们中间有一个优化曲度(Optimal Curve)。 在这个曲度上，你可以通过增加/减少一个常数(1-epsilon)来做读和写优化组合，LSM-tree/Fractal-tree 都在这个曲度之上。 总结本文主要讨论事务性引擎的技术演进，其中包含了 IO 复杂度分析，其实这个分析是基于一个 DAM(Disk Access Machine) 模型，这里不再展开。这个模型要解决什么问题呢？ 如果工程中涉及硬件层级关系，比如 Disk / Memory / CPU，数据在Disk，读取(以 block 为单位)到 Memory，查找计算(cache-line)在 CPU，不同介质间性能差距又非常之大，我们怎么做才能让整体性能更优的问题。 和当今的硬件相融合，这个模型也一样适用。 最后回到 ClickHouse 的 MergeTree 引擎，它只使用了本文中的部分优化，实现也比较简洁、高效，毕竟没有事务，撸起来也没啥心理负担。 随机变顺序，空间换时间， MergeTree 原理，请听下回分解。 References [1] Cache-Oblivious Data Structures [2] Data Structures and Algorithms for Big Databases [3] The buffer tree: A new technique for optimal I/O-algorithms [4] how the append-only btree works [5] 写优化的数据结构(1):AOF和b-tree之间 [6] 写优化的数据结构(2):buffered tree [7] 存储引擎数据结构优化(1):cpu bound [8] 存储引擎数据结构优化(2):io bound [9] nessDB [10] omt 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"B-Tree","slug":"B-Tree","permalink":"http://dbkernel.github.io/tags/B-Tree/"},{"name":"LSM-Tree","slug":"LSM-Tree","permalink":"http://dbkernel.github.io/tags/LSM-Tree/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（4）Pipeline处理器和调度器","slug":"clickhouse-and-friends-04-processor","date":"2020-06-12T12:57:10.000Z","updated":"2021-09-24T04:00:28.579Z","comments":true,"path":"2020/06/12/clickhouse-and-friends-04-processor/","link":"","permalink":"http://dbkernel.github.io/2020/06/12/clickhouse-and-friends-04-processor/","excerpt":"","text":"本文首发于 2020-06-12 19:57:10 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/06/11/clickhouse-and-friends-processor/以下为正文。 最后更新: 2020-08-15 本文谈下 ClickHouse 核心科技：处理器 Processor 和有向无环调度器 DAG Scheduler。 这些概念并不是 ClickHouse 首创，感兴趣的同学可以关注下 materialize 的 timely-dataflow，虎哥用 golang 也写过一个原型。 拼的是实现细节，正是这些模块的精良设计，才有了 ClickHous e整体的高性能。 Pipeline问题在传统数据库系统中，一个 Query 处理流程大体是: 其中在 Plan 阶段，往往会增加一个 Pipeline 组装(一个 transformer 代表一次数据处理)： 所有 transformer 被编排成一个流水线(pipeline)，然后交给 executor 串行式执行，每执行一个 transformer 数据集就会被加工并输出，一直到下游的 sinker。 可以看到，这种模型的优点是简单，缺点是性能低，无法发挥 CPU 的并行能力，通常叫火山模型(volcano-style)，对于 OLTP 低延迟来说足够，对于计算密集的 OLAP 来说是远远不够的，CPU 不到 100% 就是犯罪！ 对于上面的例子，如果 transformer1 和 transformer2 没有交集，那么它们就可以并行处理： 这样就涉及到一些比较灵魂的问题： 如何实现 transformer 的灵活编排？ 如何实现 transformer 间的数据同步？ 如何实现 transformer 间的并行调度？ Processor 和 DAG Scheduler1. Transformer 编排ClickHouse 实现了一系列基础 transformer 模块，见 src/Processors/Transforms，比如: FilterTransform – WHERE 条件过滤 SortingTransform – ORDER BY 排序 LimitByTransform – LIMIT 裁剪 当我们执行: 1SELECT * FROM t1 WHERE id=1 ORDER BY time DESC LIMIT 10 对于 ClickHouse 的 QueryPipeline 来说，它会按照以下方式进行编排组装： 12345QueryPipeline::addSimpleTransform(Source)QueryPipeline::addSimpleTransform(FilterTransform)QueryPipeline::addSimpleTransform(SortingTransform)QueryPipeline::addSimpleTransform(LimitByTransform)QueryPipeline::addSimpleTransform(Sinker) 这样就实现了 Transformer 的编排，但是执行时数据如何进行同步呢？ 2. Transformer 数据同步当 QueryPipeline 进行 transformer 编排时，我们还需要进行更加底层的 DAG 连通构建。 1234connect(Source.OutPort, FilterTransform.InPort)connect(FilterTransform.OutPort, SortingTransform.InPort)connect(SortingTransform.OutPort, LimitByTransform.InPort)connect(LimitByTransform.OutPort, Sinker.InPort) 这样就实现了数据的流向关系，一个 transformer 的 OutPort 对接另外一个的 InPort，就像我们现实中的水管管道一样，接口有 3 通甚至多通。 3. Transformer 执行调度现在管道组装起来了，那么管道内的水如何进行处理和给压流动呢？ ClickHouse 定义了一套 transform 状态，processor 根据这些状态来实现调度。 12345678910enum class Status&#123; NeedData // 等待数据流进入 PortFull, // 管道流出端阻塞 Finished, // 完成状态，退出 Ready, // 切换到 work 函数，进行逻辑处理 Async, // 切换到 schedule 函数，进行异步处理 Wait, // 等待异步处理 ExpandPipeline, // Pipeline 需要裂变&#125;; 当 source 生成数据后，它的状态会设置为 PortFull，意思是等着流入其他 transformer 的 InPort，processor 会开始调度 FilterTransformer(NeedData) 的 Prepare，进行 PullData，然后它的状态设置为 Ready，等待 processor 调度 Work 方法进行数据Filter处理，大家就这样靠状态让 processor 去感知，来调度和做状态迁移，直到 Finished 状态。 这里值得一提的是 ExpandPipeline 状态，它会根据 transformer 的实现，可以把一个 transformer 裂变出更多个 transformer 并行执行，达到一个爆炸效果。 Example1SELECT number + 1 FROM t1; 为了更加深入理解 ClickHouse 的 processor 和 scheduler 机制，我们来一个原生态的 example: 一个 Source:{0,1,2,3,4} AdderTransformer 对每个数字做加1操作 一个 Sinker，输出结果 1. Source1234567891011121314151617181920212223242526272829class MySource : public ISource&#123;public: String getName() const override &#123; return &quot;MySource&quot;; &#125; MySource(UInt64 end_) : ISource(Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), std::make_shared&lt;DataTypeUInt64&gt;(), &quot;number&quot;&#125;&#125;)), end(end_) &#123; &#125;private: UInt64 end; bool done = false; Chunk generate() override &#123; if (done) &#123; return Chunk(); &#125; MutableColumns columns; columns.emplace_back(ColumnUInt64::create()); for (auto i = 0U; i &lt; end; i++) columns[0]-&gt;insert(i); done = true; return Chunk(std::move(columns), end); &#125;&#125;; 2. MyAddTransform12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273class MyAddTransformer : public IProcessor&#123;public: String getName() const override &#123; return &quot;MyAddTransformer&quot;; &#125; MyAddTransformer() : IProcessor( &#123;Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), std::make_shared&lt;DataTypeUInt64&gt;(), &quot;number&quot;&#125;&#125;)&#125;, &#123;Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), std::make_shared&lt;DataTypeUInt64&gt;(), &quot;number&quot;&#125;&#125;)&#125;) , input(inputs.front()) , output(outputs.front()) &#123; &#125; Status prepare() override &#123; if (output.isFinished()) &#123; input.close(); return Status::Finished; &#125; if (!output.canPush()) &#123; input.setNotNeeded(); return Status::PortFull; &#125; if (has_process_data) &#123; output.push(std::move(current_chunk)); has_process_data = false; &#125; if (input.isFinished()) &#123; output.finish(); return Status::Finished; &#125; if (!input.hasData()) &#123; input.setNeeded(); return Status::NeedData; &#125; current_chunk = input.pull(false); return Status::Ready; &#125; void work() override &#123; auto num_rows = current_chunk.getNumRows(); auto result_columns = current_chunk.cloneEmptyColumns(); auto columns = current_chunk.detachColumns(); for (auto i = 0U; i &lt; num_rows; i++) &#123; auto val = columns[0]-&gt;getUInt(i); result_columns[0]-&gt;insert(val+1); &#125; current_chunk.setColumns(std::move(result_columns), num_rows); has_process_data = true; &#125; InputPort &amp; getInputPort() &#123; return input; &#125; OutputPort &amp; getOutputPort() &#123; return output; &#125;protected: bool has_input = false; bool has_process_data = false; Chunk current_chunk; InputPort &amp; input; OutputPort &amp; output;&#125;; 3. MySink12345678910111213141516171819202122232425262728293031323334class MySink : public ISink&#123;public: String getName() const override &#123; return &quot;MySinker&quot;; &#125; MySink() : ISink(Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), std::make_shared&lt;DataTypeUInt64&gt;(), &quot;number&quot;&#125;&#125;)) &#123; &#125;private: WriteBufferFromFileDescriptor out&#123;STDOUT_FILENO&#125;; FormatSettings settings; void consume(Chunk chunk) override &#123; size_t rows = chunk.getNumRows(); size_t columns = chunk.getNumColumns(); for (size_t row_num = 0; row_num &lt; rows; ++row_num) &#123; writeString(&quot;prefix-&quot;, out); for (size_t column_num = 0; column_num &lt; columns; ++column_num) &#123; if (column_num != 0) writeChar(&#x27;\\t&#x27;, out); getPort() .getHeader() .getByPosition(column_num) .type-&gt;serializeAsText(*chunk.getColumns()[column_num], row_num, out, settings); &#125; writeChar(&#x27;\\n&#x27;, out); &#125; out.next(); &#125;&#125;; 4. DAG Scheduler1234567891011121314int main(int, char **)&#123; auto source0 = std::make_shared&lt;MySource&gt;(5); auto add0 = std::make_shared&lt;MyAddTransformer&gt;(); auto sinker0 = std::make_shared&lt;MySink&gt;(); /// Connect. connect(source0-&gt;getPort(), add0-&gt;getInputPort()); connect(add0-&gt;getOutputPort(), sinker0-&gt;getPort()); std::vector&lt;ProcessorPtr&gt; processors = &#123;source0, add0, sinker0&#125;; PipelineExecutor executor(processors); executor.execute(1);&#125; 总结从开发者角度看还是比较复杂，状态迁移还需要开发者自己控制，不过 upstream 已经做了大量的基础工作，比如对 source的封装 ISource，对 sink 的封装 ISink，还有一个基础的 ISimpleTransform，让开发者在上层使用 processor 时更加容易，可以积木式搭建出自己想要的 pipeline。 ClickHouse 的 transformer 数据单元是 Chunk，transformer 对上游 OutPort 流过来的 Chunk 进行加工，然后输出给下游的 InPort，图连通式的流水线并行工作，让 CPU 尽量满负荷工作。 当一个 SQL 被解析成 AST 后，ClickHouse 根据 AST 构建 Query Plan，然后根据 QueryPlan 构建出 pipeline，最后由 processor 负责调度和执行。 目前，ClickHouse 新版本已经默认开启 QueryPipeline，同时这块代码也在不停的迭代。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"DAG Scheduler","slug":"DAG-Scheduler","permalink":"http://dbkernel.github.io/tags/DAG-Scheduler/"},{"name":"pipeline","slug":"pipeline","permalink":"http://dbkernel.github.io/tags/pipeline/"},{"name":"processor","slug":"processor","permalink":"http://dbkernel.github.io/tags/processor/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（3）MySQL Protocol和Write调用栈","slug":"clickhouse-and-friends-03-mysql-protocol-write-stack","date":"2020-06-08T11:57:10.000Z","updated":"2021-09-24T03:59:35.901Z","comments":true,"path":"2020/06/08/clickhouse-and-friends-03-mysql-protocol-write-stack/","link":"","permalink":"http://dbkernel.github.io/2020/06/08/clickhouse-and-friends-03-mysql-protocol-write-stack/","excerpt":"","text":"本文首发于 2020-06-08 19:57:10 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/06/08/clickhouse-and-friends-mysql-protocol-write-stack/以下为正文。 上篇的MySQL Protocol和Read调用里介绍了 ClickHouse 一条查询语句的调用栈，本文继续介绍写的调用栈，开整。 Write请求 建表: 12mysql&gt; CREATE TABLE test(a UInt8, b UInt8, c UInt8) ENGINE=MergeTree() PARTITION BY (a, b) ORDER BY c;Query OK, 0 rows affected (0.03 sec) 写入数据： 1INSERT INTO test VALUES(1,1,1), (2,2,2); 调用栈分析1. 获取存储引擎 OutputStream1234567DB::StorageMergeTree::write(std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, DB::Context const&amp;) StorageMergeTree.cpp:174DB::PushingToViewsBlockOutputStream::PushingToViewsBlockOutputStream(std::__1::shared_ptr&lt;DB::IStorage&gt; const&amp;, DB::Context const&amp;, std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, bool) PushingToViewsBlockOutputStream.cpp:110DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:229DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:364DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:696DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311DB::MySQLHandler::run() MySQLHandler.cpp:141 2. 从 SQL 组装 InputStream(1,1,1), (2,2,2) 如何组装成 inputstream 结构呢？ 12345DB::InputStreamFromASTInsertQuery::InputStreamFromASTInsertQuery(std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, DB::ReadBuffer*,DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:300DB::executeQueryImpl(char const*, char const*, DB::Context&amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) executeQuery.cpp:386DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313DB::MySQLHandler::run() MySQLHandler.cpp:150 然后 12res.in = std::make_shared&lt;InputStreamFromASTInsertQuery&gt;(query_ptr, nullptr, query_sample_block, context, nullptr);res.in = std::make_shared&lt;NullAndDoCopyBlockInputStream&gt;(res.in, out_streams.at(0)); 通过 NullAndDoCopyBlockInputStream的 copyData 方法构造出 Block： 12345678910111213141516DB::ValuesBlockInputFormat::readRow(std::__1::vector&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt;, std::__1::allocator&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt; &gt; &gt;&amp;, unsigned long) ValuesBlockInputFormat.cpp:93DB::ValuesBlockInputFormat::generate() ValuesBlockInputFormat.cpp:55DB::ISource::work() ISource.cpp:48DB::InputStreamFromInputFormat::readImpl() InputStreamFromInputFormat.h:48DB::IBlockInputStream::read() IBlockInputStream.cpp:57DB::InputStreamFromASTInsertQuery::readImpl() InputStreamFromASTInsertQuery.h:31DB::IBlockInputStream::read() IBlockInputStream.cpp:57void DB::copyDataImpl&lt;DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::$_0&amp;, void (&amp;)(DB::Block const&amp;)&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::$_0&amp;, void (&amp;)(DB::Block const&amp;)) copyData.cpp:26DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*) copyData.cpp:62DB::NullAndDoCopyBlockInputStream::readImpl() NullAndDoCopyBlockInputStream.h:47DB::IBlockInputStream::read() IBlockInputStream.cpp:57void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:26DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:73DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:785DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313DB::MySQLHandler::run() MySQLHandler.cpp:150 3. 组装 OutputStream12345DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:107DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:364DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:696DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311DB::MySQLHandler::run() MySQLHandler.cpp:141 组装顺序: NullAndDoCopyBlockInputStream CountingBlockOutputStream AddingDefaultBlockOutputStream SquashingBlockOutputStream PushingToViewsBlockOutputStream MergeTreeBlockOutputStream 4. 写入OutputStream123456789101112131415DB::MergeTreeBlockOutputStream::write(DB::Block const&amp;) MergeTreeBlockOutputStream.cpp:17DB::PushingToViewsBlockOutputStream::write(DB::Block const&amp;) PushingToViewsBlockOutputStream.cpp:145DB::SquashingBlockOutputStream::finalize() SquashingBlockOutputStream.cpp:30DB::SquashingBlockOutputStream::writeSuffix() SquashingBlockOutputStream.cpp:50DB::AddingDefaultBlockOutputStream::writeSuffix() AddingDefaultBlockOutputStream.cpp:25DB::CountingBlockOutputStream::writeSuffix() CountingBlockOutputStream.h:37DB::copyDataImpl&lt;DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::&lt;lambda()&gt;&amp;, void (&amp;)(const DB::Block&amp;)&gt;(DB::IBlockInputStream &amp;, DB::IBlockOutputStream &amp;, &lt;lambda()&gt; &amp;, void (&amp;)(const DB::Block &amp;)) copyData.cpp:52DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*) copyData.cpp:138DB::NullAndDoCopyBlockInputStream::readImpl() NullAndDoCopyBlockInputStream.h:57DB::IBlockInputStream::read() IBlockInputStream.cpp:60void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:29DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:154DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:748DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311DB::MySQLHandler::run() MySQLHandler.cpp:141 通过 copyData 方法，让数据在 OutputStream 间层层透传，一直到 MergeTreeBlockOutputStream。 5. 返回 Client123456789DB::MySQLOutputFormat::finalize() MySQLOutputFormat.cpp:62DB::IOutputFormat::doWriteSuffix() IOutputFormat.h:78DB::OutputStreamToOutputFormat::writeSuffix() OutputStreamToOutputFormat.cpp:18DB::MaterializingBlockOutputStream::writeSuffix() MaterializingBlockOutputStream.h:22void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:52DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:154DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:748DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311DB::MySQLHandler::run() MySQLHandler.cpp:141 总结1INSERT INTO test VALUES(1,1,1), (2,2,2); 首先内核解析 SQL 语句生成 AST，根据 AST 获取 Interpreter：InterpreterInsertQuery。 其次 Interpreter 依次添加相应的 OutputStream。 然后从 InputStream 读取数据，写入到 OutputStream，stream 会层层渗透，一直写到底层的存储引擎。 最后写入到 Socket Output，返回结果。 ClickHouse 的 OutputStream 编排还是比较复杂，缺少类似 Pipeline 的调度和编排，但是由于模式比较固化，目前看还算清晰。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（2）MySQL Protocol和Read调用栈","slug":"clickhouse-and-friends-02-mysql-protocol-read-stack","date":"2020-06-07T09:17:10.000Z","updated":"2021-09-24T03:58:59.682Z","comments":true,"path":"2020/06/07/clickhouse-and-friends-02-mysql-protocol-read-stack/","link":"","permalink":"http://dbkernel.github.io/2020/06/07/clickhouse-and-friends-02-mysql-protocol-read-stack/","excerpt":"","text":"本文首发于 2020-06-07 17:17:10 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/以下为正文。 作为一个 OLAP 的 DBMS 来说，有2个端非常重要： 用户如何方便的链进来，这是入口端 ClickHouse 除了自己的 client 外，还提供了 MySQL/PG/GRPC/HTTP 等接入方式 数据如何方便的挂上去，这是数据源端 ClickHouse 除了自己的引擎外，还可以挂载 MySQL/Kafka 等外部数据源 这样内外互通，多条朋友多条路，以实现“数据”级的编排能力。 今天谈的是入口端的 MySQL 协议，也是本系列 ClickHouse 的第一个好朋友，用户可通过 MySQL 客户端或相关 Driver 直接链接到 ClickHouse，进行数据读写等操作。 本文通过 MySQL的 Query 请求，借用调用栈来了解下 ClickHouse 的数据读取全过程。 如何实现？入口文件在:MySQLHandler.cpp 握手协议 MySQLClient 发送 Greeting 数据报文到 MySQLHandler MySQLHandler 回复一个 Greeting-Response 报文 MySQLClient 发送认证报文 MySQLHandler 对认证报文进行鉴权，并返回鉴权结果 MySQL Protocol 实现在: Core/MySQLProtocol.h 最近的代码中调整为了 Core/MySQL/PacketsProtocolText.h Query请求当认证通过后，就可以进行正常的数据交互了。 当 MySQLClient 发送请求: 1mysql&gt; SELECT * FROM system.numbers LIMIT 5; MySQLHandler 的调用栈： 1-&gt;MySQLHandler::comQuery -&gt; executeQuery -&gt; pipeline-&gt;execute -&gt; MySQLOutputFormat::consume MySQLClient 接收到结果 在步骤2里，executeQuery(executeQuery.cpp)非常重要。 它是所有前端 Server 和 ClickHouse 内核的接入口，第一个参数是 SQL 文本(‘select 1’)，第二个参数是结果集要发送到哪里去(socket net)。 调用栈分析1SELECT * FROM system.numbers LIMIT 5 1. 获取数据源StorageSystemNumbers 数据源： 123456789101112DB::StorageSystemNumbers::read(std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::__1::shared_ptr&lt;DB::StorageInMemoryMetadata const&gt; const&amp;, DB::SelectQueryInfo const&amp;, DB::Context const&amp;, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) StorageSystemNumbers.cpp:135DB::ReadFromStorageStep::ReadFromStorageStep(std::__1::shared_ptr&lt;DB::RWLockImpl::LockHolderImpl&gt;, std::__1::shared_ptr&lt;DB::StorageInMemoryMetadata const&gt;&amp;, DB::SelectQueryOptions,DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) memory:3028DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) InterpreterSelectQuery.cpp:1361DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::IBlockInputStream&gt; const&amp;, std::__1::optional&lt;DB::Pipe&gt;) InterpreterSelectQuery.cpp:791DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&amp;) InterpreterSelectQuery.cpp:472DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&amp;) InterpreterSelectWithUnionQuery.cpp:183DB::InterpreterSelectWithUnionQuery::execute() InterpreterSelectWithUnionQuery.cpp:198DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:385DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;,DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307DB::MySQLHandler::run() MySQLHandler.cpp:141 这里最主要的是 ReadFromStorageStep 函数，从不同 storage 里获取数据源 pipe: 1Pipes pipes = storage-&gt;read(required_columns, metadata_snapshot, query_info, *context, processing_stage, max_block_size, max_streams); 2. Pipeline构造12345678910111213DB::LimitTransform::LimitTransform(DB::Block const&amp;, unsigned long, unsigned long, unsigned long, bool, bool, std::__1::vector&lt;DB::SortColumnDescription, std::__1::allocator&lt;DB::SortColumnDescription&gt; &gt;) LimitTransform.cpp:21DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:2214DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:2299DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:3570DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:4400DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) LimitStep.cpp:33DB::ITransformingStep::updatePipeline(std::__1::vector&lt;std::__1::unique_ptr&lt;DB::QueryPipeline, std::__1::default_delete&lt;DB::QueryPipeline&gt; &gt;, std::__1::allocator&lt;std::__1::unique_ptr&lt;DB::QueryPipeline, std::__1::default_delete&lt;DB::QueryPipeline&gt; &gt; &gt; &gt;) ITransformingStep.cpp:21DB::QueryPlan::buildQueryPipeline() QueryPlan.cpp:154DB::InterpreterSelectWithUnionQuery::execute() InterpreterSelectWithUnionQuery.cpp:200DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:385DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:722DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307DB::MySQLHandler::run() MySQLHandler.cpp:141 3. Pipeline执行123456789101112131415161718DB::LimitTransform::prepare(std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;, std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;) LimitTransform.cpp:67DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:291DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373DB::PipelineExecutor::initializeExecution(unsigned long) PipelineExecutor.cpp:747DB::PipelineExecutor::executeImpl(unsigned long) PipelineExecutor.cpp:764DB::PipelineExecutor::execute(unsigned long) PipelineExecutor.cpp:479DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:833DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307DB::MySQLHandler::run() MySQLHandler.cpp:141 4. Output执行发送1234567891011DB::MySQLOutputFormat::consume(DB::Chunk) MySQLOutputFormat.cpp:53DB::IOutputFormat::work() IOutputFormat.cpp:62DB::executeJob(DB::IProcessor *) PipelineExecutor.cpp:155operator() PipelineExecutor.cpp:172DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic&lt;bool&gt;*) PipelineExecutor.cpp:630DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) PipelineExecutor.cpp:546DB::PipelineExecutor::executeImpl(unsigned long) PipelineExecutor.cpp:812DB::PipelineExecutor::execute(unsigned long) PipelineExecutor.cpp:479DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:800DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311DB::MySQLHandler::run() MySQLHandler.cpp:141 总结ClickHouse 的模块化比较清晰，像乐高积木一样可以组合拼装，当我们执行: 1SELECT * FROM system.numbers LIMIT 5 首先内核解析 SQL 语句生成 AST，然后根据 AST 获取数据源 Source，pipeline.Add(Source)。 其次根据 AST 信息生成 QueryPlan，根据 QueryPlan 再生成相应的 Transform，pipeline.Add(LimitTransform)。 然后添加 Output Sink 作为数据发送对象，pipeline.Add(OutputSink)。 执行 pipeline, 各个 Transformer 开始工作。 ClickHouse 的 Transformer 调度系统叫做 Processor，也是决定性能的重要模块，详情见 Pipeline 处理器和调度器。 ClickHouse 是一辆手动挡的豪华跑车，免费拥有，海啸们！ 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"源码分析 | ClickHouse和他的朋友们（1）编译、开发、测试","slug":"clickhouse-and-friends-01-development","date":"2020-06-05T11:37:10.000Z","updated":"2021-09-22T15:24:33.493Z","comments":true,"path":"2020/06/05/clickhouse-and-friends-01-development/","link":"","permalink":"http://dbkernel.github.io/2020/06/05/clickhouse-and-friends-01-development/","excerpt":"","text":"本文首发于 2020-06-05 19:37:10 《ClickHouse和他的朋友们》系列文章转载自圈内好友 BohuTANG 的博客，原文链接：https://bohutang.me/2020/06/05/clickhouse-and-friends-development/以下为正文。 一次偶然的机会，和ClickHouse团队做了一次线下沟通，Alexey提到ClickHouse的设计哲学: The product must solve actual problem And do it better than others 用工程思维解决商业问题的典范啊！ 对用户来说，他们关心的不是什么天花乱坠、上天入地的高科技，只是需要一个能很好解决自己问题的方案，这在开源社区是非常难得的，靠实力“野蛮式”生长。 于是，我对这个散发着伏特加味道的利器充满了好奇，并参与到ClickHouse的社区中一探究竟，第一感觉是开放、友好、战斗力强(AK47 vs CK16, ClickHouse 2016年开源)。 本文先从编译和测试入手，再到如何为社区贡献Patch，希望对那些想参与CK社区的同学有所帮助。 如何本地编译和测试ClickHouse？源码获取1git clone --recursive https://github.com/ClickHouse/ClickHouse 编译准备1234567sudo apt install build-essentialsudo apt-get install software-properties-commonsudo apt-add-repository ppa:ubuntu-toolchain-r/testsudo apt-get updatesudo apt-get install gcc-9 g++-9 git python ninja-buildsudo snap install cmake 开始编译1234567cd ClickHousemkdir buildcd buildexport CC=gcc-9export CXX=g++-9cmake ..ninja 测试方法ClickHouse的测试在官方development/tests文档里有详细的介绍，这里列举3个常用的测试模式： 1. Functional Tests功能测试，主要用于ClickHouse内部功能测试，方式：输入一个sql文件，输出一个result，类似MySQL里的mtr，测试集合 12cd tests./clickhouse-test -c &quot;../build/programs/clickhouse-client&quot; 00001_select_1 2. Integration Tests集成测试，主要用于涉及第三方服务的测试，比如MySQL/Postgres/MongoDB等，以容器化方式编排调度(pytest)运行，测试集合 由于涉及模块较多，集成测试环境的搭建有一定的难度，建议使用官方的docker镜像。比如要跑test_mysql_protocol下的集成测试集： 123cd tests/integrationdocker pull yandex/clickhouse-integration-tests-runner./runner --binary /your/ClickHouse/build/programs/clickhouse --bridge-binary /your/ClickHouse/build/programs/clickhouse-odbc-bridge --configs-dir /your/ClickHouse/programs/server/ &#x27;test_mysql_protocol/test.py::test_java_client -ss -vv&#x27; 3. Unit Tests单元测试，主要用于代码模块的测试，测试集在各个模块的tests目录，比如: Core/tests 如果大家想了解某个模块是如何工作的，强烈建议去翻翻该模块的tests目录，比如想了解processor的工作机制，跟踪调试 Processors/tests/ 即可。 如何给ClickHouse社区提Patch？1. fork首先在自己的github上fork一份ClickHouse代码，比如 https://github.com/BohuTANG/ClickHouse 2. clone到本地12git clone --recursive https://github.com/BohuTANG/ClickHousegit checkout -B mysql_replica(branch名字) 3. 创建新的分支1git checkout -B mysql_replica(branch名字) 4. 功能开发开发者可以提交一个Draft Pull Request到官方，github会显示这个Pull Request处于Draft状态，官方是无法Merge的 5. can be testd标签等待Upstream打[can be tested]标签，一旦被标记CI狂魔们就强势开跑，跑一轮大概需要几十个小时。协助开发者发现一些代码Style、编译以及测试等错误，这样开发者就可以在自己的分支不停的迭代、修正。 如果只是修改typo，这个标签Upstream通常不会添加。 6. 开发完毕开发完成，测试OK，把Draft提升为正式Pull Request，等待Upstraem Review。 7. Merge到Master如果Upstream通过，你的代码会被Merge到Master，恭喜你成为ClickHouse贡献者 8. 注意事项ClickHouse Upstream迭代非常快，一定要多关注master分支进度，尽量保持自己的分支代码与master同步。否则Upstream Docker更新，自己的test可能就过不了。 建议把doc/development读一遍。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"}],"tags":[{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}]},{"title":"特性介绍 | MySQL select count(*) 、count(1)、count(列) 详解（1）：概念及区别","slug":"mysql-select-count-functions-01-concepts-and-differences","date":"2020-05-06T07:55:15.000Z","updated":"2021-09-24T04:11:08.572Z","comments":true,"path":"2020/05/06/mysql-select-count-functions-01-concepts-and-differences/","link":"","permalink":"http://dbkernel.github.io/2020/05/06/mysql-select-count-functions-01-concepts-and-differences/","excerpt":"","text":"本文首发于 2020-05-05 21:55:15 一、前言从接触MySQL开始断断续续的看过一些文章，对count()操作众说纷纭，其中分歧点主要在于count(1)和count(*)哪个效率高，有说count(1)比count(*)快的（这种说法更普遍），有说二者一样快的。个人理解这两种行为可能适用于的是不同的版本，我只关心较新的MySQL版本是什么行为，详见下文。 二、含义首先，先说明一下常见count()操作及含义： count(*)：计算包括NULL值在内的行数，SQL92定义的标准统计行数的语法。 count(1)：计算包括NULL值在内的行数，其中的1是恒真表达式。 count(列名)：计算指定列的行数，但不包含NULL值。 三、具体区别MySQL手册中相关描述如下： For transactional storage engines such as InnoDB, storing an exact row count is problematic. Multiple transactions may be occurring at the same time, each of which may affect the count. InnoDB does not keep an internal count of rows in a table because concurrent transactions might “see” different numbers of rows at the same time. Consequently, SELECT COUNT(*) statements only count rows visible to the current transaction. Prior to MySQL 5.7.18, InnoDB processes SELECT COUNT(*) statements by scanning the clustered index. As of MySQL 5.7.18, InnoDB processes SELECT COUNT(*) statements by traversing the smallest available secondary index unless an index or optimizer hint directs the optimizer to use a different index. If a secondary index is not present, the clustered index is scanned. Processing SELECT COUNT(*) statements takes some time if index records are not entirely in the buffer pool. For a faster count, create a counter table and let your application update it according to the inserts and deletes it does. However, this method may not scale well in situations where thousands of concurrent transactions are initiating updates to the same counter table. If an approximate row count is sufficient, use SHOW TABLE STATUS. InnoDB handles SELECT COUNT(*) and SELECT COUNT(1) operations in the same way. There is no performance difference. For MyISAM tables, COUNT(*) is optimized to return very quickly if the SELECT retrieves from one table, no other columns are retrieved, and there is no WHERE clause. For example: 1&gt;mysql&gt; SELECT COUNT(*) FROM student; This optimization only applies to MyISAM tables, because an exact row count is stored for this storage engine and can be accessed very quickly.COUNT(1) is only subject to the same optimization if the first column is defined as NOT NULL. 官方这段描述要点如下： InnoDB是事务引擎，支持MVCC，并发事务可能同时“看到”不同的行数，所以，InnoDB不保留表中的行数，SELECT COUNT(*)语句只计算当前事务可见的行数。 在MySQL 5.7.18之前，InnoDB通过扫描聚集索引处理SELECT COUNT(*)语句。从MySQL 5.7.18开始，InnoDB通过遍历最小的可用二级索引来处理SELECT COUNT(*)语句，除非索引或优化器明确指示使用不同的索引。如果不存在二级索引，则扫描聚集索引。这样的设计单从 IO 的角度就节省了很多开销。 InnoDB以同样的方式处理SELECT COUNT(*)和SELECT COUNT(1)操作，没有性能差异。 因此，建议使用符合SQL标准的count(*)。 对于MyISAM表，由于MyISAM引擎存储了精确的行数，因此，如果SELECT COUNT(*)语句不包含WHERE子句，则会很快返回。这个很好理解，如果带了where条件，就需要扫表了。 如果索引记录不完全在缓冲池中，则处理SELECT(*)语句需要一些时间。为了更快的计数，您可以创建一个计数器表，并让您的应用程序按插入和删除操作更新它。然而，这种方法在同一计数器表中启动成千上万个并发事务的情况下，可能无法很好地扩展。如果一个近似的行数足够，可以使用SHOW TABLE STATUS查询行数。 到这里我们明白了 count(*) 和 count(1) 本质上面其实是一样的，那么 count(column) 又是怎么回事呢？ count(column) 也是会遍历整张表，但是不同的是它会拿到 column 的值以后判断是否为空，然后再进行累加，那么如果针对主键需要解析内容，如果是二级索引需要再次根据主键获取内容，则要多一次 IO 操作，所以 count(column) 的性能肯定不如前两者，如果按照效率比较的话：*count()=count(1)&gt;count(primary key)&gt;count(非主键column)**。 四、建议基于以上描述，如果要查询innodb存储引擎的表的总行数，有如下建议： 若仅仅是想获取大概的行数，建议使用show table status或查询information_schema.tables：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667mysql&gt; use db6;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+---------------+| Tables_in_db6 |+---------------+| t1 |+---------------+1 row in set (0.01 sec)mysql&gt; select count(*) from t1;+----------+| count(*) |+----------+| 2 |+----------+1 row in set (0.00 sec)mysql&gt; show table status\\G*************************** 1. row *************************** Name: t1 Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 2 Avg_row_length: 8192 Data_length: 16384Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2020-04-21 12:00:44 Update_time: NULL Check_time: NULL Collation: utf8mb4_general_ci Checksum: NULL Create_options: Comment:1 row in set (0.00 sec)mysql&gt; select * from information_schema.tables where table_name = &#x27;t1&#x27;\\G*************************** 1. row *************************** TABLE_CATALOG: def TABLE_SCHEMA: db6 TABLE_NAME: t1 TABLE_TYPE: BASE TABLE ENGINE: InnoDB VERSION: 10 ROW_FORMAT: Dynamic TABLE_ROWS: 2 AVG_ROW_LENGTH: 8192 DATA_LENGTH: 16384MAX_DATA_LENGTH: 0 INDEX_LENGTH: 0 DATA_FREE: 0 AUTO_INCREMENT: NULL CREATE_TIME: 2020-04-21 12:00:44 UPDATE_TIME: NULL CHECK_TIME: NULLTABLE_COLLATION: utf8mb4_general_ci CHECKSUM: NULL CREATE_OPTIONS: TABLE_COMMENT:1 row in set (0.00 sec) 反之，如果必须要获取准确的总行数，建议： 创建一个计数器表，并让您的应用程序按插入和删除操作更新它。 若业务插入和删除相对较少，也可以考虑缓存到 redis。 篇幅有限，深入验证、源码分析将在下一篇文章中介绍。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"Select","slug":"Select","permalink":"http://dbkernel.github.io/tags/Select/"},{"name":"Count","slug":"Count","permalink":"http://dbkernel.github.io/tags/Count/"}]},{"title":"特性介绍 | MySQL 自增列详解（1）：自增列概念及使用","slug":"mysql-auto_increment-details-01-concepts-and-usage","date":"2019-12-09T11:37:10.000Z","updated":"2021-09-24T04:10:26.285Z","comments":true,"path":"2019/12/09/mysql-auto_increment-details-01-concepts-and-usage/","link":"","permalink":"http://dbkernel.github.io/2019/12/09/mysql-auto_increment-details-01-concepts-and-usage/","excerpt":"一直想写一些关于自增列的文章，今天下班比较早，Let’s do this.","text":"一直想写一些关于自增列的文章，今天下班比较早，Let’s do this. 本文首发于 2019-12-09 19:37:10 1. 概念自增列，即 AUTO_INCREMENT，可用于为新的记录生成唯一标识。 要求： AUTO_INCREMENT 是数据列的一种属性，只适用于整数类型数据列。 AUTO_INCREMENT 数据列必须具备 NOT NULL 属性。 2. 使用方法2.1. 创建含自增列的表1234567-- 不指定 AUTO_INCREMENT 的值，则从1开始mysql&gt; create table t1(a int auto_increment primary key,b int);Query OK, 0 rows affected (0.01 sec)-- 手动指定 AUTO_INCREMENT 的值mysql&gt; create table t2(a int auto_increment primary key,b int) AUTO_INCREMENT=100;Query OK, 0 rows affected (0.02 sec) 2.2. 插入数据12345678910111213141516-- 不指定自增列mysql&gt; insert into t1(b) values(1),(2);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+---+------+| a | b |+---+------+| 1 | 1 || 2 | 2 |+---+------+3 rows in set (0.00 sec)-- 指定自增列mysql&gt; insert into t1(a,b) values(3,3);Query OK, 1 row affected (0.00 sec) 2.3. 如何查看表的 AUTO_INCREMENT 涨到了多少？1234567891011mysql&gt; show create table t1;+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `a` int(11) NOT NULL AUTO_INCREMENT, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 2.4. 插入数据时能否有空洞？可以的，但要注意 AUTO_INCREMENT 的值一定比自增列当前最大的记录值大。 1234567891011121314151617181920212223242526-- 创造空洞mysql&gt; insert into t1(a,b) values(5,5);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+---+------+| a | b |+---+------+| 1 | 1 || 2 | 2 || 3 | 3 || 5 | 5 |+---+------+5 rows in set (0.00 sec)mysql&gt; show create table t1;+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `a` int(11) NOT NULL AUTO_INCREMENT, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 2.5. 能否插入重复记录既然自增列是唯一记录，那么肯定不能插入重复记录。 123-- 尝试插入重复记录mysql&gt; insert into t1(a,b) values(5,5);ERROR 1062 (23000): Duplicate entry &#x27;5&#x27; for key &#x27;PRIMARY&#x27; 2.6. 怎么修改 AUTO_INCREMENT 的值？注意：AUTO_INCREMENT 不能小于当前自增列记录的最大值。 12345678910111213141516171819202122232425262728293031323334-- 尝试将 AUTO_INCREMENT 设为10mysql&gt; alter table t1 AUTO_INCREMENT=10;Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show create table t1;+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `a` int(11) NOT NULL AUTO_INCREMENT, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8 |+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)-- 尝试将 AUTO_INCREMENT 设为4mysql&gt; alter table t1 AUTO_INCREMENT=4;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0-- 由于自增列最大记录值是5，那么 AUTO_INCREMENT 不能小于5，因此该值为6mysql&gt; show create table t1;+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `a` int(11) NOT NULL AUTO_INCREMENT, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 3. 问题3.1. 自增列是否有上限？由上文可见，自增列会一直增加，那是否有上限呢？ 上文中表 t1 的自增列是 int 类型，由下表（MySQL 5.7）可见取值范围是 -2147483648 到 2147483647（ -231 ~ 231 - 1 ）。 Type Storage (Bytes) Minimum Value Signed Minimum Value Unsigned Maximum Value Signed Maximum Value Unsigned TINYINT 1 -128 0 127 255 SMALLINT 2 -32768 0 32767 65535 MEDIUMINT 3 -8388608 0 8388607 16777215 INT 4 -2147483648 0 2147483647 4294967295 BIGINT 8 -263 0 263-1 264-1 验证如下： 12345678910111213141516171819202122232425262728mysql&gt; show create table t1;+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `a` int(11) NOT NULL AUTO_INCREMENT, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB AUTO_INCREMENT=2147483644 DEFAULT CHARSET=utf8 |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)mysql&gt; insert into t1(b) values(0),(0),(0);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t1(b) values(0);ERROR 1062 (23000): Duplicate entry &#x27;2147483647&#x27; for key &#x27;PRIMARY&#x27;mysql&gt; show create table t1;+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| t1 | CREATE TABLE `t1` ( `a` int(11) NOT NULL AUTO_INCREMENT, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB AUTO_INCREMENT=2147483647 DEFAULT CHARSET=utf8 |+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 这里需要补充说明下 int(11) 中的数字的含义： MySQL中整数数据类型后面的(N)指定显示宽度。 显示宽度不影响查询出来的结果。 显示宽度限制了小数点的位置(只要实际数字不超过显示宽度，这种情况下，数字显示为原样)。 显示宽度也是一个有用的工具，可以让开发人员知道应该将值填充到哪个长度。 3.2. 如何避免自增列超过最大值？可以采用无符号的 BIGINT 类型（也可根据业务产生自增列的速度采用合适的类型），能极大提升自增列的范围。 1234567891011121314151617181920212223242526272829303132mysql&gt; create table t2(a bigint unsigned primary key auto_increment,b int);Query OK, 0 rows affected (0.00 sec)mysql&gt; alter table t2 auto_increment=18446744073709551613;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show create table t2;+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| t2 | CREATE TABLE `t2` ( `a` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `b` int(11) DEFAULT NULL, PRIMARY KEY (`a`)) ENGINE=InnoDB AUTO_INCREMENT=18446744073709551613 DEFAULT CHARSET=utf8 |+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row in set (0.01 sec)mysql&gt; insert into t2(b) values(0);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t2(b) values(0);ERROR 1467 (HY000): Failed to read auto-increment value from storage enginemysql&gt;mysql&gt; select * from t2;+----------------------+------+| a | b |+----------------------+------+| 18446744073709551613 | 0 |+----------------------+------+1 row in set (0.00 sec) UNSIGNED BIGINT 类型的范围究竟有多大呢？ 假如每秒自增100万次，想要消耗完需要 18446744073709551613/1000000/3600/24/365=584942年。 有的朋友会问如果自增列不是采用BIGINT类型，那么达到最大值后该表就无法写入，此时该怎么办呢？ 一般达到最大值后再次插入数据会报错ERROR 1467 (HY000): Failed to read auto-increment value from storage engine，可以通过alter table 将自增列的类型设为数值范围更大的类型（比如BIGINT）。 4. 总结 AUTO_INCREMENT 列必定唯一，且仅用于整型类型。 AUTO_INCREMENT 列会持续增长，不会因 delete 自增列最大的记录而变小。 当 AUTO_INCREMENT 列达到当前类型的最大值后将无法插入数据，会报错ERROR 1467 (HY000): Failed to read auto-increment value from storage engine，此时将自增列改为 BIGINT 类型可解决问题。 为了避免自增列达到最大值，可将其设为BIGINT类型。 使用 alter table 修改 AUTO_INCREMENT 列时，其值会取自增列当前最大记录值+1与将要设置的值的最大值。 在MySQL 5.7 中，将列设置成 AUTO_INCREMENT 之后，必须将其设置成主键/或者是主键的一部分，否则会报错ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"auto_increment","slug":"auto-increment","permalink":"http://dbkernel.github.io/tags/auto-increment/"}]},{"title":"引擎特性 | MySQL MEMORY(HEAP) 存储引擎导致 Slave 节点有本地事务","slug":"mysql-memory-engine-slave-has-local-transactions","date":"2019-04-22T12:56:52.000Z","updated":"2021-09-22T15:24:33.493Z","comments":true,"path":"2019/04/22/mysql-memory-engine-slave-has-local-transactions/","link":"","permalink":"http://dbkernel.github.io/2019/04/22/mysql-memory-engine-slave-has-local-transactions/","excerpt":"","text":"本文首发于 2019-04-22 20:56:52 1. MEMORY 引擎简介可能有的朋友对MEMORY存储引擎不太了解，首先介绍一下（以下描述来自官方）： MEMROY存储引擎（以前称为HEAP）的表把表结构存放到磁盘上，而把数据放在内存中。 每个Memory表只实际对应一个磁盘文件，在磁盘中表现为.frm文件。因为它的数据是放在内存中的，并且默认使用hash索引（也支持B-Tree索引），因此Memory类型的表访问速度非常快（比使用B-Tree索引的MyISAM表快），但是一旦服务关闭，表中的数据就会丢失。 由于MEMRORY表在mysqld重启后数据会丢失，为了获得稳定的数据源，可以在启动mysqld时添加--init-file选项，把类似insert into ... select或load data的语句放进去。 MEMROY存储引擎的典型适用场景包含如下特征： 涉及瞬态非关键数据的操作，如会话管理或缓存。 数据可以完全放入内存而不会导致操作系统交换虚拟内存页，并且要求快速访问。 只读或以读为主的数据访问模式（有限的更新）。 关于性能： 在处理更新时，单线程执行和表锁开销导致的争用会限制MEMORY性能。 尽管MEMORY表在内存中进行处理，但是对于繁忙的服务器、通用查询或读/写工作负载，它们并不一定比InnoDB表快。特别是，执行更新所涉及的表锁定会降低多个会话中内存表的并发使用速度。 MEMORY表具有以下特征： MEMORY表的空间以小块形式分配。表对插入使用100%动态哈希，不需要占用额外的内存。 被删除的行并未释放，而是放在链表中，并在插入新数据时重用。 MEMORY表使用固定长度的行存储数据。（即使是VARCHAR也不例外） MEMORY表不支持 BLOB、TEXT 列。 MEMORY表支持 AUTO_INCREMENT 列。 MEMORY表是有大小限制的，主要受限于两个参数： max_heap_table_size 和 MAX_ROWS（默认情况下MAX_ROWS依赖于max_heap_table_size，可执行ALTER TABLE tbl_name MAX_ROWS= MAX_ROWS修改MAX_ROWS）。 问：MEMORY表和临时表有什么区别？ 临时表默认使用的存储引擎是服务器指定的存储引擎（对于5.7是InnoDB），由于临时表定义和数据都放在内存中，未放到磁盘，因此用show tables招不到临时表。 如果临时表占用空间太大，MySQL会将其转为磁盘存储。而对于用户创建的MEMORY表，则不会转为磁盘存储。 12345678910mysql&gt; create temporary table temp_t1(a int primary key, b int);Query OK, 0 rows affected (0.00 sec)mysql&gt; show tables;+---------------+| Tables_in_db4 |+---------------+| t1 |+---------------+1 row in set (0.00 sec) 2. 故障分析现象： 最近碰到有用户使用 MEMORY 存储引擎，引发主从 GTID 不一致、从节点 GTID 比主节点多一条的情况。 分析： 检查日志，确认没有发生过主从切换，也就排除了主节点有 prepare 的事务然后故障（从节点变为主）、重启导致 local commit的情况。 在从节点 binlog 中找到那条本地事务，发现是 MEMORY 表的 DELETE FROM 。 该从节点发生过重启，根据 MEMORY 引擎的特性，确认是 MEMORY 表生成的。 向用户反馈问题原因后，用户将 MEMORY 表改为了 InnoDB 表。 3. 疑问3.1. 何时生成 DELETE FROM？ A server’s MEMORY tables become empty when it is shut down and restarted. If the server is a replication master, its slaves are not aware that these tables have become empty, so you see out-of-date content if you select data from the tables on the slaves. To synchronize master and slave MEMORY tables, when a MEMORY table is used on a master for the first time since it was started, a DELETE statement is written to the master’s binary log, to empty the table on the slaves also. The slave still has outdated data in the table during the interval between the master’s restart and its first use of the table. To avoid this interval when a direct query to the slave could return stale data, use the --init-file option to populate the MEMORY table on the master at startup. 这段描述的含义是： 服务器的 MEMORY 表在关闭和重新启动时会变为空。 为了防止主服务器重启、从服务器未重启导致从服务器上有过期的 MEMORY 表数据，会在重启服务器时向 binlog 写入一条 DELETE FROM 语句，这条语句会复制到从节点，以达到主从数据一致的目的。 3.2. 对于主从复制的 MySQL 集群，主或从故障重启有什么问题？ PS：不想看过程的朋友，请跳到最后看总结。 举例来说，集群有三个节点A、B、C，节点A为主节点。 情形一：MEMORY 表有数据的情况下，重启主节点、触发主从切换： 创建 MEMORY 表 mdb.t1 ，执行 insert into mdb.t1 values(1,1),(2,2),(3,3),(4,4) 插入一些数据。 关闭节点A的 MySQL，节点B变为主，之后节点A以从节点启动，此时： 节点A无数据： 12mysql&gt; select * from mdb.t1;Empty set (0.00 sec) 节点B、C有数据： 12345678910mysql&gt; select * from mdb.t1;+------+------+| a | b |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 || 4 | 4 |+------+------+4 rows in set (0.00 sec) 并且，节点A的 GTID 为 uuid_a:1-11，节点B、C的 GTID 为 uuid_a:1-10，节点A的 binlog 比另外两个节点多一条 DELTE FROM mdb.t1。 情形二：MEMORY 表无数据的情况下，重启主节点、触发主从切换： 将节点A切换为主节点，节点B、C同步了 uuid_a:1-11 这条事务，三个节点的 mdb.t1 数据为空。 关闭节点A的 MySQL，节点B变为主，之后节点A以从节点启动，此时，节点A生成了一条本地 DELETE FROM 事务 uuid_b:1-12。 情形三：MEMORY 表无数据的情况下，重启从节点： 将节点A切换为主节点，节点B、C同步了 uuid_a:1-12 这条事务 重启节点A的MySQL，节点A生成一条本地 DELETE FROM 事务 uuid_a:1-13。 情形四：MEMORY 表有数据的情况下，重启从节点： 将节点A切换为主节点，另外两个节点同步节点A的本地事务，三个节点 GTID 为 uuid_a:1-13 。 执行 INSERT 语句向 mdb.t1 插入一些数据，三个节点 GTID 为 uuid_a:1-14。 重启节点B，其生成了一条本地 DELETE FROM 事务 uuid_b:1。 3.3. 总结 测试发现，无论什么情况下，MEMORY存储引擎都会生成一条本地 DELETE FROM 事务。 在某些情况下，必须主动访问（比如 SELECT）MEMORY 表，才会触发生成 DELETE FROM。 最重要的一点，在生产环境中千万不要使用MEMORY存储引擎。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"MEMORY引擎","slug":"MEMORY引擎","permalink":"http://dbkernel.github.io/tags/MEMORY%E5%BC%95%E6%93%8E/"},{"name":"HEAP引擎","slug":"HEAP引擎","permalink":"http://dbkernel.github.io/tags/HEAP%E5%BC%95%E6%93%8E/"},{"name":"本地事务","slug":"本地事务","permalink":"http://dbkernel.github.io/tags/%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1/"}]},{"title":"特性介绍 | Linux 操作系统启动流程","slug":"process-for-starting-the-linux-os","date":"2018-09-16T13:35:02.000Z","updated":"2021-09-24T05:23:09.880Z","comments":true,"path":"2018/09/16/process-for-starting-the-linux-os/","link":"","permalink":"http://dbkernel.github.io/2018/09/16/process-for-starting-the-linux-os/","excerpt":"","text":"本文首发于 2018-09-16 21:35:02 总启动流程 这部分转自：https://www.cnblogs.com/liang-io/p/9651656.html 一般来说，所有的操作系统的启动流程基本就是： 总的来说，linux系统启动流程可以简单总结为以下几步： 开机BIOS自检，加载硬盘。 读取MBR,进行MBR引导。 grub引导菜单(Boot Loader)。 加载内核kernel。 启动init进程，依据inittab文件设定运行级别 init进程，执行rc.sysinit文件。 启动内核模块，执行不同级别的脚本程序。 执行/etc/rc.d/rc.local 启动mingetty，进入系统登陆界面。 linux系统安装时，如果要想设置开启启动项，可以： 开机到BIOS提醒界面，按键F11（Dell服务器的做法）进入BIOS设置BOOT MENU，继而设置启动项：硬盘HD启动，光盘CD/DVD启动，还是U盘USB启动。 详细流程下面就linux操作系统的启动过程做一详细解析记录。 1. 加载内核操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 12345678910[root@bastion-IDC ~]# ll /boot/total 21668-rw-r--r--. 1 root root 105195 Nov 22 2013 config-2.6.32-431.el6.x86_64drwxr-xr-x. 3 root root 1024 Aug 22 16:31 efidrwxr-xr-x. 2 root root 1024 Aug 22 16:32 grub-rw-------. 1 root root 15217153 Aug 22 16:32 initramfs-2.6.32-431.el6.x86_64.imgdrwx------. 2 root root 12288 Aug 22 16:24 lost+found-rw-r--r--. 1 root root 193758 Nov 22 2013 symvers-2.6.32-431.el6.x86_64.gz-rw-r--r--. 1 root root 2518236 Nov 22 2013 System.map-2.6.32-431.el6.x86_64-rwxr-xr-x. 1 root root 4128368 Nov 22 2013 vmlinuz-2.6.32-431.el6.x86_64 2. 启动初始化进程内核文件加载以后，就开始运行第一个程序 /sbin/init，它的作用是初始化系统环境。 由于init是第一个运行的程序，它的进程编号（pid）就是1。其他所有进程都从它衍生，都是它的子进程。 3. 确定运行级别许多程序需要开机启动。它们在Windows叫做”服务”（service），在Linux就叫做”守护进程”（daemon）。 init进程的一大任务，就是去运行这些开机启动的程序。但是，不同的场合需要启动不同的程序，比如用作服务器时，需要启动Apache，用作桌面就不需要。Linux允许为不同的场合，分配不同的开机启动程序，这就叫做”运行级别”（runlevel）。也就是说，启动时根据”运行级别”，确定要运行哪些程序。 Linux预置七种init运行级别（0-6）： 0：关机模式 （相当于poweroff） 1：单用户模式 2：无网络支持的多用户模式 3：有网络支持的多用户模式（也就是文本模式，工作中最常用的模式） 4：保留，未使用 5：有网络支持的X-windows支持多用户模式（也就是桌面图形模式） 6: 重新引导系统，即重启（相当于reboot） init进程首先读取文件 /etc/inittab，它是运行级别的设置文件。如果打开它，可以看到第一行是这样的： 123[root@bastion-IDC ~]# cat /etc/inittab....id:3:initdefault: initdefault的值是3，表明系统启动时的运行级别为3。如果需要指定其他级别，可以手动修改这个值。 那么，运行级别3有哪些程序呢，系统怎么知道每个级别应该加载哪些程序呢？ 答案是每个运行级别在/etc目录下面，都有一个对应的子目录，指定要加载的程序。 1234567/etc/rc0.d/etc/rc1.d/etc/rc2.d/etc/rc3.d/etc/rc4.d/etc/rc5.d/etc/rc6.d 上面目录名中的”rc”，表示run command（运行程序），最后的d表示directory（目录）。下面让我们看看 /etc/rc3.d 目录中到底指定了哪些程序。 12345678910111213141516171819202122232425262728293031323334[root@bastion-IDC ~]# ll /etc/rc3.d/total 0lrwxrwxrwx. 1 root root 19 Aug 22 16:30 K10saslauthd -&gt; ../init.d/saslauthdlrwxrwxrwx. 1 root root 18 Aug 22 16:47 K15svnserve -&gt; ../init.d/svnservelrwxrwxrwx. 1 root root 15 Aug 23 16:21 K25squid -&gt; ../init.d/squidlrwxrwxrwx. 1 root root 19 Dec 23 13:14 K45memcached -&gt; ../init.d/memcachedlrwxrwxrwx. 1 root root 20 Aug 22 16:30 K50netconsole -&gt; ../init.d/netconsolelrwxrwxrwx. 1 root root 13 Dec 21 17:45 K60nfs -&gt; ../init.d/nfslrwxrwxrwx. 1 root root 20 Dec 21 17:45 K69rpcsvcgssd -&gt; ../init.d/rpcsvcgssdlrwxrwxrwx. 1 root root 17 Nov 24 14:45 K75ntpdate -&gt; ../init.d/ntpdatelrwxrwxrwx. 1 root root 20 Aug 22 16:31 K87multipathd -&gt; ../init.d/multipathdlrwxrwxrwx. 1 root root 21 Aug 22 16:30 K87restorecond -&gt; ../init.d/restorecondlrwxrwxrwx. 1 root root 15 Aug 22 16:30 K89rdisc -&gt; ../init.d/rdisclrwxrwxrwx. 1 root root 22 Aug 22 16:31 S02lvm2-monitor -&gt; ../init.d/lvm2-monitorlrwxrwxrwx. 1 root root 16 Aug 22 16:31 S07iscsid -&gt; ../init.d/iscsidlrwxrwxrwx. 1 root root 19 Aug 22 16:30 S08ip6tables -&gt; ../init.d/ip6tableslrwxrwxrwx. 1 root root 18 Aug 22 16:30 S08iptables -&gt; ../init.d/iptableslrwxrwxrwx. 1 root root 17 Aug 22 16:30 S10network -&gt; ../init.d/networklrwxrwxrwx. 1 root root 16 Aug 22 16:31 S11auditd -&gt; ../init.d/auditdlrwxrwxrwx. 1 root root 17 Aug 22 16:30 S12rsyslog -&gt; ../init.d/rsysloglrwxrwxrwx. 1 root root 15 Dec 21 17:45 S13iscsi -&gt; ../init.d/iscsilrwxrwxrwx. 1 root root 17 Dec 21 17:45 S13rpcbind -&gt; ../init.d/rpcbindlrwxrwxrwx. 1 root root 17 Dec 21 17:45 S14nfslock -&gt; ../init.d/nfslocklrwxrwxrwx. 1 root root 19 Aug 22 16:31 S15mdmonitor -&gt; ../init.d/mdmonitorlrwxrwxrwx. 1 root root 17 Dec 21 17:45 S19rpcgssd -&gt; ../init.d/rpcgssdlrwxrwxrwx. 1 root root 26 Aug 22 16:31 S25blk-availability -&gt; ../init.d/blk-availabilitylrwxrwxrwx. 1 root root 15 Aug 22 16:30 S25netfs -&gt; ../init.d/netfslrwxrwxrwx. 1 root root 19 Aug 22 16:30 S26udev-post -&gt; ../init.d/udev-postlrwxrwxrwx. 1 root root 18 Oct 25 11:49 S50onealert -&gt; ../init.d/onealertlrwxrwxrwx. 1 root root 14 Aug 22 16:31 S55sshd -&gt; ../init.d/sshdlrwxrwxrwx. 1 root root 16 Oct 26 09:47 S56xinetd -&gt; ../init.d/xinetdlrwxrwxrwx. 1 root root 17 Aug 22 16:30 S80postfix -&gt; ../init.d/postfixlrwxrwxrwx. 1 root root 15 Aug 22 16:30 S90crond -&gt; ../init.d/crondlrwxrwxrwx. 1 root root 11 Aug 22 16:30 S99local -&gt; ../rc.local 可以看到： 字母S表示Start，也就是启动的意思（启动脚本的运行参数为start）。 如果这个位置是字母K，就代表Kill（关闭），即如果从其他运行级别切换过来，需要关闭的程序（启动脚本的运行参数为stop）。 后面的两位数字表示处理顺序，数字越小越早处理，所以第一个启动的程序是motd，然后是rpcbing、nfs……数字相同时，则按照程序名的字母顺序启动，所以rsyslog会先于sudo启动。 这个目录里的所有文件（除了README），就是启动时要加载的程序。如果想增加或删除某些程序，不建议手动修改 /etc/rcN.d 目录，最好是用一些专门命令进行管理（参考这里和这里）。 4. 加载开机启动程序前面提到，七种预设的”运行级别”各自有一个目录，存放需要开机启动的程序。不难想到，如果多个”运行级别”需要启动同一个程序，那么这个程序的启动脚本，就会在每一个目录里都有一个拷贝。这样会造成管理上的困扰：如果要修改启动脚本，岂不是每个目录都要改一遍？ Linux的解决办法，就是七个 /etc/rcN.d 目录里列出的程序，都设为链接文件，指向另外一个目录 /etc/init.d，真正的启动脚本都统一放在这个目录中。init进程逐一加载开机启动程序，其实就是运行这个目录里的启动脚本。 下面就是链接文件真正的指向： 12[root@bastion-IDC ~]# ls -l /etc/rc3.dlrwxrwxrwx. 1 root root 10 Aug 22 16:30 /etc/rc3.d -&gt; rc.d/rc3.d 这样做的另一个好处，就是如果你要手动关闭或重启某个进程，直接到目录 /etc/init.d 中寻找启动脚本即可。 比如，我要重启iptables服务器，就运行下面的命令： 1[root@bastion-IDC ~]# /etc/init.d/iptables restart /etc/init.d 这个目录名最后一个字母d，是directory的意思，表示这是一个目录，用来与程序 /etc/init 区分。 5. 用户登录开机启动程序加载完毕以后，就要让用户登录了。 一般来说，用户的登录方式有三种：命令行登录、ssh登录、图形界面登录。这三种情况，都有自己的方式对用户进行认证。 1）命令行登录：init进程调用getty程序（意为get teletype），让用户输入用户名和密码。输入完成后，再调用login程序，核对密码（Debian还会再多运行一个身份核对程序/etc/pam.d/login）。如果密码正确，就从文件 /etc/passwd 读取该用户指定的shell，然后启动这个shell。 2）ssh登录：这时系统调用sshd程序（Debian还会再运行/etc/pam.d/ssh ），取代getty和login，然后启动shell。 3）图形界面登录：init进程调用显示管理器，Gnome图形界面对应的显示管理器为gdm（GNOME Display Manager），然后用户输入用户名和密码。如果密码正确，就读取/etc/gdm3/Xsession，启动用户的会话。 6. 进入 login shell所谓shell，简单说就是命令行界面，让用户可以直接与操作系统对话。用户登录时打开的shell，就叫做login shell。 Linux默认的shell是Bash，它会读入一系列的配置文件。上一步的三种情况，在这一步的处理，也存在差异。 命令行登录：首先读入 /etc/profile，这是对所有用户都有效的配置；然后依次寻找下面三个文件，这是针对当前用户的配置。 123~/.bash_profile~/.bash_login~/.profile 需要注意的是，这三个文件只要有一个存在，就不再读入后面的文件了。比如，要是 ~/.bash_profile 存在，就不会再读入后面两个文件了。 ssh登录：与第一种情况完全相同。 图形界面登录：只加载 /etc/profile 和 ~/.profile。也就是说，~/.bash_profile 不管有没有，都不会运行。 7. 打开 non-login shell老实说，上一步完成以后，Linux的启动过程就算结束了，用户已经可以看到命令行提示符或者图形界面了。但是，为了内容的完整，必须再介绍一下这一步。 用户进入操作系统以后，常常会再手动开启一个shell。这个shell就叫做 non-login shell，意思是它不同于登录时出现的那个shell，不读取/etc/profile和.profile等配置文件。 non-login shell的重要性，不仅在于它是用户最常接触的那个shell，还在于它会读入用户自己的bash配置文件 ~/.bashrc。大多数时候，我们对于bash的定制，都是写在这个文件里面的。 你也许会问，要是不进入 non-login shell，岂不是.bashrc就不会运行了，因此bash 也就不能完成定制了？ 事实上，Debian已经考虑到这个问题了，请打开文件 ~/.profile，可以看到下面的代码： 12345if [ -n &quot;$BASH_VERSION&quot; ]; then if [ -f &quot;$HOME/.bashrc&quot; ]; then . &quot;$HOME/.bashrc&quot; fifi 上面代码先判断变量 $BASH_VERSION 是否有值，然后判断主目录下是否存在 .bashrc 文件，如果存在就运行该文件。 第三行开头的那个点，是source命令的简写形式，表示运行某个文件，写成”source ~/.bashrc”也是可以的。 因此，只要运行～/.profile文件，～/.bashrc文件就会连带运行。但是上一节的第一种情况提到过，如果存在～/.bash_profile文件，那么有可能不会运行～/.profile文件。解决这个问题很简单，把下面代码写入.bash_profile就行了。 123if [ -f ~/.profile ]; then . ~/.profilefi 这样一来，不管是哪种情况，.bashrc都会执行，用户的设置可以放心地都写入这个文件了。 Bash的设置之所以如此繁琐，是由于历史原因造成的。早期的时候，计算机运行速度很慢，载入配置文件需要很长时间，Bash的作者只好把配置文件分成了几个部分，阶段性载入。系统的通用设置放在 /etc/profile，用户个人的、需要被所有子进程继承的设置放在.profile，不需要被继承的设置放在.bashrc。 顺便提一下，除了Linux以外， Mac OS X 使用的shell也是Bash。但是，它只加载.bash_profile，然后在.bash_profile里面调用.bashrc。而且，不管是ssh登录，还是在图形界面里启动shell窗口，都是如此。 附：启动流程的思维导图 图片转自：https://mm.edrawsoft.cn/template/12597 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"}]},{"title":"实用工具 | Linux 定时任务 crontab 命令详解","slug":"description-of-the-crontab-command","date":"2016-11-23T02:24:45.000Z","updated":"2021-09-24T04:03:03.936Z","comments":true,"path":"2016/11/23/description-of-the-crontab-command/","link":"","permalink":"http://dbkernel.github.io/2016/11/23/description-of-the-crontab-command/","excerpt":"","text":"本文首发于 2016-11-23 10:24:45 概述Linux 下的任务调度分为两类：系统任务调度和用户任务调度。Linux 系统任务是由 cron (crond) 这个系统服务来控制的，这个系统服务是默认启动的。用户自己设置的计划任务则使用 crontab 命令。 cron 配置文件在 Ubuntu/Debian 中，配置文件路径为 /etc/crontab（CentOS也类似），其内容为： 12345678910111213141516171819202122# /etc/crontab: system-wide crontab# Unlike any other crontab you don&#x27;t have to run the `crontab&#x27;# command to install the new version when you edit this file# and files in /etc/cron.d. These files also have username fields,# that none of the other crontabs do.SHELL=/bin/shPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed17 * * * * root cd / &amp;&amp; run-parts --report /etc/cron.hourly25 6 * * * root test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily )47 6 * * 7 root test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly )52 6 1 * * root test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly )# SHELL 环境变量用于指定系统要使用的shell，此处为/bin/sh。 PATH 环境变量指定了系统执行命令的路径。 也可以添加MAILTO变量，如果指定，则表示 crond 的任务执行信息将通过电子邮件发送给指定的用户。 其他部分在后文详细讲述。 用户定期要执行的工作，比如用户数据备份、定时邮件提醒等，都可以使用 crontab 工具来定制自己的计划任务。所有非root用户定义的 crontab 文件都被保存在 /var/spool/cron 目录中，其文件名与用户名一致。 1ls /var/spool/cron/crontabs/admin 除此之外，还有两个文件/etc/cron.deny和/etc/cron.allow，前者中可列出不允许哪些用户使用 crontab 命令，后者中可列出允许哪些用户使用 crontab 命令。 crontab 文件含义用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： 1minute hour day month week command 各字段含义如下： minute：表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符： 星号(*)：代表所有可能的值，例如 month 字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号(,)：可以用逗号隔开的值指定一个列表范围，例如：1,2,5,7,8,9 。 中杠(-)：可以用整数之间的中杠表示一个整数范围，例如：2-6 表示2,3,4,5,6 。 正斜线(/)：可以用正斜线指定时间的间隔频率，例如：0-23/2表示每两小时执行一次。同时正斜线可以和星号一起使用，例如：*/10，如果用在minute字段，表示每十分钟执行一次。 crontab命令详解命令格式： 1234567usage: crontab [-u user] file crontab [ -u user ] [ -i ] &#123; -e | -l | -r &#125; (default operation is replace, per 1003.2) -e (edit user&#x27;s crontab) -l (list user&#x27;s crontab) -r (delete user&#x27;s crontab) -i (prompt before deleting user&#x27;s crontab) -u user：用于设定某个用户的crontab服务。 file: file 为命令文件名，表示将 file 作为 crontab 的任务列表文件并载入 crontab ；如果在命令行中没有指定这个文件，crontab 命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab 。 -e：编辑某个用户的 crontab 文件内容，如不指定用户则表示当前用户。 -l：显示某个用户的 crontab 文件内容，如不指定用户则表示当前用户。 -r：从 /var/spool/cron 目录中删除某个用户的crontab文件，如不指定用户，则默认删除当前用户 crontab 文件。 -i：在删除用户的 crontab 文件时给确认提示。 crontab 注意事项 crontab有2种编辑方式：直接编辑/etc/crontab文件与crontab –e，其中/etc/crontab里的计划任务是系统的计划任务，而用户的计划任务需要通过crontab –e来编辑。 每次编辑完某个用户的 cron 设置后，cron 自动在 /var/spool/cron 下生成一个与此用户同名的文件，此用户的 cron 信息都记录在这个文件中，这个文件是不可以直接编辑的，只可以用 crontab -e 来编辑。 crontab 中的 command 尽量使用绝对路径，否则会经常因为路径错误导致任务无法执行。 新创建的 cron job 不会马上执行，至少要等2分钟才能执行，可重启 cron 来立即执行。 % 在crontab文件中表示换行，因此假如脚本或命令含有%，需要使用\\%来进行转义。 crontab -e的默认编辑器是 nano ，如需使用 vim，可在/etc/profile或~/.bashrc中添加 export EDITOR=vi 来解决。 crontab 配置示例 每分钟执行1次 command（因cron默认每1分钟扫描一次，因此全为*即可）： 1* * * * * command 每小时的第3和第15分钟执行 command ： 13,15 * * * * command 每天上午8-11点的第3和15分钟执行 command ： 13,15 8-11 * * * command 每隔2天的上午8-11点的第3和15分钟执行 command ： 13,15 8-11 */2 * * command 每个星期一的上午8点到11点的第3和第15分钟执行 command ： 13,15 8-11 * * 1 command 每晚的21:30分重启 smb ： 130 21 * * * /etc/init.d/smb restart 每月1、10、22日的 4:45 重启 smb ： 145 4 1,10,22 * * /etc/init.d/smb restart 每周六、周日的 1:10 重启 smb ： 110 1 * * 6,0 /etc/init.d/smb restart 每天 18:00 至 23:00 之间每隔30分钟重启 smb ： 10,30 18-23 * * * /etc/init.d/smb restart 每隔1小时重启 smb ： 1* */1 * * * /etc/init.d/smb restart 晚上23点到早上7点之间，每隔1小时重启 smb ： 1* 23-7/1 * * * /etc/init.d/smb restart 每月的4号与每周一到周三的11点重启 smb ： 10 11 4 * mon-wed /etc/init.d/smb restart 每小时执行/etc/cron.hourly目录内的脚本： 10 1 * * * root run-parts /etc/cron.hourly 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"},{"name":"crontab","slug":"crontab","permalink":"http://dbkernel.github.io/tags/crontab/"}]},{"title":"特性分析 | GreenPlum 的并行查询优化策略详解","slug":"greenplum-parallel-query-optimization-strategy","date":"2016-11-21T01:43:07.000Z","updated":"2021-09-24T04:06:27.294Z","comments":true,"path":"2016/11/21/greenplum-parallel-query-optimization-strategy/","link":"","permalink":"http://dbkernel.github.io/2016/11/21/greenplum-parallel-query-optimization-strategy/","excerpt":"","text":"本文首发于 2016-11-21 09:43:07 架构GreenPlum 采用 Share Nothing 的架构，良好的发挥了廉价PC的作用。自此I/O不在是 DW(data warehouse) 的瓶颈，相反网络的压力会大很多。但是 GreenPlum 的查询优化策略能够避免尽量少的网络交换。对于初次接触 GreenPlum 的人来说，肯定耳目一新。 查询优化器GreenPlum 的 master 节点负责 SQL 解析和执行计划的生成，具体来说，查询优化器会将 SQL 解析成每个节点（segments）要执行的物理执行计划。 GreenPlum 采用的是基于成本的优化策略：如果有多条执行路径，会评估执行代价，找出代价最小、最有效率的一条。 不像传统的查询优化器，GreenPlum 的查询优化器必须全局的考虑整个集群，在每个候选的执行计划中考虑到节点间移动数据的开销。比如有 join，那么 join 是在各个节点分别进行的（每个节点只和自身数据做 join），所以它的查询很快。 查询计划包括了一些传统的操作，比如：扫描、Join、排序、聚合等等。 GreenPlum 中有三种数据的移动操作： Broadcast Motion (N:N)：广播数据。每个节点向其他节点广播需要发送的数据。 Redistribute Motion (N:N)：重新分布数据。利用 join 列数据的 hash 值不同，将筛选后的数据在其他 segment 重新分布。 Gather Motion (N:1)：聚合汇总数据。每个节点将 join 后的数据发到一个单节点上，通常是发到主节点 master 。 示例示例11234567891011explain select d.*,j.customer_id from data d join jd1 j on d.partner_id=j.partner_id where j.gmt_modified&gt; current_date -80; QUERY PLAN---------------------------------------------------------------------------------------- Gather Motion 88:1 (slice2) (cost=3.01..939.49 rows=2717 width=59) -&gt; Hash Join (cost=3.01..939.49 rows=2717 width=59) Hash Cond: d.partner_id::text = j.partner_id::text -&gt; Seq Scan on data d (cost=0.00..260.74 rows=20374 width=50) -&gt; Hash (cost=1.91..1.91 rows=88 width=26) -&gt; Broadcast Motion 88:88 (slice1) (cost=0.00..1.91 rows=88 width=26) -&gt; Seq Scan on jd1 j (cost=0.00..1.02 rows=1 width=26) Filter: gmt_modified &gt; (&#x27;now&#x27;::text::date - 80) 执行计划需要自下而上分析： 在各个节点扫描自己的 jd1 表数据，按照条件过滤生成数据（记为 rs）。 各节点将自己生成的 rs 依次发送到其他节点。（Broadcast Motion (N:N)） 每个节点上的 data 表的数据，和各自节点上收到的 rs 进行 join，这样能保证本机数据只和本机数据做 join 。 各节点将 join 后的结果发送给 master(Gather Motion (N:1)) 。 由上面的执行过程可以看出， GreenPlum 将 rs 给每个含有 data 表数据的节点都发了一份。 问：如果 rs 很大或者压根就没有过滤条件，会有什么问题？如何处理？ 比如本例中的表 jd1 和表data的数据行数如下： 12345=&gt; select count(*) from jd1; count------- 20(1 row) 1234=&gt; select count(*) from data; count-------- 113367 如果 rs 很大的话，广播数据时网络就会成为瓶颈。GreenPlum 的优化器很聪明，它是将小表广播到各个 segment 上，极大的降低网络开销。从这个例子能看出统计信息对于生成好的查询计划是何等重要。 示例2下面看一个复杂点的例子： 12345678910111213141516171819select c_custkey, c_name, sum(l_extendedprice * (1 - 1_discount)) as revenue, c_acctbal, n_name, c_address, c_phone, c_commentfrom customer, orders, lineitem, nationwhere c_custkey = o_custkeyand 1_orderkey = o_orderkeyand o_orderdate &gt;= date &#x27;1994-08-01&#x27;and o_orderdate &lt; date &#x27;1994-08-0l&#x27; + interval &#x27;3 month&#x27;and l_returnflag = &#x27;R&#x27;and c_nationkey = n_nationkeygroup by c_custkey, c_name, c_acctbal, c_phone, n_name, c_address, c_commentorder by revenue desc 执行计划如下： 各个节点上同时扫描各自的 nation 表数据，将各 segment 上的 nation 数据向其他节点广播（Broadcast Motion (N:N)）。 各个节点上同时扫描各自 customer 数据，和收到的 nation 数据 join 生成RS-CN 。 各个 segment 同时扫描自己 orders 表数据，过滤数据生成 RS-O 。 各个 segment 同时扫描自己 lineitem 表数据，过滤生成 RS-L 。 各个 segment 同时将各自 RS-O 和 RS-L 进行 join，生成RS-OL。注意此过程不需要 Redistribute Motion (N:N) 重新分布数据，因为 orders 和 lineitem 的 distribute column 都是orderkey，这就保证了各自需要 join 的对象都是在各自的机器上，所以 n 个节点就开始并行 join 了。 各个节点将自己在步骤5生成的 RS-OL 按照 cust-key 在所有节点间重新分布数据（Redistribute Motion (N:N)，可以按照 hash 和 range 在节点间来重新分布数据，默认是 hash），这样每个节点都会有自己的 RS-OL 。 各个节点将自己在步骤2生成的 RS-CN 和自己节点上的 RS-OL 数据进行 join，又是本机只和本机的数据进行 join 。 聚合，排序，发往主节点 master 。 总结Greenplum如何处理和优化一张大表和小表的join? Greenplum是选择将小表广播数据，而不是将大表广播。 举例说明： 表 A 有10亿条数据（empno&lt;pk&gt;,deptno,ename），表 B 有500条数据（deptno&lt;pk&gt;,dname,loc） 表 A 与表 B join on deptno 集群有11个节点：1个 master，10个 segment 按照正常的主键列 hash 分布，每个 segment 节点上只会有 1/10 的表 A 和 1/10 的表 B。 此时 GreenPlum 会让所有节点给其他节点发送各自所拥有的小表 B 的1/10的数据，这样就保证了10个节点上，每个节点都有一份完整的表 B 的数据。此时，每个节点上1/10的 A 只需要和自己节点上的 B 进行 join 就OK。所以 GreenPlum 并行处理能力惊人的原因就在这里。 最终所有节点会将 join 的结果都发给主节点 master。 由该例可见统计信息十分重要，GreenPlum 通过统计信息来确定将哪张表进行（Broadcast Motion (N:N)）。 另外，实际使用中还会出现列值倾斜的情况，比如 A 没有按照主键来 hash 分布，而是人为指定按照 deptno 的 hash 在各个节点上分布数据。若 A 中80%的数据都是sales（deptno=10）部门的，此时10个节点中，就会有一个节点上拥有了 10亿×80% 的数据，就算是将表 B 广播到其他节点 也无济于事，因为计算的压力都集中在一台机器了。所以，必须选择合适的列进行hash分布。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/categories/GreenPlum/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/tags/GreenPlum/"},{"name":"并行查询","slug":"并行查询","permalink":"http://dbkernel.github.io/tags/%E5%B9%B6%E8%A1%8C%E6%9F%A5%E8%AF%A2/"},{"name":"优化器","slug":"优化器","permalink":"http://dbkernel.github.io/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"}]},{"title":"问题定位 | PostgreSQL 报错 requested WAL segment has already been removed","slug":"postgresql-error-wal-segment-has-already-been-removed","date":"2016-04-25T12:59:52.000Z","updated":"2021-09-22T15:24:33.122Z","comments":true,"path":"2016/04/25/postgresql-error-wal-segment-has-already-been-removed/","link":"","permalink":"http://dbkernel.github.io/2016/04/25/postgresql-error-wal-segment-has-already-been-removed/","excerpt":"","text":"本文首发于 2016-04-25 20:59:52 问题描述在使用配置了热备的 PostgreSQL 数据库时，在执行大量事务时，尤其是一个需要插入几千万条数据的 insert 事务时（典型的做法是持续 insert into t select * from t;），后台 csv log 中报错如下： 1232015-07-01 13:25:29.430 CST,,,27738,,51d112c8.6c5a,1,,2015-07-01 13:25:28 CST,,0,LOG,00000,&quot;streaming replication successfully connected to primary&quot;,,,,,,,,&quot;libpqrcv_connect, libpqwalreceiver.c:171&quot;,&quot;&quot;2015-07-01 13:25:29.430 CST,,,27738,,51d112c8.6c5a,2,,2015-07-01 13:25:28 CST,,0,FATAL,XX000,&quot;could not receive data from WAL stream:FATAL: requested WAL segment 0000000800002A0000000000 has already been removed&quot;,,,,,,,,&quot;libpqrcv_receive, libpqwalreceiver.c:389&quot;,&quot;&quot; 问题分析根据报错信息分析，推测是主库大事务产生了大量 xlog，这是因为 PostgreSQL 在执行事务过程中，直到提交时才会发送到备库。 由于该事务需要执行的时间过长，超过了 checkpoint 的默认间隔，所以导致有的 xlog 还未发送到备库却被 remove 掉了。 解决方法要解决该问题，一般可用的方案有： 方法一：调大参数 wal_keep_segments 的值将 GUC 参数 wal_keep_segments 设大一些，比如设置为2000，而每个 segment 默认值为16MB，就相当于有 32000MB，那么，最多可保存 30GB 的 xlog ，超过则删除最早的 xlog 。 不过，该方法并不能从根本上解决该问题。毕竟，在生产环境中或TPCC等测试灌数时，如果某条事务需要插入几十亿条记录，有可能还是会出现该问题。 方法二：启用归档归档，就是将未发送到备库的 xlog 备份到某个目录下，待重启数据库时再将其恢复到备库中去。 GUC 参数设置示例如下： 主库的 postgresql.conf 文件中： 123456wal_level = hot_standbyarchive_mode = onarchive_command = &#x27;rsync -zaq %p postgres@pg-slave:/var/lib/pgsql/wal_restore/%f &amp;&amp; test ! -f /var/lib/pgsql/backup/wal_archive/%f &amp;&amp; cp %p /var/lib/pgsql/backup/wal_archive/&#x27;archive_timeout = 300max_wal_senders = 5wal_keep_segments = 0 备库的 postgresql.conf 文件中： 12345wal_level = hot_standbyarchive_mode = onarchive_command = &#x27;test ! -f /var/lib/pgsql/backup/wal_archive/%f &amp;&amp; cp -i %p /var/lib/pgsql/backup/wal_archive/%f &lt; /dev/null&#x27;hot_standby = onwal_keep_segments = 1 备库的 recovery.conf 文件中： 1234standby_mode = &#x27;on&#x27;primary_conninfo = &#x27;host=pg-master port=5432 user=replicator&#x27;restore_command = &#x27;cp /var/lib/psql/wal_restore/%f %p&#x27;archive_cleanup_command = &#x27;pg_archivecleanup /var/lib/pgsql/wal_restore/ %r&#x27; 方法三：启用 replication slot（PG 9.4 开始支持）该方法是根本解决方法，不会造成xlog的丢失。也就是说，在 xlog 被拷贝到从库之前，主库不会删除。 启用方法： 在 postgresql.conf 中添加： 1max_replication_slots = 2000 在拷贝到备库之前，主库要创建一个 slot： 12345678910postgres=# SELECT * FROM pg_create_physical_replication_slot(&#x27;node_a_slot&#x27;); slot_name | xlog_position-------------+--------------- node_a_slot |postgres=# SELECT * FROM pg_replication_slots; slot_name | slot_type | datoid | database | active | xmin | restart_lsn-------------+-----------+--------+----------+--------+------+------------- node_a_slot | physical | | | f | |(1 row) 在备库的 recovery.conf 文件中添加一行： 123standby_mode = &#x27;on&#x27;primary_conninfo = &#x27;host=192.168.4.225 port=19000 user=wslu password=xxxx&#x27;primary_slot_name = &#x27;node_a_slot&#x27; 参考https://www.postgresql.org/docs/9.4/static/runtime-config-replication.html https://www.postgresql.org/docs/9.4/static/warm-standby.html#CASCADING-REPLICATIONhttp://blog.2ndquadrant.com/postgresql-9-4-slots/ http://grokbase.com/t/postgresql/pgsql-general/13654jchy3/trouble-with-replication http://stackoverflow.com/questions/28201475/how-do-i-fix-a-postgresql-9-3-slave-that-cannot-keep-up-with-the-master 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"问题定位","slug":"问题定位","permalink":"http://dbkernel.github.io/tags/%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"name":"WAL","slug":"WAL","permalink":"http://dbkernel.github.io/tags/WAL/"}]},{"title":"源码分析 | 使用 gcov 和 lcov 测试 PostgreSQL 代码覆盖率","slug":"test-postgresql-code-coverage-using-gcov-and-lcov","date":"2016-03-30T07:34:34.000Z","updated":"2021-09-24T05:23:45.713Z","comments":true,"path":"2016/03/30/test-postgresql-code-coverage-using-gcov-and-lcov/","link":"","permalink":"http://dbkernel.github.io/2016/03/30/test-postgresql-code-coverage-using-gcov-and-lcov/","excerpt":"","text":"本文首发于 2016-03-30 15:34:34 引言通常我们评判一个 test case 好坏的标准之一是代码的覆盖率，一个好的 test case 应该覆盖到所有的代码。 那么问题来了，我们怎么知道这个 test case 有没有覆盖到所有的代码呢？ 以 PostgreSQL 为例，我们看看如何检测 C 语言程序的代码覆盖率。 C 代码覆盖率测试，需要用到 gcc 的配套工具gcov，还有一个可视化工具lcov。 1. 安装依赖首先需要安装依赖 gcov 和 lcov 。 gcov 在 gcc 包中已经包含了，lcov 是 ltp 的一个 gcov 扩展插件，用来产生HTML报告。 1sudo apt install lcov 2. 编译、安装 PG2.1. 编译选项介绍首先介绍一下 PostgreSQL 的编译选项 --enable-coverage： 1--enable-coverage build with coverage testing instrumentation 这个编译项对应gcc的两个参数：-fprofile-arcs 和 -ftest-coverage。 12345678# enable code coverage if --enable-coverageif test &quot;$enable_coverage&quot; = yes; then if test &quot;$GCC&quot; = yes; then CFLAGS=&quot;$CFLAGS -fprofile-arcs -ftest-coverage&quot; else as_fn_error $? &quot;--enable-coverage is supported only when using GCC&quot; &quot;$LINENO&quot; 5 fifi 通过man gcc查看这两个参数的含义： 1234567891011121314151617181920212223242526272829-fprofile-arcs Add code so that program flow arcs are instrumented. During execution the program records how many times each branch and call is executed and how many times it is taken or returns. When the compiled program exits it saves this data to a file called auxname.gcda for each source file. The data may be used for profile-directed optimizations (-fbranch-probabilities), or for test coverage analysis (-ftest-coverage). Each object file&#x27;s auxname is generated from the name of the output file, if explicitly specified and it is not the final executable, otherwise it is the basename of the source file. In both cases any suffix is removed (e.g. foo.gcda for input file dir/foo.c, or dir/foo.gcda for output file specified as -o dir/foo.o).--coverage This option is used to compile and link code instrumented for coverage analysis. The option is a synonym for -fprofile-arcs -ftest-coverage (when compiling) and -lgcov (when linking). See the documentation for those options for more details. * Compile the source files with -fprofile-arcs plus optimization and code generation options. For test coverage analysis, use the additional -ftest-coverage option. You do not need to profile every source file in a program. * Link your object files with -lgcov or -fprofile-arcs (the latter implies the former). * Run the program on a representative workload to generate the arc profile information. This may be repeated any number of times. You can run concurrent instances of your program, and provided that the file system supports locking, the data files will be correctly updated. Also &quot;fork&quot; calls are detected and correctly handled (double counting will not happen). * For profile-directed optimizations, compile the source files again with the same optimization and code generation options plus -fbranch-probabilities. * For test coverage analysis, use gcov to produce human readable information from the .gcno and .gcda files. Refer to the gcov documentation for further information. With -fprofile-arcs, for each function of your program GCC creates a program flow graph, then finds a spanning tree for the graph. Only arcs that are not on the spanning tree have to be instrumented: the compiler adds code to count the number of times that these arcs are executed. When an arc is the only exit or only entrance to a block, the instrumentation code can be added to the block; otherwise, a new basic block must be created to hold the instrumentation code.-ftest-coverage Produce a notes file that the gcov code-coverage utility can use to show program coverage. Each source file&#x27;s note file is called auxname.gcno. Refer to the -fprofile-arcs option above for a description of auxname and instructions on how to generate test coverage data. Coverage data matches the source files more closely if you do not optimize. -fprofile-arcs： -fprofile-arcs 用于产生 .c 文件对应的 .gcda 文件，.gcda 文件可以被用于 profile 驱动的优化，或者结合 gcov 来做代码覆盖分析。 编译时尽量不要使用 -O 优化，这样代码覆盖数据 .gcda 才能尽可能和代码接近。 当代码被调用时，.gcda 文件中对应的计数器会被修改，记录代码被调用的次数。 -ftest-coverage： -ftest-coverage 这个选项用于产生 .c 文件的 .gcno 文件。这个文件生成后不会被修改。结合 .gcda，可以分析测试代码覆盖率。 2.2. 编译安装123./configure --prefix=/opt/pgsql9.4.4 --with-pgport=1921 --with-perl --with-python --with-tcl --with-openssl --with-pam --with-ldap --with-libxml --with-libxslt --enable-thread-safety --enable-debug --enable-dtrace --enable-coveragegmake world &amp;&amp; gmake install-world 安装好后，我们会发现在源码目录中多了一些.gcda和.gcno的文件，每个.c文件都会对应这两个文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748postgres@wslu-&gt; lltotal 1.3M-rw-r--r-- 1 postgres postgres 22K Jun 10 03:29 gistbuildbuffers.c-rw------- 1 postgres postgres 1.6K Sep 7 14:42 gistbuildbuffers.gcda-rw-r--r-- 1 postgres postgres 15K Sep 7 14:38 gistbuildbuffers.gcno-rw-r--r-- 1 postgres postgres 70K Sep 7 14:38 gistbuildbuffers.o-rw-r--r-- 1 postgres postgres 37K Jun 10 03:29 gistbuild.c-rw------- 1 postgres postgres 2.2K Sep 7 14:42 gistbuild.gcda-rw-r--r-- 1 postgres postgres 20K Sep 7 14:38 gistbuild.gcno-rw-r--r-- 1 postgres postgres 92K Sep 7 14:38 gistbuild.o-rw-r--r-- 1 postgres postgres 43K Jun 10 03:29 gist.c-rw------- 1 postgres postgres 3.1K Sep 7 14:42 gist.gcda-rw-r--r-- 1 postgres postgres 29K Sep 7 14:38 gist.gcno-rw-r--r-- 1 postgres postgres 16K Jun 10 03:29 gistget.c-rw------- 1 postgres postgres 1.3K Sep 7 14:42 gistget.gcda-rw-r--r-- 1 postgres postgres 13K Sep 7 14:38 gistget.gcno-rw-r--r-- 1 postgres postgres 74K Sep 7 14:38 gistget.o-rw-r--r-- 1 postgres postgres 101K Sep 7 14:38 gist.o-rw-r--r-- 1 postgres postgres 39K Jun 10 03:29 gistproc.c-rw------- 1 postgres postgres 3.1K Sep 7 14:42 gistproc.gcda-rw-r--r-- 1 postgres postgres 31K Sep 7 14:38 gistproc.gcno-rw-r--r-- 1 postgres postgres 79K Sep 7 14:38 gistproc.o-rw-r--r-- 1 postgres postgres 9.1K Jun 10 03:29 gistscan.c-rw------- 1 postgres postgres 848 Sep 7 14:42 gistscan.gcda-rw-r--r-- 1 postgres postgres 6.7K Sep 7 14:38 gistscan.gcno-rw-r--r-- 1 postgres postgres 60K Sep 7 14:38 gistscan.o-rw-r--r-- 1 postgres postgres 24K Jun 10 03:29 gistsplit.c-rw------- 1 postgres postgres 1.5K Sep 7 14:42 gistsplit.gcda-rw-r--r-- 1 postgres postgres 15K Sep 7 14:38 gistsplit.gcno-rw-r--r-- 1 postgres postgres 68K Sep 7 14:38 gistsplit.o-rw-r--r-- 1 postgres postgres 21K Jun 10 03:29 gistutil.c-rw------- 1 postgres postgres 2.2K Sep 7 14:42 gistutil.gcda-rw-r--r-- 1 postgres postgres 20K Sep 7 14:38 gistutil.gcno-rw-r--r-- 1 postgres postgres 84K Sep 7 14:38 gistutil.o-rw-r--r-- 1 postgres postgres 7.1K Jun 10 03:29 gistvacuum.c-rw------- 1 postgres postgres 784 Sep 7 14:42 gistvacuum.gcda-rw-r--r-- 1 postgres postgres 7.3K Sep 7 14:38 gistvacuum.gcno-rw-r--r-- 1 postgres postgres 56K Sep 7 14:38 gistvacuum.o-rw-r--r-- 1 postgres postgres 14K Jun 10 03:29 gistxlog.c-rw------- 1 postgres postgres 1.2K Sep 7 14:42 gistxlog.gcda-rw-r--r-- 1 postgres postgres 12K Sep 7 14:38 gistxlog.gcno-rw-r--r-- 1 postgres postgres 50K Sep 7 14:38 gistxlog.o-rw-r--r-- 1 postgres postgres 538 Jun 10 03:29 Makefile-rw-r--r-- 1 postgres postgres 357 Sep 7 14:38 objfiles.txt-rw-r--r-- 1 postgres postgres 20K Jun 10 03:29 READMEpostgres@wslu-&gt; pwd/opt/soft_bak/postgresql-9.4.4/src/backend/access/gist 注意事项： 源码文件目录的权限需要改为数据库启动用户的权限，否则无法修改 .gcda 的值，也就无法获取代码被调用的次数了。 1root@wslu-&gt; chown -R postgres:postgres /opt/soft_bak/postgresql-9.4.4 接下来我们看看文件的变化，以 dbsize.c 中的两个获取 pg_database_size 的 C 函数为例： 12345postgres@wslu-&gt; ls -la|grep dbsize-rw-r--r-- 1 postgres postgres 19342 Jun 10 03:29 dbsize.c-rw------- 1 postgres postgres 2664 Sep 7 15:01 dbsize.gcda-rw-r--r-- 1 postgres postgres 23272 Sep 7 14:38 dbsize.gcno-rw-r--r-- 1 postgres postgres 89624 Sep 7 14:38 dbsize.o 调用一次： 1234567891011121314postgres@wslu-&gt; psqlpsql (9.4.4)Type &quot;help&quot; for help.postgres=# select pg_database_size(oid) from pg_database; pg_database_size------------------ 6898180 6889988 24742560 6898180 6898180 6898180(6 rows)postgres=# \\q 再次查看： 12345postgres@wslu-&gt; ls -la|grep dbsize-rw-r--r-- 1 postgres postgres 19342 Jun 10 03:29 dbsize.c-rw------- 1 postgres postgres 2664 Sep 7 15:12 dbsize.gcda-rw-r--r-- 1 postgres postgres 23272 Sep 7 14:38 dbsize.gcno-rw-r--r-- 1 postgres postgres 89624 Sep 7 14:38 dbsize.o dbsize.gcda 文件的修改时间发送了变化，说明刚才我们调用pg_database_size(oid) 时，调用了 dbsize.c 中的代码。对应的行计数器会发生变化。 3. 生成 HTML 报告1234567891011121314151617181920212223242526272829303132333435363738394041424344$ mkdir html$ cd html$ lcov --directory /opt/soft_bak/postgresql-9.4.4 --capture --output-file ./app.info# 如果你不需要所有的代码，修改以上目录即可，譬如只看 contrib 下面的代码覆盖率。$ genhtml ./app.infopostgres@wslu-&gt; lltotal 3.7Mdrwxrwxr-x 12 postgres postgres 4.0K Sep 7 15:02 access-rw-rw-r-- 1 postgres postgres 141 Sep 7 15:02 amber.png-rw-rw-r-- 1 postgres postgres 3.4M Sep 7 15:02 app.infodrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 bootstrapdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 catalogdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 commands-rw-rw-r-- 1 postgres postgres 141 Sep 7 15:02 emerald.pngdrwxrwxr-x 2 postgres postgres 12K Sep 7 15:02 executordrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 foreign-rw-rw-r-- 1 postgres postgres 9.7K Sep 7 15:02 gcov.css-rw-rw-r-- 1 postgres postgres 167 Sep 7 15:02 glass.png-rw-rw-r-- 1 postgres postgres 57K Sep 7 15:02 index.html-rw-rw-r-- 1 postgres postgres 57K Sep 7 15:02 index-sort-f.html-rw-rw-r-- 1 postgres postgres 57K Sep 7 15:02 index-sort-l.htmldrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 libdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 libpqdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 maindrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 nodesdrwxrwxr-x 3 postgres postgres 4.0K Sep 7 15:02 optdrwxrwxr-x 7 postgres postgres 4.0K Sep 7 15:02 optimizerdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 parserdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 portdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 postmasterdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 regexdrwxrwxr-x 3 postgres postgres 4.0K Sep 7 15:02 replicationdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 rewrite-rw-rw-r-- 1 postgres postgres 141 Sep 7 15:02 ruby.pngdrwxrwxr-x 3 postgres postgres 4.0K Sep 7 15:02 snowball-rw-rw-r-- 1 postgres postgres 141 Sep 7 15:02 snow.pngdrwxrwxr-x 10 postgres postgres 4.0K Sep 7 15:02 storagedrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 tcopdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 15:02 tsearch-rw-rw-r-- 1 postgres postgres 117 Sep 7 15:02 updown.pngdrwxrwxr-x 3 postgres postgres 4.0K Sep 7 15:02 usrdrwxrwxr-x 14 postgres postgres 4.0K Sep 7 15:02 utils 4. 查看报告浏览器中打开 index.html 即可查看。 后记PostgreSQL 其实已经在 Makefile 提供了生成代码覆盖 HTML 的 target 。 1[root@wslu postgresql-9.4.4]# make coverage-html 产生的html目录如下： 123456789101112131415[root@wslu postgresql-9.4.4]# cd coverage[root@wslu coverage]# lltotal 224-rw-r--r-- 1 root root 141 Sep 7 19:17 amber.png-rw-r--r-- 1 root root 141 Sep 7 19:17 emerald.png-rw-r--r-- 1 root root 9893 Sep 7 19:17 gcov.css-rw-r--r-- 1 root root 167 Sep 7 19:17 glass.png-rw-r--r-- 1 root root 58737 Sep 7 19:18 index.html-rw-r--r-- 1 root root 58730 Sep 7 19:18 index-sort-f.html-rw-r--r-- 1 root root 58730 Sep 7 19:18 index-sort-l.html-rw-r--r-- 1 root root 141 Sep 7 19:17 ruby.png-rw-r--r-- 1 root root 141 Sep 7 19:17 snow.pngdrwxr-xr-x 11 root root 4096 Sep 7 19:18 src-rw-r--r-- 1 root root 117 Sep 7 19:17 updown.pngdrwxr-xr-x 3 root root 4096 Sep 7 19:18 usr 每次对代码改动后，执行完 make check 或其他回归测试手段后，就可以执行 make coverage-html 了。 参考链接 Magus Test Archive lcov lcov readme GitHub - linux-test-project/ltp: Linux Test Project http://linux-test-project.github.io/ Gcov (Using the GNU Compiler Collection (GCC)) CodeCoverage - PostgreSQL wiki PostgreSQL: Documentation: devel: 33.5. Test Coverage Examination 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"gcov","slug":"gcov","permalink":"http://dbkernel.github.io/tags/gcov/"},{"name":"lcov","slug":"lcov","permalink":"http://dbkernel.github.io/tags/lcov/"},{"name":"测试","slug":"测试","permalink":"http://dbkernel.github.io/tags/%E6%B5%8B%E8%AF%95/"}]},{"title":"源码分析 | PostgreSQL 回归测试详解","slug":"postgresql-regression-test-details","date":"2016-03-30T07:29:35.000Z","updated":"2021-09-22T15:24:33.492Z","comments":true,"path":"2016/03/30/postgresql-regression-test-details/","link":"","permalink":"http://dbkernel.github.io/2016/03/30/postgresql-regression-test-details/","excerpt":"","text":"本文首发于 2016-03-30 15:29:35 背景回归测试是 PostgreSQL 的测试方法之一。 回归测试，需要事先定义好测试脚本（通常是 SQL 脚本，放在 sql 目录中），同时定义好调用执行测试脚本的预期正确输出文件（通常放在 expected 目录中)。 测试使用 make check 或 make installcheck 进行，它会通过 pg_regress 程序调用 sql 目录中的 SQL，并收集输出结果（通常放到 results 目录中），最后 pg_regress 会对 expected 目录和 results 目录中的文件使用 diff 进行一一比较。 如果比较发现文件内容不一致，会将不一致的结果输出到 regression.diffs 文件中，并返回这个 TEST CASE failed。 但是这种测试方法实际上有一些需要注意的地方，例如我们使用不同的本地化设置，时区可能得到的结果和期望的结果就不一样。另外有些不可预知的结果，例如随机值，数据的顺序，执行计划和优化器相关参数有关。这些因素都可能导致测试结果和预期不一致，那么我们就需要人为去修复这种 failed。 PostgreSQL 的主代码测试文件在 src/test/regress 目录中。 这个目录的结构如下： 1234567891011121314151617181920postgres@digoal-&gt; ll -rttotal 1.2M-rw-r--r-- 1 postgres postgres 579 Jun 10 03:29 standby_schedule # 测试standby的调度配置, 其实就是调度sql里的文件名-rw-r--r-- 1 postgres postgres 2.3K Jun 10 03:29 serial_schedule # 串行测试的调度配置-rw-r--r-- 1 postgres postgres 937 Jun 10 03:29 resultmap # 不同的测试平台的结果映射文件，因为不同平台某些测试结果可能不相同，所以一个expected文件不能支持所有的平台。例如浮点数测试。-rwxr-xr-x 1 postgres postgres 4.4K Jun 10 03:29 regressplans.sh-rw-r--r-- 1 postgres postgres 20K Jun 10 03:29 regress.c-rw-r--r-- 1 postgres postgres 159 Jun 10 03:29 README-rw-r--r-- 1 postgres postgres 2.7K Jun 10 03:29 pg_regress_main.c-rw-r--r-- 1 postgres postgres 1.6K Jun 10 03:29 pg_regress.h-rw-r--r-- 1 postgres postgres 69K Jun 10 03:29 pg_regress.c-rw-r--r-- 1 postgres postgres 3.6K Jun 10 03:29 parallel_schedule # 并行测试的调度配置-rw-r--r-- 1 postgres postgres 624 Jun 10 03:29 Makefile-rw-r--r-- 1 postgres postgres 5.6K Jun 10 03:29 GNUmakefiledrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 outputdrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 inputdrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 data 一些测试数据drwxrwxrwx 2 postgres postgres 4.0K Sep 7 14:51 sql # 测试用到的SQLdrwxrwxr-x 2 postgres postgres 4.0K Sep 7 14:52 results # 通过pg_regress调用sql目录中的脚本，得到的结果drwxrwxrwx 2 postgres postgres 4.0K Sep 7 14:51 expected # 执行sql目录中的文件对应的正确返回结果 上层目录结构如下，其中包含了一些其他的测试目标，例如隔离级别的测试，本地化测试，性能测试，线程安全测试。等。 123456789101112postgres@digoal-&gt; cd /opt/soft_bak/postgresql-9.4.4/src/testpostgres@digoal-&gt; lltotal 36Kdrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 examplesdrwxrwxrwx 4 postgres postgres 4.0K Jun 10 03:41 isolationdrwxrwxrwx 6 postgres postgres 4.0K Jun 10 03:38 locale-rw-r--r-- 1 postgres postgres 389 Jun 10 03:29 Makefiledrwxrwxrwx 4 postgres postgres 4.0K Jun 10 03:38 mbdrwxrwxrwx 4 postgres postgres 4.0K Jun 10 03:38 performancedrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 perldrwxrwxrwx 10 postgres postgres 4.0K Sep 7 19:17 regressdrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 thread 接下来我们看看 PostgreSQL 的回归测试程序 pg_regress 的用法，它不会安装到 PGHOME/bin 中，只在 src/test/regress 中存在。 123456789101112131415161718192021222324252627282930313233343536373839404142434445$ cd src/test/regress$ src/test/regress/pg_regress --helpPostgreSQL regression test driverUsage: pg_regress [OPTION]... [EXTRA-TEST]...Options: --config-auth=DATADIR update authentication settings for DATADIR --create-role=ROLE create the specified role before testing --dbname=DB use database DB (default &quot;regression&quot;) --debug turn on debug mode in programs that are run --dlpath=DIR look for dynamic libraries in DIR --encoding=ENCODING use ENCODING as the encoding --inputdir=DIR take input files from DIR (default &quot;.&quot;) --launcher=CMD use CMD as launcher of psql --load-extension=EXT load the named extension before running the tests; can appear multiple times --load-language=LANG load the named language before running the tests; can appear multiple times --max-connections=N maximum number of concurrent connections (default is 0, meaning unlimited) --outputdir=DIR place output files in DIR (default &quot;.&quot;) --schedule=FILE use test ordering schedule from FILE (can be used multiple times to concatenate) --temp-install=DIR create a temporary installation in DIR --use-existing use an existing installationOptions for &quot;temp-install&quot; mode: --extra-install=DIR additional directory to install (e.g., contrib) --no-locale use C locale --port=PORT start postmaster on PORT --temp-config=FILE append contents of FILE to temporary config --top-builddir=DIR (relative) path to top level build directoryOptions for using an existing installation: --host=HOST use postmaster running on HOST --port=PORT use postmaster running at PORT --user=USER connect as USER --psqldir=DIR use psql in DIR (default: configured bindir)The exit status is 0 if all tests passed, 1 if some tests failed, and 2if the tests could not be run for some reason.Report bugs to &lt;pgsql-bugs@postgresql.org&gt;. 回归测试用法在 PostgreSQL 源码根目录，或者源码的 regress 目录中执行如下： 12make check # 测试时需要初始化数据库集群make installcheck # 使用以及启动的数据库集群测试，不需要初始化数据库集群 以下同时测试主代码以及 contrib 的代码： 12make check-worldmake installcheck-world 如果要使用自定义的 diff 参数，可以设置一个环境变量，例如：make check PG_REGRESS_DIFF_OPTS=&#39;-u&#39;。 同时我们还可以使用不同的 LOCALE 进行测试。例如： 123make check LANG=de_DE.utf8make check NO_LOCALE=1make check LANG=C ENCODING=EUC_JP 当我们要测试调度中不包含的测试 SQL 时，可以使用 EXTRA_TESTS 参数，至于这些脚本为什么默认不包含在调度中，可能是因为这些 SQL 脚本可能对平台的依赖比较严重，所以没有放到默认的测试中。例如： 12make check EXTRA_TESTS=collate.linux.utf8 LANG=en_US.utf8make check EXTRA_TESTS=numeric_big 接下来我们看看调度文件以及 sql 脚本目录： 123456789101112131415postgres@digoal-&gt; pwd/opt/soft_bak/postgresql-9.4.4/src/test/regresspostgres@digoal-&gt; less serial_schedule# src/test/regress/serial_schedule# This should probably be in an order similar to parallel_schedule.test: tablespacetest: booleantest: chartest: nametest: varchartest: texttest: int2test: int4test: int8...... 并行调度： 1234567891011121314151617postgres@digoal-&gt; less parallel_schedule# ----------# src/test/regress/parallel_schedule## By convention, we put no more than twenty tests in any one parallel group;# this limits the number of connections needed to run the tests.# ----------# run tablespace by itself, and first, because it forces a checkpoint;# we&#x27;d prefer not to have checkpoints later in the tests because that# interferes with crash-recovery testing.test: tablespace# ----------# The first group of parallel tests# ----------test: boolean char name varchar text int2 int4 int8 oid float4 float8 bit numeric txid uuid enum money rangetypes pg_lsn regproc...... 调度文件的 test: 后面跟的就是sql目录下的文件名(不含 .sql 后缀)。 123456789101112131415postgres@digoal-&gt; less sql/total 1940drwxrwxrwx 2 postgres postgres 4096 Sep 7 14:51 ./drwxrwxrwx 10 postgres postgres 4096 Sep 7 22:34 ../-rw-r--r-- 1 postgres postgres 2237 Jun 10 03:29 abstime.sql-rw-r--r-- 1 postgres postgres 4097 Jun 10 03:29 advisory_lock.sql-rw-r--r-- 1 postgres postgres 20295 Jun 10 03:29 aggregates.sql-rw-r--r-- 1 postgres postgres 24882 Jun 10 03:29 alter_generic.sql-rw-r--r-- 1 postgres postgres 54461 Jun 10 03:29 alter_table.sql-rw-r--r-- 1 postgres postgres 17244 Jun 10 03:29 arrays.sql-rw-r--r-- 1 postgres postgres 594 Jun 10 03:29 async.sql-rw-r--r-- 1 postgres postgres 1365 Jun 10 03:29 bitmapops.sql-rw-r--r-- 1 postgres postgres 6406 Jun 10 03:29 bit.sql-rw-r--r-- 1 postgres postgres 4164 Jun 10 03:29 boolean.sql...... 所以前面提到的 EXTRA_TESTS 实际上也是 sql 目录中的文件名(不带 .sql 后缀)。 12make check EXTRA_TESTS=collate.linux.utf8 LANG=en_US.utf8make check EXTRA_TESTS=numeric_big 来实际的试一下吧： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546postgres@digoal-&gt; pwd/opt/soft_bak/postgresql-9.4.4/src/test/regresspostgres@digoal-&gt; make installcheck-parallel //并行测试，使用已经开启的现有的数据库集群make -C ../../../src/port all......../../../src/test/regress/pg_regress --inputdir=. --psqldir=&#x27;/opt/pgsql9.4.4/bin&#x27; --dlpath=. --schedule=./parallel_schedule(using postmaster on /data01/pg_root_1921, port 1921)============== dropping database &quot;regression&quot; ==============DROP DATABASE============== creating database &quot;regression&quot; ==============CREATE DATABASEALTER DATABASE============== running regression test queries ==============test tablespace ... ok......parallel group (19 tests): limit conversion sequence returning without_oid polymorphism copy2 xml prepare plancache rowtypes temp domain with truncate largeobject rangefuncs alter_table plpgsql plancache ... ok limit ... ok plpgsql ... ok copy2 ... ok temp ... ok domain ... ok rangefuncs ... FAILED prepare ... ok without_oid ... ok conversion ... ok truncate ... ok alter_table ... ok sequence ... ok polymorphism ... FAILED rowtypes ... ok returning ... ok largeobject ... ok with ... FAILED xml ... oktest stats ... ok......========================= 22 of 145 tests failed.=========================The differences that caused some tests to fail can be viewed in thefile &quot;/opt/soft_bak/postgresql-9.4.4/src/test/regress/regression.diffs&quot;. A copy of the test summary that you seeabove is saved in the file &quot;/opt/soft_bak/postgresql-9.4.4/src/test/regress/regression.out&quot;.make: *** [installcheck-parallel] Error 1 有些测试失败了，diff 文件已经输出到 /opt/soft_bak/postgresql-9.4.4/src/test/regress/regression.diffs，我们可以查看一下看看为什么测试结果和预期结果不一致。 12345678910111213141516171819202122232425262728293031postgres@digoal-&gt; less regression.diffs*** /opt/soft_bak/postgresql-9.4.4/src/test/regress/expected/pg_lsn.out 2015-06-10 03:29:38.000000000 +0800--- /opt/soft_bak/postgresql-9.4.4/src/test/regress/results/pg_lsn.out 2015-09-07 22:45:04.413922536 +0800****************** 72,92 **** generate_series(1, 5) k WHERE i &lt;= 10 AND j &gt; 0 AND j &lt;= 10 ORDER BY f;! QUERY PLAN! --------------------------------------------------------------------------! Sort! Sort Key: (((((i.i)::text || &#x27;/&#x27;::text) || (j.j)::text))::pg_lsn)! -&gt; HashAggregate! Group Key: ((((i.i)::text || &#x27;/&#x27;::text) || (j.j)::text))::pg_lsn -&gt; Nested Loop -&gt; Function Scan on generate_series k! -&gt; Materialize! -&gt; Nested Loop! -&gt; Function Scan on generate_series j! Filter: ((j &gt; 0) AND (j &lt;= 10))! -&gt; Function Scan on generate_series i! Filter: (i &lt;= 10)! (12 rows) SELECT DISTINCT (i || &#x27;/&#x27; || j)::pg_lsn f FROM generate_series(1, 10) i,--- 72,90 ---- generate_series(1, 5) k WHERE i &lt;= 10 AND j &gt; 0 AND j &lt;= 10 ORDER BY f;...... 对于主代码，如果我们需要自定义测试 SQL，我们可以修改 regress/sql 目录下的文件，或者新增文件。同时修改 regress/expected 目录下的对应期望文件，或者现在期望文件。 如果是新增文件的情况，我们还需要修改调度文件 regress/serial_schedule和regress/parallel_schedule，把测试加入调度。 最后，再以 ltree 插件为例，看看如何配置一个外加插件的回归测试。 ltree 的源码目录： 1234567891011121314151617181920212223postgres@digoal-&gt; cd contrib/postgres@digoal-&gt; cd ltree/postgres@digoal-&gt; ll -rttotal 1.1M-rw-r--r-- 1 postgres postgres 517 Jun 10 03:29 Makefile-rw-r--r-- 1 postgres postgres 2.4K Jun 10 03:29 ltxtquery_op.c-rw-r--r-- 1 postgres postgres 11K Jun 10 03:29 ltxtquery_io.c-rw-r--r-- 1 postgres postgres 7.9K Jun 10 03:29 ltree--unpackaged--1.0.sql-rw-r--r-- 1 postgres postgres 994 Jun 10 03:29 ltreetest.sql-rw-r--r-- 1 postgres postgres 13K Jun 10 03:29 ltree_op.c-rw-r--r-- 1 postgres postgres 6.9K Jun 10 03:29 _ltree_op.c-rw-r--r-- 1 postgres postgres 14K Jun 10 03:29 ltree_io.c-rw-r--r-- 1 postgres postgres 7.3K Jun 10 03:29 ltree.h-rw-r--r-- 1 postgres postgres 16K Jun 10 03:29 ltree_gist.c-rw-r--r-- 1 postgres postgres 13K Jun 10 03:29 _ltree_gist.c-rw-r--r-- 1 postgres postgres 155 Jun 10 03:29 ltree.control-rw-r--r-- 1 postgres postgres 18K Jun 10 03:29 ltree--1.0.sql-rw-r--r-- 1 postgres postgres 7.1K Jun 10 03:29 lquery_op.c-rw-r--r-- 1 postgres postgres 263 Jun 10 03:29 crc32.h-rw-r--r-- 1 postgres postgres 4.1K Jun 10 03:29 crc32.cdrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 sqldrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 expecteddrwxrwxrwx 2 postgres postgres 4.0K Jun 10 03:38 data contrib/ltree 的 Makefile 如下（在这里配置回归测试的调度，用到变量 REGRESS，对应 sql 目录中的脚本文件名）： 12345678910111213141516171819202122# contrib/ltree/MakefileMODULE_big = ltreeOBJS = ltree_io.o ltree_op.o lquery_op.o _ltree_op.o crc32.o \\ ltxtquery_io.o ltxtquery_op.o ltree_gist.o _ltree_gist.oPG_CPPFLAGS = -DLOWER_NODEEXTENSION = ltreeDATA = ltree--1.0.sql ltree--unpackaged--1.0.sqlREGRESS = ltreeifdef USE_PGXSPG_CONFIG = pg_configPGXS := $(shell $(PG_CONFIG) --pgxs)include $(PGXS)elsesubdir = contrib/ltreetop_builddir = ../..include $(top_builddir)/src/Makefile.globalinclude $(top_srcdir)/contrib/contrib-global.mkendif 其中： 1include $(PGXS) 或： 1include $(top_builddir)/src/Makefile.global 都指向了： 1src/makefiles/pgxs.mk 这个 makefile 中会用到回归测试相关的两个变量： 12# REGRESS -- list of regression test cases (without suffix)# REGRESS_OPTS -- additional switches to pass to pg_regress 引用 src/makefiles/pgxs.mk 的部分内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849ifdef REGRESS# Select database to use for running the testsifneq ($(USE_MODULE_DB),) REGRESS_OPTS += --dbname=$(CONTRIB_TESTDB_MODULE)else REGRESS_OPTS += --dbname=$(CONTRIB_TESTDB)endif# where to find psql for running the testsPSQLDIR = $(bindir)# When doing a VPATH build, must copy over the data files so that the# driver script can find them. We have to use an absolute path for# the targets, because otherwise make will try to locate the missing# files using VPATH, and will find them in $(srcdir), but the point# here is that we want to copy them from $(srcdir) to the build# directory.ifdef VPATHabs_builddir := $(shell pwd)test_files_src := $(wildcard $(srcdir)/data/*.data)test_files_build := $(patsubst $(srcdir)/%, $(abs_builddir)/%, $(test_files_src))all: $(test_files_build)$(test_files_build): $(abs_builddir)/%: $(srcdir)/% $(MKDIR_P) $(dir $@) ln -s $&lt; $@endif # VPATH.PHONY: submakesubmake:ifndef PGXS $(MAKE) -C $(top_builddir)/src/test/regress pg_regress$(X)endif# against installed postmasterinstallcheck: submake $(REGRESS_PREP) $(pg_regress_installcheck) $(REGRESS_OPTS) $(REGRESS)ifdef PGXScheck: @echo &#x27;&quot;$(MAKE) check&quot; is not supported.&#x27; @echo &#x27;Do &quot;$(MAKE) install&quot;, then &quot;$(MAKE) installcheck&quot; instead.&#x27;elsecheck: all submake $(REGRESS_PREP) $(pg_regress_check) --extra-install=$(subdir) $(REGRESS_OPTS) $(REGRESS)endifendif # REGRESS 这里用到了 ltree 中 Makefile 中定义的 subdir 和 REGRESS 变量，如下： 123top_builddir = ../..subdir = contrib/ltreeREGRESS = ltree 所以我们在 contrib/ltree 中执行 make check 会执行：(指PGXS未定义时) 1$(pg_regress_check) --extra-install=$(subdir) $(REGRESS_OPTS) $(REGRESS) pg_regress_check 这个变量在 src/Makefile.global 中定义了，其实就是 pg_regress 命令的调用： 1234src/Makefile.globalsrc/Makefile.global:srcdir = .pg_regress_locale_flags = $(if $(ENCODING),--encoding=$(ENCODING)) $(NOLOCALE)pg_regress_check = $(top_builddir)/src/test/regress/pg_regress --inputdir=$(srcdir) --temp-install=./tmp_check --top-builddir=$(top_builddir) $(pg_regress_locale_flags) $(EXTRA_REGRESS_OPTS) 在 contrib/ltree 中执行 make check 最终执行的是(没有定义的变量直接忽略)： 1../../src/test/regress/pg_regress --inputdir=. --temp-install=./tmp_check --top-builddir=../.. --extra-install=contrib/ltree ltree 我们可以直接到 ltree 的源码目录测试这条命令： 1234567891011121314151617181920[root@digoal ~]# chown -R postgres:postgres /opt/soft_bak/postgresql-9.4.4[root@digoal ~]# su - postgrespostgres@digoal-&gt; cd /opt/soft_bak/postgresql-9.4.4/contrib/ltree/postgres@digoal-&gt; ../../src/test/regress/pg_regress --inputdir=. --temp-install=./tmp_check --top-builddir=../.. --extra-install=contrib/ltree ltree============== removing existing temp installation ============================ creating temporary installation ============================ initializing database system ============================ starting postmaster ==============running on port 57636 with PID 27852============== creating database &quot;regression&quot; ==============CREATE DATABASEALTER DATABASE============== running regression test queries ==============test ltree ... ok============== shutting down postmaster ============================ removing temporary installation =================================== All 1 tests passed.===================== 另外一种测试时 installcheck，和 check 不同的是，installcheck 不需要初始化数据库，是在我们开启了数据库集群的情况下的测试。 同样的方法，我们可以发现它调用的是： 1$(pg_regress_installcheck) $(REGRESS_OPTS) $(REGRESS) 通过 src/Makefile.global 的定义： 12pg_regress_installcheck = $(top_builddir)/src/test/regress/pg_regress --inputdir=$(srcdir) --psqldir=&#x27;$(PSQLDIR)&#x27; $(pg_regress_locale_flags) $(EXTRA_REGRESS_OPTS)bindir := $(shell $(PG_CONFIG) --bindir) 以及 src/makefiles/pgxs.mk 1PSQLDIR = $(bindir) 最终转换为： 1../../src/test/regress/pg_regress --inputdir=. --psqldir=&#x27;/opt/pgsql/bin&#x27; ltree 启动数据库后，就可以进行测试了。同样需要注意(PGPORT PGHOST PGDATABASE PGUSER 等)环境变量。 12345678910111213141516postgres@digoal-&gt; pg_ctl startpostgres@digoal-&gt; pwd/opt/soft_bak/postgresql-9.4.4/contrib/ltreepostgres@digoal-&gt; ../../src/test/regress/pg_regress --inputdir=. --psqldir=&#x27;/opt/pgsql/bin&#x27; ltree(using postmaster on /data01/pg_root_1921, port 1921)============== dropping database &quot;regression&quot; ==============DROP DATABASE============== creating database &quot;regression&quot; ==============CREATE DATABASEALTER DATABASE============== running regression test queries ==============test ltree ... ok===================== All 1 tests passed.===================== 所以插件的回归测试配置也很简单，同样需要 sql, expected 目录，以及通过配置 Makefile 来指定需要回归测试的 sql 脚本。 参考 http://www.postgresql.org/docs/devel/static/regress-run.html http://www.postgresql.org/docs/devel/static/regress-variant.html 各种Makefile 123456src/Makefile.globalsrc/Makefilesrc/makefiles/pgxs.mkcontrib/contrib-global.mkcontrib/xx/Makefile...... 本文转自：https://github.com/digoal/blog/blob/master/201509/20150907_04.md 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"回归测试","slug":"回归测试","permalink":"http://dbkernel.github.io/tags/%E5%9B%9E%E5%BD%92%E6%B5%8B%E8%AF%95/"}]},{"title":"最佳实践 | 源码编译安装配置 Postgres-XC 集群并用 pg_basebackup 配置 Datanode 热备","slug":"how-to-install-postgres-xc-on-linux","date":"2016-03-15T11:56:52.000Z","updated":"2021-09-22T15:24:33.492Z","comments":true,"path":"2016/03/15/how-to-install-postgres-xc-on-linux/","link":"","permalink":"http://dbkernel.github.io/2016/03/15/how-to-install-postgres-xc-on-linux/","excerpt":"","text":"本文首发于 2016-03-15 19:56:52 注意：本篇文章成文时 Postgres-XC 还未改名为 Postgres-X2 。 1. 下载源码1git clone git@github.com:postgres-x2/postgres-x2.git 2. 安装依赖对于 Ubuntu/Debian： 12345678910111213141516171819202122232425262728apt-get install -y git-coreapt-get install -y gcc g++apt-get install -y ccacheapt-get install -y libreadline-devapt-get install -y bison flexapt-get install -y zlib1g-devapt-get install -y openssl libssl-devapt-get install -y libpam-devapt-get install -y libcurl4-devapt-get install -y libbz2-devapt-get install -y python-devapt-get install -y sshapt-get install -y libcurl4-devPackage libcurl4-dev is a virtual package provided by:libcurl4-openssl-dev 7.38.0-4+deb8u2libcurl4-nss-dev 7.38.0-4+deb8u2libcurl4-gnutls-dev 7.38.0-4+deb8u2apt-get install -y python-pippip install lockfilepip install paramikopip install setuptoolspip install epydocpip install psiNote: debian8 required pip install --pre psi 对于 CentOS： 123456789101112131415161718192021222324252627yum install –y git.x86_64yum install –y gcc.x86_64 gcc-c++.x86_64yum install –y ccache.x86_64yum install readline.x86_64 readline-devel.x86_64yum install bison.x86_64 bison-devel.x86_64yum install flex.x86_64 flex-devel.x86_64yum install zlib.x86_64 zlib-devel.x86_64yum install -y openssl.x86_64 openssl-devel.x86_64yum install -y pam.x86_64 pam-devel.x86_64yum install –y libcurl.x86_64 libcurl-devel.x86_64yum install bzip2-libs.x86_64 bzip2.x86_64 bzip2-devel.x86_64yum install libssh2.x86_64 libssh2-devel.x86_64yum install python-devel.x86_64yum install -y python-pip.noarch# 接着执行：pip install lockfilepip install paramikopip install setuptoolspip install epydocpip install psi# 或者执行：yum install python-lockfile.noarchyum install python-PSI.x86_64yum install python-paramiko.noarchyum install python-setuptools.noarchyum install epydoc.noarch 3. 编译安装1234$ cd postgres-x2$ ./configure --prefix=/home/wslu/pgsql --enable-debug #其中--prefix指定编绎完成后将要安装的路径，必须使用全路径，wslu为使用者。$ make #执行编绎$ make install #执行安装 4. 初始化、启动4.1. 初始化 GTM123456$ cd /home/wslu/pgsql#设置PTAH变量$ export PATH=/home/user/pgsql/bin:$PATH#使用初始化gtm命令initgtm$ ./bin/initgtm -Z gtm -D data/gtm/ 4.2. 初始化数据库节点初始化所有数据库节点（CO、DN）： 123456#使用初始化db命令initdb$ ./bin/initdb -U wslu -A trust --locale=C -D data/co1 # -U 使用者 -D 数据目录/节点$ ./bin/initdb -U wslu -A trust --locale=C -D data/co2$ ./bin/initdb -U wslu -A trust --locale=C -D data/dn1$ ./bin/initdb -U wslu -A trust --locale=C -D data/dn2$ ./bin/initdb -U wslu -A trust --locale=C -D data/dn3 4.3. 编辑配置文件编辑 data/co1/postgresql.conf： 1234# 默认值gtm_port = 6666# pgxc_node_name 不能重复pgxc_node_name = co1 编辑 data/co2/postgresql.conf： 12gtm_port = 6666pgxc_node_name = co2 编辑 data/dn1/postgresql.conf： 12gtm_port = 6666pgxc_node_name = dn1 编辑 data/dn2/postgresql.conf： 12gtm_port = 6666pgxc_node_name = dn2 编辑 data/dn2/postgresql.conf： 12gtm_port = 6666pgxc_node_name = dn3 4.4. 启动服务依次启动 gtm、datanode、coordinator： 123456789101112# ./bin/gtm_ctl start -S gtm -D data/gtm -l data/gtm/gtm.log //启动gtm（由于切换为相对路径后找不到对应的文件夹，所以创建日志会失败）$ ./bin/gtm_ctl start -Z gtm -D data/gtm -l gtm.log //启动gtm# vim data/gtm/gtm.log # 使用日志查看gtm是否启动$ ./bin/pg_ctl start -Z datanode -D data/dn1 -l data/dn1/postgresql.log -o &quot;-p 24071&quot; //启动datanode dn1， DN1_PORT=24071 根据需要自由设置# vim data/dn1/postgresql.log # 同样使用日志查看是否启动$ ./bin/pg_ctl start -Z datanode -D data/dn2 -l data/dn2/postgresql.log -o &quot;-p 24072&quot; //启动 dn2， DN2_PORT=24072$ ./bin/pg_ctl start -Z datanode -D data/dn3 -l data/dn3/postgresql.log -o &quot;-p 24073&quot; //启动 dn3， DN3_PORT=24073$ ./bin/pg_ctl start -Z coordinator -D data/co1 -l data/co1/postgresql.log -o &quot;-p 24076&quot; //启动 coordinator co1， CO1_PORT=24076$ ./bin/pg_ctl start -Z coordinator -D data/co2 -l data/co2/postgresql.log -o &quot;-p 24077&quot; //启动 co2， CO2_PORT= 24077 5. 配置集群节点指定动态库位置： 1$ export LD_LIBRARY_PATH=/home/wslu/pgsql/lib:$LD_LIBRARY_PATH 配置集群节点： 123456789# 进入co1创建节点，co1_port=24076$ ./bin/psql -p 24076 postgres postgres CREATE NODE dn1 WITH (HOST = &#x27;localhost&#x27;, type = &#x27;datanode&#x27;, PORT = 24071, id = 1, content = 1); //在协调器上注册节点，各端口号与上面一致 CREATE NODE dn2 WITH (HOST = &#x27;localhost&#x27;, type = &#x27;datanode&#x27;, PORT = 24072, id = 2, content = 2); CREATE NODE dn3 WITH (HOST = &#x27;localhost&#x27;, type = &#x27;datanode&#x27;, PORT = 24073, id = 3, content = 3); CREATE NODE co1 WITH (HOST = &#x27;localhost&#x27;, type = &#x27;coordinator&#x27;, PORT = 24076, id = 4, content = 4); CREATE NODE co2 WITH (HOST = &#x27;localhost&#x27;, type = &#x27;coordinator&#x27;, PORT = 24077, id = 5, content = 5); SELECT pgxc_pool_reload(); 至此，集群配置完成。 6. 常见操作6.1. 停止集群1234567$ ./bin/pg_ctl stop -D data/co1 -m immediate$ ./bin/pg_ctl stop -D data/co2 -m immediate$ ./bin/pg_ctl stop -D data/dn1 -m immediate$ ./bin/pg_ctl stop -D data/dn2 -m immediate$ ./bin/pg_ctl stop -D data/dn3 -m immediate$ ./bin/gtm_ctl stop -Z gtm -D data/gtm$ rm -f data/gtm/register.node 6.2. 启动集群123456$ ./bin/gtm_ctl start -Z gtm -D data/gtm -p ./bin -l data/gtm/gtm.log$ ./bin/pg_ctl start -l data/dn1/postgresql.log -Z datanode -D data/dn1 -o &quot;-p 24071&quot;$ ./bin/pg_ctl start -l data/dn2/postgresql.log -Z datanode -D data/dn2 -o &quot;-p 24072&quot;$ ./bin/pg_ctl start -l data/dn3/postgresql.log -Z datanode -D data/dn3 -o &quot;-p 24073&quot;$ ./bin/pg_ctl start -l data/co1/postgresql.log -Z coordinator -D data/co1 -o &quot;-p 24076&quot;$ ./bin/pg_ctl start -l data/co2/postgresql.log -Z coordinator -D data/co2 -o &quot;-p 24077&quot; 6.3. 清理数据如需清除数据，请先停止服务器集群，然后清除数据存储目录: 12345678$ ./bin/pg_ctl stop -D data/co1 -m immediate$ ./bin/pg_ctl stop -D data/co2 -m immediate$ ./bin/pg_ctl stop -D data/dn1 -m immediate$ ./bin/pg_ctl stop -D data/dn2 -m immediate$ ./bin/pg_ctl stop -D data/dn3 -m immediate$ ./bin/gtm_ctl stop -Z gtm -D data/gtm$ rm -f data/gtm/register.node$ rm -rf data 7. 配置 Datanode 热备7.1. 修改所有 CO 和 DN 的 pg_hba.conf将下面两行的注释去掉： 12345678910111213141516171819$ vi data/co1/pg_hba.confhost replication wslu 127.0.0.1/32 trusthost replication wslu ::1/128 trust$ vi data/co2/pg_hba.confhost replication wslu 127.0.0.1/32 trusthost replication wslu ::1/128 trust$ vi data/dn1/pg_hba.confhost replication wslu 127.0.0.1/32 trusthost replication wslu ::1/128 trust$ vi data/dn2/pg_hba.confhost replication wslu 127.0.0.1/32 trusthost replication wslu ::1/128 trust$ vi data/dn3/pg_hba.confhost replication wslu 127.0.0.1/32 trusthost replication wslu ::1/128 trust 此处为了测试方便，将校验方式设为 trust；实际生产中要改为 md5，即根据账户密码验证。 7.2. 修改所有 CO 和 DN 的 postgresql.conf添加以下内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ vi data/co1/postgresql.conflisten_addresses = &#x27;*&#x27;log_line_prefix = &#x27;%t:%r:%u@%d:[%p]: &#x27;#logging_collector = onport = 24076wal_level = archive$ vi data/co2/postgresql.conflisten_addresses = &#x27;*&#x27;log_line_prefix = &#x27;%t:%r:%u@%d:[%p]: &#x27;#logging_collector = onport = 24077wal_level = archive$ vi data/dn1/postgresql.confhot_standby = on#logging_collector = onlisten_addresses = &#x27;*&#x27;log_line_prefix = &#x27;%t:%r:%u@%d:[%p]: &#x27;wal_keep_segments = 10wal_level = hot_standbymax_wal_senders = 5include_if_exists = &#x27;synchronous_standby_names.conf&#x27;port = 24071$ vi data/dn2/postgresql.confhot_standby = on#logging_collector = onlisten_addresses = &#x27;*&#x27;log_line_prefix = &#x27;%t:%r:%u@%d:[%p]: &#x27;wal_keep_segments = 10wal_level = hot_standbymax_wal_senders = 5include_if_exists = &#x27;synchronous_standby_names.conf&#x27;port = 24072$ vi data/dn3/postgresql.confhot_standby = on#logging_collector = onlisten_addresses = &#x27;*&#x27;log_line_prefix = &#x27;%t:%r:%u@%d:[%p]: &#x27;wal_keep_segments = 10wal_level = hot_standbymax_wal_senders = 5include_if_exists = &#x27;synchronous_standby_names.conf&#x27;port = 24073 7.3. 创建备 DN在数据库集群开启的前提下执行下列指令，以创建备 Datanode 目录： 123$ pg_basebackup -D data/dn1s -Fp -Xs -v -P -h localhost -p 24071 -U wslu$ pg_basebackup -D data/dn2s -Fp -Xs -v -P -h localhost -p 24072 -U wslu$ pg_basebackup -D data/dn3s -Fp -Xs -v -P -h localhost -p 24073 -U wslu 7.4. 在所有备 DN 新建 recovery.conf1234567891011$ vi dn1s/recovery.confstandby_mode = &#x27;on&#x27;primary_conninfo = &#x27;user=wslu host=localhost port=24071 sslmode=disable sslcompression=1&#x27;$ vi dn2s/recovery.confstandby_mode = &#x27;on&#x27;primary_conninfo = &#x27;user=wslu host=localhost port=24072 sslmode=disable sslcompression=1&#x27;$ vi dn3s/recovery.confstandby_mode = &#x27;on&#x27;primary_conninfo = &#x27;user=wslu host=localhost port=24073 sslmode=disable sslcompression=1&#x27; 7.5. 在所有主 DN 新建 synchronous_standby_names.conf12vi data/dn1/synchronous_standby_names.confsynchronous_standby_names=&#x27;*&#x27; 7.6. 在所有 CO 添加备 DN 节点这里以 co1 为例，co2 也要执行同样操作（ 对于支持热备的其他 pg 商用数据库，类型不是 datanode 而是 standby）： 1234$ ./bin/psql -p 24076 postgres postgres //进入co1创建节点，co1_port=24076 CREATE NODE dn1s WITH (HOST = &#x27;localhost&#x27;, type = &#x27;datanode&#x27;, PORT = 34071, id = 6, content = 1); //在协调器上注册节点，各端口号与上面一致 CREATE NODE dn2s WITH (HOST = &#x27;localhost&#x27;, type = &#x27;datanode&#x27;, PORT = 34072, id = 7, content = 2); CREATE NODE dn3s WITH (HOST = &#x27;localhost&#x27;, type = &#x27;datanode&#x27;, PORT = 34073, id = 8, content = 3); 7.7. 启动所有备 DN 服务123./bin/pg_ctl start -D data/dn1s -l data/dn1s/postgresql.log -o &quot;-p 34071&quot;./bin/pg_ctl start -D data/dn2s -l data/dn2s/postgresql.log -o &quot;-p 34072&quot;./bin/pg_ctl start -D data/dn3s -l data/dn3s/postgresql.log -o &quot;-p 34073&quot; 相应的，停止所有备 DN 节点服务的指令为： 123./bin/pg_ctl stop -D data/dn1s -m immediate./bin/pg_ctl stop -D data/dn2s -m immediate./bin/pg_ctl stop -D data/dn3s -m immediate 8. Q&amp;A8.1. 如何提升备 DN 为主 DN我并未实现成功，但参照其他 PostgreSQL 的分布式数据库，步骤如下： 杀掉主 DN 进程，在备 DN 的目录下创建一个触发文件（例如：promote）文件。 通过 kill -SIGUSR1 备DN进程号 指令给备 DN 的 postmaster 进程发送一个 SIGUSR1 信号。 在主 CO 执行类似 alter node dn1s with(promote); 的指令。 退出 psql，再重新连入 psql。 此时，备 DN 就作为主 DN 运行了，可执行 DDL、DML 等所有操作。 8.2. 当备 DN 挂掉时，如何关闭主备 DN 之间的数据同步也就是关闭 walsender 和 walreciever。 这就涉及到源码级别了，一般做两步： 将主 DN 状态改为 OutSync（别的数据库的做法）。 在代码中将 SyncRepStandbyNames 设为 &quot;&quot;。 补充本教程关于配置备 DN 的描述只能对各个 DN 的数据做备份，并未成功实现某个 DN 挂掉了自动切换到备 DN。 另外，我并未在 Postgres-XC（现在 github 改名为了 Postgres-X2）源码的回归测试中看到如何在 pgxc_nodes 系统表创建备 DN 节点。 不过，GreenPlum（以 PostgreSQL 为基础开发的分布式数据库）有此功能，可做参考。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"Postgres-X2","slug":"Postgres-X2","permalink":"http://dbkernel.github.io/categories/Postgres-X2/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"Postgres-X2","slug":"Postgres-X2","permalink":"http://dbkernel.github.io/tags/Postgres-X2/"},{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"http://dbkernel.github.io/tags/Postgres-XC/"}]},{"title":"特性分析 | GreenPlum Primary/Mirror 同步机制","slug":"greenplum-primary-mirror-sync-mechanism","date":"2016-01-21T12:02:26.000Z","updated":"2021-09-22T15:24:33.178Z","comments":true,"path":"2016/01/21/greenplum-primary-mirror-sync-mechanism/","link":"","permalink":"http://dbkernel.github.io/2016/01/21/greenplum-primary-mirror-sync-mechanism/","excerpt":"","text":"本文首发于 2016-01-21 20:02:26 引言PostgreSQL 主备同步机制是通过流复制实现，其原理见 PG主备流复制机制。 Greenplum 是基于 PostgreSQL 开发的，它的主备也是通过流复制实现，但是 Segment 节点中的 Primary 和 Mirror 之间的数据同步是基于文件级别的同步实现的。 为什么Primary和Mirror不能再使用流复制实现呢？ 主要有两个原因: Append Only 表不写 WAL 日志，所以 Append Only 表的数据就不能通过 XLOG 发送到 Mirror 再 Apply 。 pg_control等文件也是不写 WAL 日志，也只能通过文件方式同步到 Mirror 。 GreenPlum 总体结构Greenplum 的架构采用了 MPP 无共享体系。在 MPP 系统中，每个数据节点有自己的CPU、磁盘和内存(Share nothing)，每个节点内的 CPU 不能访问另一个节点的内存。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为数据重分配(Data Redistribution)。 Master 负责协调整个集群 ，一个数据节点可以配置多个节点实例(Segment Instances)，节点实例并行处理查询(SQL)。 Primary和Mirror同步机制Primary 和 Mirror 同步的内容主要有两部分，即文件和数据。之所以 Primary 和 Mirror 要同步文件，是 Primary 和 Mirror 之间可以自动 failover，只有两者保持同步才能相互替代。如果只把数据同步过去，pg_control、pg_clog、pg_subtrans 没有同步，那么从 Primary 切换到 Mirror 会出现问题。 Master 和 slave 却不用担心这些问题，Append Only 表的数据只会存在 Segment，所以 WAL 日志足够保持 Master 和 slave 同步(只要是流复制，pg_control、pg_clog、pg_subtrans 这些文件Slave会自动更新，无需从 Master 同步)。 1. 数据同步当 Master 向 Primary 下发执行计划后，Primary 开始执行，如果是 DML 操作，那么 Primary 会产生 XLOG 及更新 page。会在 SlruPhysicalWritePage 函数中(写数据页)产生FileRepOperationOpen、FileRepOperationWrite、FileRepOperationFlush、FileRepOperationClose等指令消息(消息中包含具体要更新的文件page及内容)，通过 primary sender 进程向 Mirror 发送 Message，然后 Mirror 的 mirror consumer 等进程解析消息，执行变更。XLOG 通过XLogWrite函数(写XLOG)执行同样的操作，把 XLOG 更新同步过去。 2. 文件同步Primary 会有个 recovery 进程，这个进程会循环把 Primary 的 pg_control、pg_clog、pg_subtrans 等文件覆盖到 Mirror。同时检查 XLOG 是否一致，如果不一致以 Primary 为主，对 Mirror 进行覆盖。除了把 Primary 部分文件同步到 Mirror 之外，recovery 进程还会将 Mirror 上面的临时文件删掉。 总结Primary 和 Mirror 同步数据的时候，Primary 对于每一次写 page 都会通过消息发送到 Mirror，如果 Primary 大量的更新 page，那么 Primary 和 Mirror 同步将有可能成为瓶颈。 本文转自：http://mysql.taobao.org/monthly/2016/01/02/ 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/categories/GreenPlum/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/tags/GreenPlum/"},{"name":"主从同步","slug":"主从同步","permalink":"http://dbkernel.github.io/tags/%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"}]},{"title":"最佳实践 | CentOS 和 Ubuntu 下安装配置 GreenPlum 数据库集群 - 源码 & 安装包","slug":"how-to-install-greenplum-on-linux","date":"2016-01-14T11:55:08.000Z","updated":"2021-09-24T13:54:15.014Z","comments":true,"path":"2016/01/14/how-to-install-greenplum-on-linux/","link":"","permalink":"http://dbkernel.github.io/2016/01/14/how-to-install-greenplum-on-linux/","excerpt":"","text":"本文首发于 2016-01-14 19:55:08 本文介绍如何在 CentOS/RedHat、Ubuntu/Debian 下通过安装包方式和源码方式安装配置 GreenPlum 集群。 1. 安装步骤1.1. 规划 192.168.4.93（h93） 1个主master 2个主segment、2个镜像segment 192.168.4.94（h94） 1个备master 2个主segment、2个镜像segment 安装在/home/wslu/gp/gpsql目录下。 注意： 如无特殊说明，本文后续步骤需要在 h93 和 h94 都执行。 1.2. 安装依赖按如下方式在在 h93 和 h94 安装依赖。 对于 Ubuntu/Debian： 12345678910111213141516171819202122232425262728apt-get install -y git-coreapt-get install -y gcc g++apt-get install -y ccacheapt-get install -y libreadline-devapt-get install -y bison flexapt-get install -y zlib1g-devapt-get install -y openssl libssl-devapt-get install -y libpam-devapt-get install -y libcurl4-devapt-get install -y libbz2-devapt-get install -y python-devapt-get install -y sshapt-get install -y libcurl4-devPackage libcurl4-dev is a virtual package provided by:libcurl4-openssl-dev 7.38.0-4+deb8u2libcurl4-nss-dev 7.38.0-4+deb8u2libcurl4-gnutls-dev 7.38.0-4+deb8u2apt-get install -y python-pippip install lockfilepip install paramikopip install setuptoolspip install epydocpip install psiNote: debian8 required pip install --pre psi 对于 CentOS： 123456789101112131415161718192021222324252627yum install –y git.x86_64yum install –y gcc.x86_64 gcc-c++.x86_64yum install –y ccache.x86_64yum install readline.x86_64 readline-devel.x86_64yum install bison.x86_64 bison-devel.x86_64yum install flex.x86_64 flex-devel.x86_64yum install zlib.x86_64 zlib-devel.x86_64yum install -y openssl.x86_64 openssl-devel.x86_64yum install -y pam.x86_64 pam-devel.x86_64yum install –y libcurl.x86_64 libcurl-devel.x86_64yum install bzip2-libs.x86_64 bzip2.x86_64 bzip2-devel.x86_64yum install libssh2.x86_64 libssh2-devel.x86_64yum install python-devel.x86_64yum install -y python-pip.noarch# 接着执行：pip install lockfilepip install paramikopip install setuptoolspip install epydocpip install psi# 或者执行：yum install python-lockfile.noarchyum install python-PSI.x86_64yum install python-paramiko.noarchyum install python-setuptools.noarchyum install epydoc.noarch 1.3. 安装包方式安装 从官网下载greenplum-db-4.3.6.1-build-2-RHEL5-x86_64.zip。 解压：1unzip greenplum-db-4.3.6.1-build-2-RHEL5-x86_64.zip 以普通用户安装：12$ ./greenplum-db-4.3.6.1-build-2-RHEL5-x86_64.bin安装路径选择 /home/wslu/gp/gpsql 1.4. 源码安装1.4.1. 克隆源码123$ mkdir /home/wslu/gp/greenplum$ cd /home/wslu/gp/greenplum$ git clone https://github.com/greenplum-db/gpdb. 1.4.2. 编译安装1234$ cd /home/wslu/gp/greenplum$ CFLAGS+=&quot;-O2&quot; ./configure--prefix=/home/wslu/gp/gpsql --enable-debug --enable-depend --enable-cassert$ make$ make install 安装时如果遇到某些 python 包（lockfile、 paramiko、PSI等）找不到，可以参考 HAWQ 项目，将 &lt;hawq_src&gt;/tools/bin/pythonSrc/ 下所有的压缩包拷贝到/home/wslu/gp/greenplum/gpMgmt/bin/pythonSrc/ext/ 中，然后再 make install 即可。 至此集群源码编译完成。 1.5. 设置参数1.5.1. 设置操作系统参数 关闭防火墙。 加速SSH连接： 12sudo sed -i &#x27;s/^GSS/#&amp;/g&#x27; /etc/ssh/sshd_config # 用来加速SSH连接的service sshd restart 设置内核和内存方面的参数： 12345678910111213141516171819202122# 设置内核参数, 并在启动时生效sysctl -p - &gt;&gt;/etc/sysctl.conf &lt;&lt;EOF# configurationskernel.sysrq=1kernel.core_pattern=corekernel.core_uses_pid=1kernel.msgmnb=65536kernel.msgmax=65536kernel.msgmni=2048kernel.sem=25600 3200000 10000 14200net.ipv4.tcp_syncookies=1net.ipv4.ip_forward=0net.ipv4.conf.default.accept_source_route=0net.ipv4.tcp_tw_recycle=1net.ipv4.tcp_max_syn_backlog=4096net.ipv4.conf.all.arp_filter=1net.ipv4.ip_local_port_range=1025 65535net.core.netdev_max_backlog=10000net.core.rmem_max=2097152net.core.wmem_max=2097152vm.overcommit_memory=1EOF 可以参考官方推荐设置共享内存相关参数： 123456789101112131415161718192021# vi /etc/sysctl.confkernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000kernel.sem = 250 512000 100 2048kernel.sysrq = 1kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.msgmni = 2048net.ipv4.tcp_syncookies = 1net.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.ipv4.conf.all.arp_filter = 1net.ipv4.ip_local_port_range = 1025 65535net.core.netdev_max_backlog = 10000net.core.rmem_max = 2097152net.core.wmem_max = 2097152vm.overcommit_memory = 2 设置文件读写相关参数： 12345678# 设置limitscat &gt;&gt;/etc/security/limits.d/greenplum.conf &lt;&lt;EOF# GreenPlum configurations* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072EOF 1.5.2. 设置数据库相关参数GUC参数设置示例（需要根据机器配置调整）： 123456789work_mem=1GBshared_buffers=2GBmax_connections=500max_pool_size=2000enable_mergejoin=offenable_nestloop=offmax_prepared_transactions=50autovacuum=offinterconnect_setup_timeout=1200 1.6. demo 集群 提示： 如果不想用demo集群，可以直接跳过本小节。 安装完成后，可以使用如下指令创建 demo 集群（在本机创建包含3个segment，3个segment-mirror，1个master的集群）： 123456$ cd /home/wslu/gp/gpsql$ source greenplum_path.sh$ gpssh-exkeys –h localhost$ cd gpAux/gpdemo$ make cluster$ source gpdemo-env.sh 1.7. 设置环境变量12$ source gpsql/greenplum_path.sh$ export MASTER_DATA_DIRECTORY=/home/wslu/gp/gpsql/data/master/gpseg-1 1.8. 交换 SSH 密钥12gpssh-exkeys –h h93gpssh-exkeys –h h94 1.9. 初始化集群 在 h93 和 h94 执行下述指令，以创建数据目录： 1$ mkdir gpsql/data/primary gpsql/data/mirror gpsql/data/master –p 在 h93 创建配置文件 configs/gpinitsystem_config，内容如下： 1234567891011121314ARRAY_NAME=&quot;EMC Greenplum DW&quot;SEG_PREFIX=gpsegPORT_BASE=40000declare -a DATA_DIRECTORY=(/home/wslu/gp/gpsql/data/primary /home/wslu/gp/gpsql/data/primary)MASTER_HOSTNAME=h93MASTER_DIRECTORY=/home/wslu/gp/gpsql/data/masterMASTER_PORT=5432TRUSTED_SHELL=sshCHECK_POINT_SEGMENTS=8ENCODING=UNICODEMIRROR_PORT_BASE=50000REPLICATION_PORT_BASE=41000MIRROR_REPLICATION_PORT_BASE=51000declare -a MIRROR_DATA_DIRECTORY=(/home/wslu/gp/gpsql/data/mirror /home/wslu/gp/gpsql/data/mirror) 注意：configs目录是我自己创建的、便于保存自定义配置文件的目录。该步骤的目的是创建一个初始化时要用的配置文件，并没有路径的要求。 在 h93 创建配置文件 configs/hostfile_gpinitsystem，内容如下： 12h93h94 注意：configs 目录是我自己创建的、便于保存自定义配置文件的目录。该步骤的目的是创建一个初始化时要用的配置文件，并没有路径的要求。 在 h93 执行下述指令初始化集群： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164[wslu@h93 gpsql]$ gpinitsystem -c configs/gpinitsystem_config -h configs/hostfile_gpinitsystem –a20160114:14:30:03:005980 gpinitsystem:h93:wslu-[INFO]:-Checking configuration parameters, please wait...20160114:14:30:03:005980 gpinitsystem:h93:wslu-[INFO]:-Reading Greenplum configuration file configs/gpinitsystem_config20160114:14:30:03:005980 gpinitsystem:h93:wslu-[INFO]:-Locale has not been set in configs/gpinitsystem_config, will set to default value20160114:14:30:03:005980 gpinitsystem:h93:wslu-[INFO]:-Locale set to en_US.utf820160114:14:30:03:005980 gpinitsystem:h93:wslu-[INFO]:-No DATABASE_NAME set, will exit following template1 updates20160114:14:30:03:005980 gpinitsystem:h93:wslu-[INFO]:-MASTER_MAX_CONNECT not set, will set to default value 25020160114:14:30:03:005980 gpinitsystem:h93:wslu-[INFO]:-Checking configuration parameters, Completed20160114:14:30:04:005980 gpinitsystem:h93:wslu-[INFO]:-Commencing multi-home checks, please wait.....20160114:14:30:05:005980 gpinitsystem:h93:wslu-[INFO]:-Configuring build for standard array20160114:14:30:05:005980 gpinitsystem:h93:wslu-[INFO]:-Commencing multi-home checks, Completed20160114:14:30:05:005980 gpinitsystem:h93:wslu-[INFO]:-Building primary segment instance array, please wait.......20160114:14:30:08:005980 gpinitsystem:h93:wslu-[INFO]:-Building group mirror array type , please wait.......20160114:14:30:12:005980 gpinitsystem:h93:wslu-[INFO]:-Checking Master host20160114:14:30:12:005980 gpinitsystem:h93:wslu-[INFO]:-Checking new segment hosts, please wait...........20160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Checking new segment hosts, Completed20160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Greenplum Database Creation Parameters20160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:---------------------------------------20160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Master Configuration20160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:---------------------------------------20160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Master instance name = EMC Greenplum DW20160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Master hostname = h9320160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Master port = 543220160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Master instance dir = /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:30:28:005980 gpinitsystem:h93:wslu-[INFO]:-Master LOCALE = en_US.utf820160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Greenplum segment prefix = gpseg20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Master Database =20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Master connections = 25020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Master buffers = 128000kB20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Segment connections = 75020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Segment buffers = 128000kB20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Checkpoint segments = 820160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Encoding = UNICODE20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Postgres param file = Off20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Initdb to be used = /home/wslu/gp/gpsql/bin/initdb20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-GP_LIBRARY_PATH is = /home/wslu/gp/gpsql/lib20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Ulimit check = Passed20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Array host connect type = Single hostname per node20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Master IP address [1] = ::120160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Master IP address [2] = 192.168.4.9320160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Master IP address [3] = fe80::225:90ff:fe3b:86c220160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Standby Master = Not Configured20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Primary segment # = 220160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Total Database segments = 420160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Trusted shell = ssh20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Number segment hosts = 220160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Mirror port base = 5000020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Replicaton port base = 4100020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Mirror replicaton port base= 5100020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Mirror segment # = 220160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Mirroring config = ON20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Mirroring type = Group20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:----------------------------------------20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Greenplum Primary Segment Configuration20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:----------------------------------------20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h93 /home/wslu/gp/gpsql/data/primary/gpseg0 40000 2 0 4100020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h93 /home/wslu/gp/gpsql/data/primary/gpseg1 40001 3 1 4100120160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h94 /home/wslu/gp/gpsql/data/primary/gpseg2 40000 4 2 4100020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h94 /home/wslu/gp/gpsql/data/primary/gpseg3 40001 5 3 4100120160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:---------------------------------------20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-Greenplum Mirror Segment Configuration20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:---------------------------------------20160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h94 /home/wslu/gp/gpsql/data/mirror/gpseg0 50000 6 0 5100020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h94 /home/wslu/gp/gpsql/data/mirror/gpseg1 50001 7 1 5100120160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h93 /home/wslu/gp/gpsql/data/mirror/gpseg2 50000 8 2 5100020160114:14:30:29:005980 gpinitsystem:h93:wslu-[INFO]:-h93 /home/wslu/gp/gpsql/data/mirror/gpseg3 50001 9 3 51001Continue with Greenplum creation Yy/Nn&gt;y20160114:14:30:32:005980 gpinitsystem:h93:wslu-[INFO]:-Building the Master instance database, please wait...20160114:14:31:08:005980 gpinitsystem:h93:wslu-[INFO]:-Starting the Master in admin mode20160114:14:32:01:005980 gpinitsystem:h93:wslu-[INFO]:-Commencing parallel build of primary segment instances20160114:14:32:01:005980 gpinitsystem:h93:wslu-[INFO]:-Spawning parallel processes batch [1], please wait.......20160114:14:32:02:005980 gpinitsystem:h93:wslu-[INFO]:-Waiting for parallel processes batch [1], please wait..............................................................20160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:------------------------------------------------20160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:-Parallel process exit status20160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:------------------------------------------------20160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:-Total processes marked as completed = 420160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:-Total processes marked as killed = 020160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:-Total processes marked as failed = 020160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:------------------------------------------------20160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:-Commencing parallel build of mirror segment instances20160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:-Spawning parallel processes batch [1], please wait.......20160114:14:33:01:005980 gpinitsystem:h93:wslu-[INFO]:-Waiting for parallel processes batch [1], please wait............................................20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:------------------------------------------------20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-Parallel process exit status20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:------------------------------------------------20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-Total processes marked as completed = 420160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-Total processes marked as killed = 020160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-Total processes marked as failed = 020160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:------------------------------------------------20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-Deleting distributed backout files20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-Removing back out file20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-No errors generated from parallel processes20160114:14:33:43:005980 gpinitsystem:h93:wslu-[INFO]:-Restarting the Greenplum instance in production mode20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Starting gpstop with args: -a -i -m -d /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Gathering information and validating the environment...20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Obtaining Greenplum Master catalog information20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Obtaining Segment details from master...20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Greenplum Version: &#x27;postgres (Greenplum Database) 4.3.99.00 build dev&#x27;20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-There are 0 connections to the database20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Commencing Master instance shutdown with mode=&#x27;immediate&#x27;20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Master host=h9320160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Commencing Master instance shutdown with mode=immediate20160114:14:33:43:001932 gpstop:h93:wslu-[INFO]:-Master segment instance directory=/home/wslu/gp/gpsql/data/master/gpseg-120160114:14:33:44:001932 gpstop:h93:wslu-[INFO]:-Attempting forceful termination of any leftover master process20160114:14:33:44:001932 gpstop:h93:wslu-[INFO]:-Terminating processes for segment /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:33:45:002019 gpstart:h93:wslu-[INFO]:-Starting gpstart with args: -a -d /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:33:45:002019 gpstart:h93:wslu-[INFO]:-Gathering information and validating the environment...20160114:14:33:45:002019 gpstart:h93:wslu-[INFO]:-Greenplum Binary Version: &#x27;postgres (Greenplum Database) 4.3.99.00 build dev&#x27;20160114:14:33:45:002019 gpstart:h93:wslu-[INFO]:-Greenplum Catalog Version: &#x27;300701081&#x27;20160114:14:33:45:002019 gpstart:h93:wslu-[INFO]:-Starting Master instance in admin mode20160114:14:33:46:002019 gpstart:h93:wslu-[INFO]:-Obtaining Greenplum Master catalog information20160114:14:33:46:002019 gpstart:h93:wslu-[INFO]:-Obtaining Segment details from master...20160114:14:33:46:002019 gpstart:h93:wslu-[INFO]:-Setting new master era20160114:14:33:46:002019 gpstart:h93:wslu-[INFO]:-Master Started...20160114:14:33:46:002019 gpstart:h93:wslu-[INFO]:-Shutting down master20160114:14:33:47:002019 gpstart:h93:wslu-[INFO]:-Commencing parallel primary and mirror segment instance startup, please wait...........20160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:-Process results...20160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:-----------------------------------------------------20160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:- Successful segment starts = 820160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:- Failed segment starts = 020160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:- Skipped segment starts (segments are marked down in configuration) = 020160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:-----------------------------------------------------20160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:-20160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:-Successfully started 8 of 8 segment instances20160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:-----------------------------------------------------20160114:14:33:55:002019 gpstart:h93:wslu-[INFO]:-Starting Master instance h93 directory /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:33:56:002019 gpstart:h93:wslu-[INFO]:-Command pg_ctl reports Master h93 instance active20160114:14:33:56:002019 gpstart:h93:wslu-[INFO]:-No standby master configured. skipping...20160114:14:33:56:002019 gpstart:h93:wslu-[INFO]:-Database successfully started20160114:14:33:59:005980 gpinitsystem:h93:wslu-[INFO]:-Completed restart of Greenplum instance in production mode20160114:14:33:59:005980 gpinitsystem:h93:wslu-[INFO]:-Loading gp_toolkit...20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-Scanning utility log file for any warning messages20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-Log file scan check passed20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-Greenplum Database instance successfully created20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-------------------------------------------------------20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-To complete the environment configuration, please20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-update wslu .bashrc file with the following20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-1. Ensure that the greenplum_path.sh file is sourced20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-2. Add &quot;export MASTER_DATA_DIRECTORY=/home/wslu/gp/gpsql/data/master/gpseg-1&quot;20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:- to access the Greenplum scripts for this instance:20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:- or, use -d /home/wslu/gp/gpsql/data/master/gpseg-1 option for the Greenplum scripts20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:- Example gpstate -d /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-Script log file = /home/wslu/gpAdminLogs/gpinitsystem_20160114.log20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-To remove instance, run gpdeletesystem utility20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-To initialize a Standby Master Segment for this Greenplum instance20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-Review options for gpinitstandby20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-------------------------------------------------------20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-The Master /home/wslu/gp/gpsql/data/master/gpseg-1/pg_hba.conf post gpinitsystem20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-has been configured to allow all hosts within this new20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-array to intercommunicate. Any hosts external to this20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-new array must be explicitly added to this file20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-Refer to the Greenplum Admin support guide which is20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:-located in the /home/wslu/gp/gpsql/docs directory20160114:14:34:02:005980 gpinitsystem:h93:wslu-[INFO]:------------------------------------------------------- 查看目录结构： 123456789101112131415161718[wslu@h93 gpsql]$ ls datamaster mirror primary[wslu@h93 gpsql]$ ls data/master/gpseg-1[wslu@h93 gpsql]$ ls data/mirror/gpseg2 gpseg3[wslu@h93 gpsql]$ ls data/primary/gpseg0 gpseg1[wslu@h93 gpsql]$[wslu@h94 gpsql]$ ls data/master mirror primary[wslu@h94 gpsql]$ ls data/master/[wslu@h94 gpsql]$ ls data/primary/gpseg2 gpseg3[wslu@h94 gpsql]$ ls data/mirror/gpseg0 gpseg1[wslu@h94 gpsql]$ 在 h94 初始化备 master（主备 master 必须在不同主机，如果要配置单机多节点，则不能配置备 master。这是因为目前主备 master 必须在相同目录，所以必然不同主机。如果端口不是5432，那么需要指定PGPORT）： 1234567891011121314151617181920212223242526272829303132[wslu@h93 gpsql]$ PGPORT=5432 PGDATABASE=postgres gpinitstandby -s h9420160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Validating environment and parameters for standby initialization...20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Checking for filespace directory /home/wslu/gp/gpsql/data/master/gpseg-1 on h9420160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:------------------------------------------------------20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum standby master initialization parameters20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:------------------------------------------------------20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum master hostname = h9320160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum master data directory = /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum master port = 543220160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum standby master hostname = h9420160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum standby master port = 543220160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum standby master data directory = /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-Greenplum update system catalog = On20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:------------------------------------------------------20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:- Filespace locations20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:------------------------------------------------------20160114:14:40:47:003933 gpinitstandby:h93:wslu-[INFO]:-pg_system -&gt; /home/wslu/gp/gpsql/data/master/gpseg-1Do you want to continue with standby master initialization? Yy|Nn (default=N):&gt; y20160114:14:40:53:003933 gpinitstandby:h93:wslu-[INFO]:-Syncing Greenplum Database extensions to standby20160114:14:40:53:003933 gpinitstandby:h93:wslu-[INFO]:-The packages on h94 are consistent.20160114:14:40:53:003933 gpinitstandby:h93:wslu-[INFO]:-Adding standby master to catalog...20160114:14:40:53:003933 gpinitstandby:h93:wslu-[INFO]:-Database catalog updated successfully.20160114:14:40:54:003933 gpinitstandby:h93:wslu-[INFO]:-Updating pg_hba.conf file...20160114:14:41:00:003933 gpinitstandby:h93:wslu-[INFO]:-pg_hba.conf files updated successfully.20160114:14:41:09:003933 gpinitstandby:h93:wslu-[INFO]:-Updating filespace flat files...20160114:14:41:09:003933 gpinitstandby:h93:wslu-[INFO]:-Filespace flat file updated successfully.20160114:14:41:10:003933 gpinitstandby:h93:wslu-[INFO]:-Starting standby master20160114:14:41:10:003933 gpinitstandby:h93:wslu-[INFO]:-Checking if standby master is running on host: h94 in directory: /home/wslu/gp/gpsql/data/master/gpseg-120160114:14:41:11:003933 gpinitstandby:h93:wslu-[INFO]:-Cleaning up pg_hba.conf backup files...20160114:14:41:17:003933 gpinitstandby:h93:wslu-[INFO]:-Backup files of pg_hba.conf cleaned up successfully.20160114:14:41:17:003933 gpinitstandby:h93:wslu-[INFO]:-Successfully created standby master on h94 此时，h94的data/master目录就不为空了： 12$ [wslu@h94 gpsql]$ ls data/master/gpseg-1 1.10. 测试12345678910111213[wslu@h93 gpsql]$ psql -p 5432 postgrespsql (8.3devel)Type &quot;help&quot; for help.postgres=#postgres=#postgres=# \\db List of tablespaces Name | Owner | Filespae Name------------+-------+--------------- pg_default | wslu | pg_system pg_global | wslu | pg_system(2 rows) 至此，集群完成了初始化。 1.11. 补充：如何将所有节点部署在一台主机？如果要将所有节点配置在一台主机，比如：在 h93 配置2个主 segment、2个镜像 segment、1个 master，只需要把hostfile_config中的 h94 删掉，然后在 h93 删除 data/primary，data/mirror，data/master 目录下的内容，重新初始化即可。 2. GreenPlum 常用指令说明： 每次使用集群的任何指令前，必须执行： 12$ source greenplum-path.sh$ exportMASTER_DATA_DIRECTORY=/home/wslu/gp/gpsql/data/master/gpseg-1 下文不再赘述。 2.1. 启动集群手动启动集群： 1$ gpstart –a 2.2. 停止集群1$ gpstop –a 2.3. 重启集群1$ gpstop –a –r 2.4. 查看集群状态1$ gpstate –m | -e 2.5. reload 配置文件在不停止集群情况下，若配置文件发生变更，reload配置文件： 1$ gpstop –u 2.6. 维护模式下启动 master仅仅启动 master 来执行维护管理任务，不会影响 segment 中的数据。例如，在维护模式下你可以仅连接 master 实例的数据库并且编辑系统表设置。 以维护模式启动 master： 1$ gpstart –m 维护模式下连接 master 来维护系统表。例如： 1$ PGOPTIONS=&#x27;-c gp_session_role=utility&#x27; psql template1 完成管理任务后，使 master 关闭工具模式。然后，重启进入正常模式： 1$ gpstop -m 2.7. 访问数据库可以使用 psql 连接集群： 12345 [wslu@h93 gpsql]$ psql -p 5432 postgrespsql (8.3devel)Type &quot;help&quot; for help.postgres=# 2.8. GUC 参数配置使用 gpconfig 设置 guc 参数： 1$ gpconfig -c gp_vmem_protect_limit -v4096MB gpconfig 可以设置 master 和所有 segment 的 guc 参数，也可以使用 --masteronly 参数只设置 master 的参数。设置完 guc 参数后需要根据 guc 参数类型决定重启集群或 reload 配置文件。 显示guc参数： 12$ psql –c ‘showstatement_mem;’ 或 gpconfig –show statement_mem$ psql –c ‘show all;’ 或 gpconfig –l 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/categories/GreenPlum/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/tags/GreenPlum/"}]},{"title":"实用工具 | PostgreSQL 数据库压力测试工具 pgbench 使用示例","slug":"example-of-postgresql-pgbench","date":"2015-12-23T13:04:17.000Z","updated":"2021-09-24T13:52:42.949Z","comments":true,"path":"2015/12/23/example-of-postgresql-pgbench/","link":"","permalink":"http://dbkernel.github.io/2015/12/23/example-of-postgresql-pgbench/","excerpt":"","text":"本文首发于 2015-12-23 21:04:17 环境PG数据库提供了一款轻量级的压力测试工具叫 pgbench，其实就是一个编译好后的扩展性的可执行文件。 测试环境： CentOS 5.7 in VMWare 8.0 PG：9.1.2 数据库参数： max_connection=100 其他默认 注意： 本文只为说明 pgbench 的使用方法，因此，并未对数据库参数调优。 安装进入源码安装包，编译、安装： 123cd postgresql-9.1.2/contrib/pgbench/make allmake install 安装完毕以后可以在 bin 文件夹下看到新生成的 pgbench 文件： 12$ ll $PGHOME/bin/pgbench-rwxr-xr-x. 1 postgres postgres 50203 Jul 8 20:28 pgbench 参数介绍12345678910111213141516171819202122232425262728293031323334353637[postgres@localhost bin]$ pgbench --helppgbench is a benchmarking tool for PostgreSQL.Usage: pgbench [OPTIONS]... [DBNAME]Initialization options: -i invokes initialization mode -F NUM fill factor -s NUM scaling factorBenchmarking options: -c NUM number of concurrent database clients (default: 1) -C establish new connection for each transaction -D VARNAME=VALUE define variable for use by custom script -f FILENAME read transaction script from FILENAME -j NUM number of threads (default: 1) -l write transaction times to log file -M &#123;simple|extended|prepared&#125; protocol for submitting queries to server (default: simple) -n do not run VACUUM before tests -N do not update tables &quot;pgbench_tellers&quot; and &quot;pgbench_branches&quot; -r report average latency per command -s NUM report this scale factor in output -S perform SELECT-only transactions -t NUM number of transactions each client runs (default: 10) -T NUM duration of benchmark test in seconds -v vacuum all four standard tables before testsCommon options: -d print debugging output -h HOSTNAME database server host or socket directory -p PORT database server port number -U USERNAME connect as specified database user --help show this help, then exit --version output version information, then exit 部分参数中文含义： 1234567891011121314151617181920212223-c, --client=NUM数据库客户端数量, 可以理解为数据库会话数量(postgres进程数), 默认为1-C, --connect每个事务创建一个连接,由于PG使用进程模型, 可以测试频繁Kill/Create进程的性能表现-j, --jobs=NUMpgbench的工作线程数-T, --time=NUM以秒为单位的压测时长-v, --vacuum-all每次测试前执行vacuum命令, 避免&quot;垃圾&quot;空间的影响-M, --protocol=simple|extended|prepared提交查询命令到服务器使用的协议, simple是默认选项, prepared是类似绑定-r, --report-latencies报告每条命令(SQL语句)的平均延时-S, --select-only只执行查询语句 初始化测试数据初始化数据： 1234567891011121314151617[postgres@localhost ~]$ pgbench -i pgbenchcreating tables...10000 tuples done.20000 tuples done.30000 tuples done.40000 tuples done.50000 tuples done.60000 tuples done.70000 tuples done.80000 tuples done.90000 tuples done.100000 tuples done.set primary key...NOTICE: ALTER TABLE / ADD PRIMARY KEY will create implicit index &quot;pgbench_branches_pkey&quot; for table &quot;pgbench_branches&quot;NOTICE: ALTER TABLE / ADD PRIMARY KEY will create implicit index &quot;pgbench_tellers_pkey&quot; for table &quot;pgbench_tellers&quot;NOTICE: ALTER TABLE / ADD PRIMARY KEY will create implicit index &quot;pgbench_accounts_pkey&quot; for table &quot;pgbench_accounts&quot;vacuum...done. 查看表数据： 123456789101112131415161718192021222324252627[postgres@localhost ~]$ psql -d pgbenchpsql (9.1.2)Type &quot;help&quot; for help.pgbench=# select count(1) from pgbench_accounts; count-------- 100000(1 row)pgbench=# select count(1) from pgbench_branches; count------- 1(1 row)pgbench=# select count(1) from pgbench_history; count------- 0(1 row)pgbench=# select count(1) from pgbench_tellers; count------- 10(1 row) 查看表结构： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849pgbench=# \\d+ pgbench_accounts Table &quot;public.pgbench_accounts&quot; Column | Type | Modifiers | Storage | Description----------+---------------+-----------+----------+------------- aid | integer | not null | plain | bid | integer | | plain | abalance | integer | | plain | filler | character(84) | | extended |Indexes: &quot;pgbench_accounts_pkey&quot; PRIMARY KEY, btree (aid)Has OIDs: noOptions: fillfactor=100pgbench=# \\d+ pgbench_branches Table &quot;public.pgbench_branches&quot; Column | Type | Modifiers | Storage | Description----------+---------------+-----------+----------+------------- bid | integer | not null | plain | bbalance | integer | | plain | filler | character(88) | | extended |Indexes: &quot;pgbench_branches_pkey&quot; PRIMARY KEY, btree (bid)Has OIDs: noOptions: fillfactor=100pgbench=# \\d+ pgbench_history Table &quot;public.pgbench_history&quot; Column | Type | Modifiers | Storage | Description--------+-----------------------------+-----------+----------+------------- tid | integer | | plain | bid | integer | | plain | aid | integer | | plain | delta | integer | | plain | mtime | timestamp without time zone | | plain | filler | character(22) | | extended |Has OIDs: nopgbench=# \\d+ pgbench_tellers Table &quot;public.pgbench_tellers&quot; Column | Type | Modifiers | Storage | Description----------+---------------+-----------+----------+------------- tid | integer | not null | plain | bid | integer | | plain | tbalance | integer | | plain | filler | character(84) | | extended |Indexes: &quot;pgbench_tellers_pkey&quot; PRIMARY KEY, btree (tid)Has OIDs: noOptions: fillfactor=100 说明： 这里使用的是默认的参数值，-s参数时可指定测试数据的数据量，-f可以指定测试的脚本，这里用的是默认脚本。 不要在生产的库上做，新建一个测试库（当生产上有同名的测试表时将被重置）。 测试1个session1234567891011121314151617181920212223242526[postgres@localhost ~]$ nohup pgbench -c 1 -T 20 -r pgbench &gt; file.out 2&gt;&amp;1[postgres@localhost ~]$ more file.outnohup: ignoring inputstarting vacuum...end.transaction type: TPC-B (sort of)scaling factor: 1query mode: simplenumber of clients: 1number of threads: 1duration: 20 snumber of transactions actually processed: 12496 tps = 624.747958 (including connections establishing) tps = 625.375564 (excluding connections establishing)statement latencies in milliseconds: 0.005299 \\set nbranches 1 * :scale 0.000619 \\set ntellers 10 * :scale 0.000492 \\set naccounts 100000 * :scale 0.000700 \\setrandom aid 1 :naccounts 0.000400 \\setrandom bid 1 :nbranches 0.000453 \\setrandom tid 1 :ntellers 0.000430 \\setrandom delta -5000 5000 0.050707 BEGIN; 0.200909 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 0.098718 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 0.111621 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 0.107297 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.095156 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 0.919101 END; 2. 50个session1234567891011121314151617181920212223242526[postgres@localhost ~]$nohup pgbench -c 50 -T 20 -r pgbench &gt; file.out 2&gt;&amp;1[postgres@localhost ~]$ more file.outnohup: ignoring inputstarting vacuum...end.transaction type: TPC-B (sort of)scaling factor: 1query mode: simplenumber of clients: 50number of threads: 1duration: 20 snumber of transactions actually processed: 7504 tps = 370.510431 (including connections establishing) tps = 377.964565 (excluding connections establishing)statement latencies in milliseconds: 0.004291 \\set nbranches 1 * :scale 0.000769 \\set ntellers 10 * :scale 0.000955 \\set naccounts 100000 * :scale 0.000865 \\setrandom aid 1 :naccounts 0.000513 \\setrandom bid 1 :nbranches 0.000580 \\setrandom tid 1 :ntellers 0.000522 \\setrandom delta -5000 5000 0.604671 BEGIN; 1.480723 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 0.401148 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 104.713566 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 21.562787 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.412209 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 2.243497 END; 3. 100个session超过100个会报错，因为数据库当前设置最大 session 是100。 1234567891011121314151617181920212223242526[postgres@localhost ~]$ nohup pgbench -c 100 -T 20 -r pgbench&gt; file.out 2&gt;&amp;1[postgres@localhost ~]$ more file.outnohup: ignoring inputstarting vacuum...end.transaction type: TPC-B (sort of)scaling factor: 1query mode: simplenumber of clients: 100number of threads: 1duration: 20 snumber of transactions actually processed: 6032 tps = 292.556692 (including connections establishing) tps = 305.595090 (excluding connections establishing)statement latencies in milliseconds: 0.004508 \\set nbranches 1 * :scale 0.000787 \\set ntellers 10 * :scale 0.000879 \\set naccounts 100000 * :scale 0.001620 \\setrandom aid 1 :naccounts 0.000485 \\setrandom bid 1 :nbranches 0.000561 \\setrandom tid 1 :ntellers 0.000656 \\setrandom delta -5000 5000 3.660809 BEGIN; 4.198062 UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; 1.727076 SELECT abalance FROM pgbench_accounts WHERE aid = :aid; 281.955832 UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; 27.054125 UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; 0.524155 INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 2.710619 END; 参考http://www.postgresql.org/docs/9.1/static/pgbench.html 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"pgbench","slug":"pgbench","permalink":"http://dbkernel.github.io/tags/pgbench/"}]},{"title":"特性分析 | PostgreSQL Primary/Standby 主备流复制机制","slug":"postgresql-primary-standby-streaming-replication","date":"2015-11-21T12:02:26.000Z","updated":"2021-09-24T13:56:03.183Z","comments":true,"path":"2015/11/21/postgresql-primary-standby-streaming-replication/","link":"","permalink":"http://dbkernel.github.io/2015/11/21/postgresql-primary-standby-streaming-replication/","excerpt":"","text":"本文首发于 2015-11-21 20:02:26 引言PostgreSQL 在 9.0 之后引入了主备流复制机制，通过流复制，备库不断的从主库同步相应的数据，并在备库 apply 每个 WAL record，这里的流复制每次传输单位是 WAL 日志的 record 。而 PostgreSQL 9.0 之前提供的方法是主库写完一个 WAL 日志文件后，才把 WAL 日志文件传送到备库，这样的方式导致主备延迟特别大。同时，PostgreSQL 9.0 之后提供了 Hot Standby，备库在应用 WAL record 的同时也能够提供只读服务，大大提升了用户体验。 主备总体结构PostgreSQL 主备流复制的核心部分由 walsender，walreceiver 和 startup 三个进程组成。 walsender 进程是用来发送 WAL 日志记录的，执行顺序如下： 1PostgresMain()-&gt;exec_replication_command()-&gt;StartReplication()-&gt;WalSndLoop()-&gt;XLogSendPhysical() walreceiver 进程是用来接收 WAL 日志记录的，执行顺序如下： 1sigusr1_handler()-&gt;StartWalReceiver()-&gt;AuxiliaryProcessMain()-&gt;WalReceiverMain()-&gt;walrcv_receive() startup 进程是用来 apply 日志的，执行顺序如下： 1PostmasterMain()-&gt;StartupDataBase()-&gt;AuxiliaryProcessMain()-&gt;StartupProcessMain()-&gt;StartupXLOG() walsender 和 walreceiver 进程流复制过程walsender 和 walreceiver 交互主要分为以下几个步骤： walreceiver 启动后通过 recovery.conf 文件中的 primary_conninfo 参数信息连向主库，主库通过连接参数 replication=true 启动 walsender 进程； walreceiver 执行 identify_system 命令，获取主库 systemid/timeline/xlogpos 等信息，执行 TIMELINE_HISTORY 命令拉取 history 文件； 执行 wal_startstreaming 开始启动流复制，通过 walrcv_receive 获取 WAL 日志，期间也会回应主库发过来的心跳信息(接收位点、flush 位点、apply 位点)，向主库发送 feedback 信息(最老的事务 id)，避免 vacuum 删掉备库正在使用的记录； 执行 walrcv_endstreaming 结束流复制，等待 startup 进程更新 receiveStart 和 receiveStartTLI，一旦更新，进入步骤2。 walreceiver和startup进程startup 进程进入 standby 模式和 apply 日志主要过程： 读取 pg_control 文件，找到 redo 位点；读取 recovery.conf，如果配置 standby_mode=on 则进入 standby 模式。 如果是 Hot Standby 需要初始化 clog、subtrans、事务环境等。初始化 redo 资源管理器，比如 Heap、Heap2、Database、XLOG 等。 读取 WAL record，如果 record 不存在需要调用 XLogPageRead-&gt;WaitForWALToBecomeAvailable-&gt;RequestXLogStreaming 唤醒 walreceiver从walsender 获取 WAL record。 对读取的 WAL record 进行 redo，通过 record-&gt;xl_rmid 信息，调用相应的 redo 资源管理器进行 redo 操作。比如 heap_redo 的 XLOG_HEAP_INSERT 操作，就是通过 record 的信息在 buffer page 中增加一个 record： 12345678910111213141516171819202122232425MemSet((char *) htup, 0, sizeof(HeapTupleHeaderData)); /* PG73FORMAT: get bitmap [+ padding] [+ oid] + data */ memcpy((char *) htup + offsetof(HeapTupleHeaderData, t_bits), (char *) xlrec + SizeOfHeapInsert + SizeOfHeapHeader, newlen); newlen += offsetof(HeapTupleHeaderData, t_bits); htup-&gt;t_infomask2 = xlhdr.t_infomask2; htup-&gt;t_infomask = xlhdr.t_infomask; htup-&gt;t_hoff = xlhdr.t_hoff; HeapTupleHeaderSetXmin(htup, record-&gt;xl_xid); HeapTupleHeaderSetCmin(htup, FirstCommandId); htup-&gt;t_ctid = xlrec-&gt;target.tid; offnum = PageAddItem(page, (Item) htup, newlen, offnum, true, true); if (offnum == InvalidOffsetNumber) elog(PANIC, &quot;heap_insert_redo: failed to add tuple&quot;); freespace = PageGetHeapFreeSpace(page); /* needed to update FSM below */ PageSetLSN(page, lsn); if (xlrec-&gt;flags &amp; XLOG_HEAP_ALL_VISIBLE_CLEARED) PageClearAllVisible(page); MarkBufferDirty(buffer); 还有部分 redo 操作(vacuum 产生的 record)需要检查在 Hot Standby模式下的查询冲突，比如某些 tuples 需要 remove，而存在正在执行的query 可能读到这些 tuples，这样就会破坏事务隔离级别。通过函数 ResolveRecoveryConflictWithSnapshot 检测冲突，如果发生冲突，那么就把这个 query 所在的进程 kill 掉。 检查一致性，如果一致了，Hot Standby 模式可以接受用户只读查询；更新共享内存中 XLogCtlData 的 apply 位点和时间线；如果恢复到时间点，时间线或者事务id需要检查是否恢复到当前目标； 回到步骤3，读取next WAL record 。 本文转自：http://mysql.taobao.org/monthly/2015/10/04/ 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"主从同步","slug":"主从同步","permalink":"http://dbkernel.github.io/tags/%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"}]},{"title":"应用案例 | PostgreSQL libpq 网络库接口操作数据库示例","slug":"example-of-postgresql-libpq","date":"2015-11-12T01:23:46.000Z","updated":"2021-09-22T15:24:33.493Z","comments":true,"path":"2015/11/12/example-of-postgresql-libpq/","link":"","permalink":"http://dbkernel.github.io/2015/11/12/example-of-postgresql-libpq/","excerpt":"","text":"本文首发于 2015-11-12 09:23:46 前言本文成文较早，测试程序验证的是 PostgreSQL 9.3 版本的 libpq，理论上，对其他版本也应该适用。 关于 libpq 各个函数接口的说明，参考：http://www.postgres.cn/docs/9.3/libpq.html 示例1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/* * testlibpq.c * * Test the C version of libpq, the PostgreSQL frontend library. */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;libpq-fe.h&gt;static voidexit_nicely(PGconn *conn)&#123; PQfinish(conn); exit(1);&#125;intmain(int argc, char **argv)&#123; const char *conninfo; PGconn *conn; PGresult *res; int nFields; int i, j; /* * If the user supplies a parameter on the command line, use it as the * conninfo string; otherwise default to setting dbname=postgres and using * environment variables or defaults for all other connection parameters. */ if (argc &gt; 1) conninfo = argv[1]; else conninfo = &quot;dbname = postgres&quot;; /* Make a connection to the database */ conn = PQconnectdb(conninfo); /* Check to see that the backend connection was successfully made */ if (PQstatus(conn) != CONNECTION_OK) &#123; fprintf(stderr, &quot;Connection to database failed: %s&quot;, PQerrorMessage(conn)); exit_nicely(conn); &#125; /* * Our test case here involves using a cursor, for which we must be inside * a transaction block. We could do the whole thing with a single * PQexec() of &quot;select * from pg_database&quot;, but that&#x27;s too trivial to make * a good example. */ /* Start a transaction block */ res = PQexec(conn, &quot;BEGIN&quot;); if (PQresultStatus(res) != PGRES_COMMAND_OK) &#123; fprintf(stderr, &quot;BEGIN command failed: %s&quot;, PQerrorMessage(conn)); PQclear(res); exit_nicely(conn); &#125; /* * Should PQclear PGresult whenever it is no longer needed to avoid memory * leaks */ PQclear(res); /* * Fetch rows from pg_database, the system catalog of databases */ res = PQexec(conn, &quot;DECLARE myportal CURSOR FOR select * from pg_database&quot;); if (PQresultStatus(res) != PGRES_COMMAND_OK) &#123; fprintf(stderr, &quot;DECLARE CURSOR failed: %s&quot;, PQerrorMessage(conn)); PQclear(res); exit_nicely(conn); &#125; PQclear(res); res = PQexec(conn, &quot;FETCH ALL in myportal&quot;); if (PQresultStatus(res) != PGRES_TUPLES_OK) &#123; fprintf(stderr, &quot;FETCH ALL failed: %s&quot;, PQerrorMessage(conn)); PQclear(res); exit_nicely(conn); &#125; /* first, print out the attribute names */ nFields = PQnfields(res); for (i = 0; i &lt; nFields; i++) printf(&quot;%-15s&quot;, PQfname(res, i)); printf(&quot;\\n\\n&quot;); /* next, print out the rows */ for (i = 0; i &lt; PQntuples(res); i++) &#123; for (j = 0; j &lt; nFields; j++) printf(&quot;%-15s&quot;, PQgetvalue(res, i, j)); printf(&quot;\\n&quot;); &#125; PQclear(res); /* close the portal ... we don&#x27;t bother to check for errors ... */ res = PQexec(conn, &quot;CLOSE myportal&quot;); PQclear(res); /* end the transaction */ res = PQexec(conn, &quot;END&quot;); PQclear(res); /* close the connection to the database and cleanup */ PQfinish(conn); return 0;&#125; 示例2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129/* * testlibpq2.c * Test of the asynchronous notification interface * * Start this program, then from psql in another window do * NOTIFY TBL2; * Repeat four times to get this program to exit. * * Or, if you want to get fancy, try this: * populate a database with the following commands * (provided in src/test/examples/testlibpq2.sql): * * CREATE TABLE TBL1 (i int4); * * CREATE TABLE TBL2 (i int4); * * CREATE RULE r1 AS ON INSERT TO TBL1 DO * (INSERT INTO TBL2 VALUES (new.i); NOTIFY TBL2); * * and do this four times: * * INSERT INTO TBL1 VALUES (10); */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#include &lt;sys/time.h&gt;#include &lt;libpq-fe.h&gt;static voidexit_nicely(PGconn *conn)&#123; PQfinish(conn); exit(1);&#125;intmain(int argc, char **argv)&#123; const char *conninfo; PGconn *conn; PGresult *res; PGnotify *notify; int nnotifies; /* * If the user supplies a parameter on the command line, use it as the * conninfo string; otherwise default to setting dbname=postgres and using * environment variables or defaults for all other connection parameters. */ if (argc &gt; 1) conninfo = argv[1]; else conninfo = &quot;dbname = postgres&quot;; /* Make a connection to the database */ conn = PQconnectdb(conninfo); /* Check to see that the backend connection was successfully made */ if (PQstatus(conn) != CONNECTION_OK) &#123; fprintf(stderr, &quot;Connection to database failed: %s&quot;, PQerrorMessage(conn)); exit_nicely(conn); &#125; /* * Issue LISTEN command to enable notifications from the rule&#x27;s NOTIFY. */ res = PQexec(conn, &quot;LISTEN TBL2&quot;); if (PQresultStatus(res) != PGRES_COMMAND_OK) &#123; fprintf(stderr, &quot;LISTEN command failed: %s&quot;, PQerrorMessage(conn)); PQclear(res); exit_nicely(conn); &#125; /* * should PQclear PGresult whenever it is no longer needed to avoid memory * leaks */ PQclear(res); /* Quit after four notifies are received. */ nnotifies = 0; while (nnotifies &lt; 4) &#123; /* * Sleep until something happens on the connection. We use select(2) * to wait for input, but you could also use poll() or similar * facilities. */ int sock; fd_set input_mask; sock = PQsocket(conn); if (sock &lt; 0) break; /* shouldn&#x27;t happen */ FD_ZERO(&amp;input_mask); FD_SET(sock, &amp;input_mask); if (select(sock + 1, &amp;input_mask, NULL, NULL, NULL) &lt; 0) &#123; fprintf(stderr, &quot;select() failed: %s\\n&quot;, strerror(errno)); exit_nicely(conn); &#125; /* Now check for input */ PQconsumeInput(conn); while ((notify = PQnotifies(conn)) != NULL) &#123; fprintf(stderr, &quot;ASYNC NOTIFY of &#x27;%s&#x27; received from backend PID %d\\n&quot;, notify-&gt;relname, notify-&gt;be_pid); PQfreemem(notify); nnotifies++; &#125; &#125; fprintf(stderr, &quot;Done.\\n&quot;); /* close the connection to the database and cleanup */ PQfinish(conn); return 0;&#125; 示例3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211/* * testlibpq3.c * Test out-of-line parameters and binary I/O. * * Before running this, populate a database with the following commands * (provided in src/test/examples/testlibpq3.sql): * * CREATE TABLE test1 (i int4, t text, b bytea); * * INSERT INTO test1 values (1, &#x27;joe&#x27;&#x27;s place&#x27;, &#x27;\\\\000\\\\001\\\\002\\\\003\\\\004&#x27;); * INSERT INTO test1 values (2, &#x27;ho there&#x27;, &#x27;\\\\004\\\\003\\\\002\\\\001\\\\000&#x27;); * * The expected output is: * * tuple 0: got * i = (4 bytes) 1 * t = (11 bytes) &#x27;joe&#x27;s place&#x27; * b = (5 bytes) \\000\\001\\002\\003\\004 * * tuple 0: got * i = (4 bytes) 2 * t = (8 bytes) &#x27;ho there&#x27; * b = (5 bytes) \\004\\003\\002\\001\\000 */#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;libpq-fe.h&gt;/* for ntohl/htonl */#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;static voidexit_nicely(PGconn *conn)&#123; PQfinish(conn); exit(1);&#125;/* * This function prints a query result that is a binary-format fetch from * a table defined as in the comment above. We split it out because the * main() function uses it twice. */static voidshow_binary_results(PGresult *res)&#123; int i, j; int i_fnum, t_fnum, b_fnum; /* Use PQfnumber to avoid assumptions about field order in result */ i_fnum = PQfnumber(res, &quot;i&quot;); t_fnum = PQfnumber(res, &quot;t&quot;); b_fnum = PQfnumber(res, &quot;b&quot;); for (i = 0; i &lt; PQntuples(res); i++) &#123; char *iptr; char *tptr; char *bptr; int blen; int ival; /* Get the field values (we ignore possibility they are null!) */ iptr = PQgetvalue(res, i, i_fnum); tptr = PQgetvalue(res, i, t_fnum); bptr = PQgetvalue(res, i, b_fnum); /* * The binary representation of INT4 is in network byte order, which * we&#x27;d better coerce to the local byte order. */ ival = ntohl(*((uint32_t *) iptr)); /* * The binary representation of TEXT is, well, text, and since libpq * was nice enough to append a zero byte to it, it&#x27;ll work just fine * as a C string. * * The binary representation of BYTEA is a bunch of bytes, which could * include embedded nulls so we have to pay attention to field length. */ blen = PQgetlength(res, i, b_fnum); printf(&quot;tuple %d: got\\n&quot;, i); printf(&quot; i = (%d bytes) %d\\n&quot;, PQgetlength(res, i, i_fnum), ival); printf(&quot; t = (%d bytes) &#x27;%s&#x27;\\n&quot;, PQgetlength(res, i, t_fnum), tptr); printf(&quot; b = (%d bytes) &quot;, blen); for (j = 0; j &lt; blen; j++) printf(&quot;\\\\%03o&quot;, bptr[j]); printf(&quot;\\n\\n&quot;); &#125;&#125;intmain(int argc, char **argv)&#123; const char *conninfo; PGconn *conn; PGresult *res; const char *paramValues[1]; int paramLengths[1]; int paramFormats[1]; uint32_t binaryIntVal; /* * If the user supplies a parameter on the command line, use it as the * conninfo string; otherwise default to setting dbname=postgres and using * environment variables or defaults for all other connection parameters. */ if (argc &gt; 1) conninfo = argv[1]; else conninfo = &quot;dbname = postgres&quot;; /* Make a connection to the database */ conn = PQconnectdb(conninfo); /* Check to see that the backend connection was successfully made */ if (PQstatus(conn) != CONNECTION_OK) &#123; fprintf(stderr, &quot;Connection to database failed: %s&quot;, PQerrorMessage(conn)); exit_nicely(conn); &#125; /* * The point of this program is to illustrate use of PQexecParams() with * out-of-line parameters, as well as binary transmission of data. * * This first example transmits the parameters as text, but receives the * results in binary format. By using out-of-line parameters we can * avoid a lot of tedious mucking about with quoting and escaping, even * though the data is text. Notice how we don&#x27;t have to do anything * special with the quote mark in the parameter value. */ /* Here is our out-of-line parameter value */ paramValues[0] = &quot;joe&#x27;s place&quot;; res = PQexecParams(conn, &quot;SELECT * FROM test1 WHERE t = $1&quot;, 1, /* one param */ NULL, /* let the backend deduce param type */ paramValues, NULL, /* don&#x27;t need param lengths since text */ NULL, /* default to all text params */ 1); /* ask for binary results */ if (PQresultStatus(res) != PGRES_TUPLES_OK) &#123; fprintf(stderr, &quot;SELECT failed: %s&quot;, PQerrorMessage(conn)); PQclear(res); exit_nicely(conn); &#125; show_binary_results(res); PQclear(res); /* * In this second example we transmit an integer parameter in binary * form, and again retrieve the results in binary form. * * Although we tell PQexecParams we are letting the backend deduce * parameter type, we really force the decision by casting the parameter * symbol in the query text. This is a good safety measure when sending * binary parameters. */ /* Convert integer value &quot;2&quot; to network byte order */ binaryIntVal = htonl((uint32_t) 2); /* Set up parameter arrays for PQexecParams */ paramValues[0] = (char *) &amp;binaryIntVal; paramLengths[0] = sizeof(binaryIntVal); paramFormats[0] = 1; /* binary */ res = PQexecParams(conn, &quot;SELECT * FROM test1 WHERE i = $1::int4&quot;, 1, /* one param */ NULL, /* let the backend deduce param type */ paramValues, paramLengths, paramFormats, 1); /* ask for binary results */ if (PQresultStatus(res) != PGRES_TUPLES_OK) &#123; fprintf(stderr, &quot;SELECT failed: %s&quot;, PQerrorMessage(conn)); PQclear(res); exit_nicely(conn); &#125; show_binary_results(res); PQclear(res); /* close the connection to the database and cleanup */ PQfinish(conn); return 0;&#125; 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"libpq","slug":"libpq","permalink":"http://dbkernel.github.io/tags/libpq/"}]},{"title":"特性介绍 | PostgreSQL 的依赖约束详解 - 系统表 pg_depend & pg_constraint","slug":"postgresql-dependency-constraint-details","date":"2015-11-04T07:28:08.000Z","updated":"2021-09-24T04:12:19.830Z","comments":true,"path":"2015/11/04/postgresql-dependency-constraint-details/","link":"","permalink":"http://dbkernel.github.io/2015/11/04/postgresql-dependency-constraint-details/","excerpt":"","text":"本文首发于 2015-11-04 15:28:08 前言本文成文较早，依赖的是 PostgreSQL 9.3 版本，后续内核版本可能不兼容，但核心原理是相通的，可做参考。 表结构pg_dependpg_depend 是 postgres 的一张系统表，用来记录数据库对象之间的依赖关系，除了常见的主外键，还有其他一些内部依赖关系，可以通过这个系统表呈现出来。 123456789101112131415postgres=# \\d+ pg_depend Table &quot;pg_catalog.pg_depend&quot; Column | Type | Modifiers | Storage | Stats target | Description-------------+---------+-----------+---------+--------------+------------- classid | oid | not null | plain | | 系统OID objid | oid | not null | plain | | 对象OID objsubid | integer | not null | plain | | refclassid | oid | not null | plain | | 引用系统OID refobjid | oid | not null | plain | | 引用对象ID refobjsubid | integer | not null | plain | | deptype | &quot;char&quot; | not null | plain | | pg_depend类型Indexes: &quot;pg_depend_depender_index&quot; btree (classid, objid, objsubid) &quot;pg_depend_reference_index&quot; btree (refclassid, refobjid, refobjsubid)Has OIDs: no OID 是 Object Identifier 的缩写，是对象 ID 的意思，因为是无符号的4字节类型，表示范围不够大，所以一般不用做主键使用，仅用在系统内部，比如系统表等应用。可以与一些整型数字进行转换。与之相关的系统参数是 default_with_oids ，默认是 off 。 pg_depend.deptype 字段自 9.1 版本之后多了一个 extension 的类型，目前类型有： DEPENDENCY_NORMAL (n) ：普通的依赖对象，如表与schema的关系。 DEPENDENCY_AUTO (a) ：自动的依赖对象，如主键约束。 DEPENDENCY_INTERNAL (i) ：内部的依赖对象，通常是对象本身。 DEPENDENCY_EXTENSION (e) ：9.1新增的的扩展依赖。 DEPENDENCY_PIN (p) ：系统内置的依赖。 pg_constraint123456789101112131415161718192021222324252627282930313233postgres=# \\d pg_constraint Table &quot;pg_catalog.pg_constraint&quot; Column | Type | Modifiers---------------+--------------+----------- conname | name | not null -- 约束名 connamespace | oid | not null -- 约束所在命名空间的OID contype | &quot;char&quot; | not null -- 约束类型 condeferrable | boolean | not null -- 约束是否可以推迟 condeferred | boolean | not null -- 缺省情况下，约束是否可以推迟 convalidated | boolean | not null -- 约束是否经过验证 conrelid | oid | not null -- 约束所在的表的OID contypid | oid | not null -- 约束所在的域的OID conindid | oid | not null -- 如果是唯一、主键、外键或排除约束，则为支持这个约束的索引；否则为0 confrelid | oid | not null -- 如果是外键，则为参考的表；否则为 0 confupdtype | &quot;char&quot; | not null -- 外键更新操作代码 confdeltype | &quot;char&quot; | not null -- 外键删除操作代码 confmatchtype | &quot;char&quot; | not null -- 外键匹配类型 conislocal | boolean | not null coninhcount | integer | not null -- 约束直接继承祖先的数量 connoinherit | boolean | not null conkey | smallint[] | -- 如果是表约束（包含外键，但是不包含约束触发器），则是约束字段的列表 confkey | smallint[] | -- 如果是一个外键，是参考的字段的列表 conpfeqop | oid[] | -- 如果是一个外键，是PK = FK比较的相等操作符的列表 conppeqop | oid[] | -- 如果是一个外键，是PK = PK比较的相等操作符的列表 conffeqop | oid[] | -- 如果是一个外键，是FK = FK比较的相等操作符的列表 conexclop | oid[] | -- 如果是一个排除约束，是每个字段排除操作符的列表 conbin | pg_node_tree | -- 如果是一个检查约束，那就是其表达式的内部形式 consrc | text | -- 如果是检查约束，则是表达式的人类可读形式Indexes: &quot;pg_constraint_oid_index&quot; UNIQUE, btree (oid) &quot;pg_constraint_conname_nsp_index&quot; btree (conname, connamespace) &quot;pg_constraint_conrelid_index&quot; btree (conrelid) &quot;pg_constraint_contypid_index&quot; btree (contypid) 查询依赖关系的 SQL如下 SQL 可以列出系统和用户对象的各种依赖关系： 1234567891011121314151617181920212223SELECT classid::regclass AS &quot;depender object class&quot;, CASE classid WHEN &#x27;pg_class&#x27;::regclass THEN objid::regclass::text WHEN &#x27;pg_type&#x27;::regclass THEN objid::regtype::text WHEN &#x27;pg_proc&#x27;::regclass THEN objid::regprocedure::text ELSE objid::text END AS &quot;depender object identity&quot;, objsubid, refclassid::regclass AS &quot;referenced object class&quot;, CASE refclassid WHEN &#x27;pg_class&#x27;::regclass THEN refobjid::regclass::text WHEN &#x27;pg_type&#x27;::regclass THEN refobjid::regtype::text WHEN &#x27;pg_proc&#x27;::regclass THEN refobjid::regprocedure::text ELSE refobjid::text END AS &quot;referenced object identity&quot;, refobjsubid, CASE deptype WHEN &#x27;p&#x27; THEN &#x27;pinned&#x27; WHEN &#x27;i&#x27; THEN &#x27;internal&#x27; WHEN &#x27;a&#x27; THEN &#x27;automatic&#x27; WHEN &#x27;n&#x27; THEN &#x27;normal&#x27; END AS &quot;dependency type&quot;FROM pg_catalog.pg_depend WHERE (objid &gt;= 16384 OR refobjid &gt;= 16384); 我通常喜欢在 where 后面加个条件 and deptype &lt;&gt;&#39;i&#39; ，以排除 internal 依赖。 示例创建一张表： 12postgres=# create table tbl_parent(id int);CREATE TABLE 执行查询依赖关系的 SQL： 1234postgres=# 执行上面的SQL; depender object class | depender object identity | objsubid | referenced object class | referenced object identity | refobjsubid | dependency type-----------------------+--------------------------+----------+-------------------------+------------- pg_class | tbl_parent | 0 | pg_namespace | 2200 | 0 | normal(1 row) 看起来只是建了个表，没有约束，实际上该表是建立在 schema 下面的，因此只依赖于 schema 。 添加主键约束： 123456postgres=# alter table tbl_parent add primary key(id);ALTER TABLE depender object class | depender object identity | objsubid | referenced object class | referenced object identity | refobjsubid | dependency type-----------------------+--------------------------+----------+-------------------------+------- pg_class | tbl_parent | 0 | pg_namespace | 2200 | 0 | normal pg_constraint | 16469 | 0 | pg_class | tbl_parent | 1 | automatic(2 rows) 约束类型变为了automatic，表明这个主键约束是依赖于表上的，是自动模式，详细信息可以在系统表 pg_constrant 里面查询。 正常情况下用户删除有依赖关系的对象时，会提示需要先删除依赖的对象。但是如果通过系统表删除有依赖关系的对象时，若操作有误，就会导致异常。例如：下面的操作就会导致报错cache lookup failed for constraint： 12345678910111213141516postgres=# select oid,conname,connamespace,contype from pg_constraint where conname like &#x27;tbl_parent%&#x27;; oid | conname | connamespace | contype-------+-----------------+--------------+--------- 16469 | tbl_parent_pkey | 2200 | p(1 row)postgres=# delete from pg_constraint where conname like &#x27;tbl_parent%&#x27;;DELETE 1postgres=# select oid,conname,connamespace,contype from pg_constraint where conname like &#x27;tbl_parent%&#x27;; oid | conname | connamespace | contype-----+---------+--------------+---------(0 rows)postgres=# drop table tbl_parent;ERROR: cache lookup failed for constraint 16469 --16496是约束的OIDpostgres=# 之所以出现该报错，是因为手动把约束对象删除了，但在 pg_depend 里却仍然存在依赖关系，因此，删除该表时，由于找不到最里层的依赖对象而报错。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"pg_depend","slug":"pg-depend","permalink":"http://dbkernel.github.io/tags/pg-depend/"},{"name":"pg_constraint","slug":"pg-constraint","permalink":"http://dbkernel.github.io/tags/pg-constraint/"}]},{"title":"程序人生 | C 语言编译器对内存空间的分配原则","slug":"c-compiler-memory-allocation-principles","date":"2015-05-04T06:50:16.000Z","updated":"2021-09-24T03:53:41.784Z","comments":true,"path":"2015/05/04/c-compiler-memory-allocation-principles/","link":"","permalink":"http://dbkernel.github.io/2015/05/04/c-compiler-memory-allocation-principles/","excerpt":"","text":"本文首发于 2015-05-04 14:50:16 概述一个由 C/C++ 编译的程序占用的内存分为以下几个部分： 栈区（stack）：由编译器自动分配、释放，存放函数的参数值、局部变量的值等，其操作方式类似于数据结构中的栈。一般大家常说的堆栈和栈是一样的，就是栈(stack)，而说 堆 时才是堆 heap 。 堆区（heap）：一般由程序员分配释放，若程序员不释放，程序结束时由OS回收。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。 全局区（静态区，static）：全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。程序结束后由系统释放。 文字常量区：常量字符串就是放在这里的。程序结束后由系统释放。 程序代码区：存放函数体的二进制代码。 举例来说： 1234567891011121314151617//main.cppint a = 0; // 全局初始化区char *p1; // 全局未初始化区main()&#123;int b; // 栈char s[] = &quot;abc&quot;; // 栈char *p2; // 栈char *p3 = &quot;123456&quot;; // 123456\\0 在常量区，p3在栈上。static int c =0； // 全局（静态）初始化区// 分配的 10 个和 20 个字节的区域就在堆区。p1 = (char *)malloc(10);p2 = (char *)malloc(20);strcpy(p1, &quot;123456&quot;); // 123456\\0 放在常量区，编译器可能会将它与p3所指向的&quot;123456&quot;优化成同一个位置&#125; 在函数体中定义的变量通常是在栈上，用malloc, calloc, realloc等分配内存的函数分配得到的就是在堆上。 在所有函数体外定义的是全局量，加了static修饰符后不管在哪里都存放在全局区（静态区），在所有函数体外定义的static变量表示在该文件中有效，不能extern到别的文件使用，在函数体内定义的static表示只在该函数体内有效。 函数中的”123456”这样的字符串存放在常量区。 还有就是函数调用时会在栈上有一系列的保留现场及传递参数的操作。 关于堆和栈1. 内存分配方面堆： 一般由程序员分配释放，若程序员不释放，程序结束时可能由OS回收。它与数据结构中的堆是两回事，分配方式是类似于链表。可能用到的关键字如下：new、malloc、delete、free等等。 栈： 由编译器(Compiler)自动分配释放，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。当一个函数调用完返回后它会释放该函数中所有的栈空间。 2. 申请方式栈（stack）： 由系统自动分配。例如，声明在函数中一个局部变量 int b;，系统自动在栈中为b开辟空间。 堆（heap）： 需要程序员自己申请，并指明大小。 在C中 malloc 函数： 1p1 = (char *)malloc(10); 在C++中用 new 运算符： 1p2 = (char *)new char(10); 但注意 p1、p2 本身是在栈中的。 3. 申请后系统的响应栈： 只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。 堆： 首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序。对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样代码中的 delete 语句才能正确的释放本内存空间。另外，由于找到的堆结点的大小不一定正好等于申请的大小，系统会自动的将多余的那部分重新放入空闲链表中。 4. 申请大小的限制栈： 在 Windows 下，栈是向低地址扩展的数据结构，是一块连续的内存的区域。这句话的意思是栈顶的地址和栈的最大容量是系统预先规定好的，在 Windows 下，栈的大小一般是2M，如果申请的空间超过栈的剩余空间时，将提示 overflow。因此，能从栈获得的空间较小。 栈不够用的情况一般是程序中分配了大量数组和递归函数层次太深。 当一个函数调用完返回后它会释放该函数中所有的栈空间。 堆： 堆是向高地址扩展的数据结构，是不连续的内存区域。这是由于系统是用链表来存储的空闲内存地址的，自然是不连续的，而链表的遍历方向是由低地址向高地址。堆的大小受限于计算机系统中有效的虚拟内存。由此可见，堆获得的空间比较灵活，也比较大。 5. 申请效率的比较栈由系统自动分配，速度较快。但程序员是无法控制的。 堆是由malloc分配的内存，一般速度比较慢，而且容易产生内存碎片，不过用起来最方便。 6. 堆和栈中的存储内容栈： 在函数调用时，第一个进栈的是主函数后的下一条指令（函数调用语句的下一条可执行语句）的地址，然后是函数的各个参数（在大多数的C编译器中，参数是由右往左入栈的），然后是函数中的局部变量。注意静态变量是不入栈的。 当本次函数调用结束后，局部变量先出栈，然后是参数，最后栈顶指针指向最开始存的地址，也就是主函数中的下一条指令，程序由该点继续运行。 堆： 一般是在堆的头部用一个字节存放堆的大小。堆中的具体内容有程序员安排。 7. 存取效率方面堆： char *s1 = &quot;Hellow Word&quot;；是在编译时就确定的； 栈：char s1[] = &quot;Hellow Word&quot;；是在运行时赋值的；用数组比用指针速度要快一些，因为指针在底层汇编中需要用edx寄存器中转一下，而数组在栈上直接读取。 在栈上存取数据比通过指针在堆上存取数据快些。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"gcc","slug":"gcc","permalink":"http://dbkernel.github.io/tags/gcc/"},{"name":"编译器","slug":"编译器","permalink":"http://dbkernel.github.io/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"},{"name":"内存分配","slug":"内存分配","permalink":"http://dbkernel.github.io/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"}]},{"title":"程序人生 | unix 网络编程之 getaddrinfo 函数详解及使用举例","slug":"unix-getaddrinfo-function-detailed-usage","date":"2015-01-03T13:04:36.000Z","updated":"2021-09-24T05:24:35.708Z","comments":true,"path":"2015/01/03/unix-getaddrinfo-function-detailed-usage/","link":"","permalink":"http://dbkernel.github.io/2015/01/03/unix-getaddrinfo-function-detailed-usage/","excerpt":"","text":"本文首发于 2015-01-03 21:04:36 概述IPv4 中使用 gethostbyname() 函数完成主机名到地址解析，这个函数仅仅支持 IPv4 ，且不允许调用者指定所需地址类型的任何信息，返回的结构只包含了用于存储 IPv4 地址的空间。 IPv6中引入了getaddrinfo()的新API，它是协议无关的，既可用于 IPv4 也可用于IPv6 。 getaddrinfo函数能够处理名字到地址以及服务到端口这两种转换，返回的是一个addrinfo的结构（列表）指针而不是一个地址清单。这些addrinfo结构随后可由socket函数直接使用。 getaddrinfo函数把协议相关性安全隐藏在这个库函数内部。应用程序只要处理由getaddrinfo函数填写的套接口地址结构。该函数在 POSIX规范中定义了。 函数说明包含头文件： 123#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt; 函数原型： 1int getaddrinfo( const char *hostname, const char *service, const struct addrinfo *hints, struct addrinfo **result ); 参数说明： hostname: 一个主机名或者地址串(IPv4 的点分十进制串或者 IPv6 的 16 进制串)。 service：服务名可以是十进制的端口号，也可以是已定义的服务名称，如 ftp、http 等。 hints：可以是一个空指针，也可以是一个指向某个 addrinfo 结构体的指针，调用者在这个结构中填入关于期望返回的信息类型的暗示。 result：本函数通过 result 指针参数返回一个指向 addrinfo 结构体链表的指针。 返回值： 0：成功；非0：出错。 参数设置在getaddrinfo函数之前通常需要对以下6个参数进行以下设置：nodename、servname、hints的ai_flags、ai_family、ai_socktype、ai_protocol。 在6项参数中，对函数影响最大的是nodename，sername和hints.ai_flag，而ai_family只是有地址为v4地址或v6地址的区别。ai_protocol一般为0不作改动。 getaddrinfo在实际使用中的几种常用参数设置： 一般情况下，client/server编程中，server端调用bind（如果面向连接的还需要listen）；client则无需调用bind函数，解析地址后直接connect（面向连接）或直接发送数据（无连接）。因此，比较常见的情况有： 通常服务器端在调用getaddrinfo之前，ai_flags设置AI_PASSIVE，用于bind；主机名nodename通常会设置为NULL，返回通配地址[::]。 客户端调用getaddrinfo时，ai_flags一般不设置AI_PASSIVE，但是主机名nodename和服务名servname（更愿意称之为端口）则应该不为空。 当然，即使不设置AI_PASSIVE，取出的地址也并非不可以被bind，很多程序中ai_flags直接设置为0，即3个标志位都不设置，这种情况下只要hostname和servname设置的没有问题就可以正确bind。 上述情况只是简单的client/server中的使用，但实际在使用getaddrinfo和查阅国外开源代码的时候，曾遇到一些将servname（即端口）设为NULL的情况（当然，此时nodename必不为NULL，否则调用getaddrinfo会报错）。 使用须知1）如果本函数返回成功，那么由result参数指向的变量已被填入一个指针，它指向的是由其中的ai_next成员串联起来的addrinfo结构链表。 1234567891011struct addrinfo&#123; int ai_flags; int ai_family; int ai_socktype; int ai_protocol; size_t ai_addrlen; struct sockaddr *ai_addr; /* 我认为这个成员是这个函数最大的便利。 */ char *ai_canonname; struct addrinfo *ai_next;&#125;; 其中，sockaddr结构体为： 123456在linux环境下，结构体struct sockaddr在/usr/include/linux/socket.h中定义，具体如下：typedef unsigned short sa_family_t;struct sockaddr &#123; sa_family_t sa_family; /* address family, AF_xxx */ char sa_data[14]; /* 14 bytes of protocol address */&#125; 而且，sockaddr一般要转换为sockaddr_in： 1234567struct sockaddr_in&#123; short int sin_family; unsigned short int sin_port; struct in_addr sin_addr; unsigned char sin_zero[8];&#125; 2）可以导致返回多个addrinfo结构的情形有以下2个： 如果与hostname参数关联的地址有多个，那么适用于所请求地址簇的每个地址都返回一个对应的结构。 如果service参数指定的服务支持多个套接口类型，那么每个套接口类型都可能返回一个对应的结构，具体取决于hints结构的ai_socktype成员。 举例来说：如果指定的服务既支持TCP也支持UDP，那么调用者可以把hints结构中的ai_socktype成员设置成SOCK_DGRAM，使得返回的仅仅是适用于数据报套接口的信息。 3）我们必须先分配一个hints结构，把它清零后填写需要的字段，再调用getaddrinfo，然后遍历一个链表逐个尝试每个返回地址。 4）getaddrinfo解决了把主机名和服务名转换成套接口地址结构的问题。 5）如果getaddrinfo出错，那么返回一个非0的错误值。输出出错信息，不要用perror，而应该用gai_strerror，该函数原型为： 1const char *gai_strerror( int error ); 该函数以getaddrinfo返回的非0错误值的名字和含义为他的唯一参数，返回一个指向对应的出错信息串的指针。 6）由getaddrinfo返回的所有存储空间都是动态获取的，这些存储空间必须通过调用freeaddrinfo返回给系统，该函数原型为： 1void freeaddrinfo( struct addrinfo *ai ); ai参数应指向由getaddrinfo返回的第一个addrinfo结构。 这个链表中的所有结构以及它们指向的任何动态存储空间都被释放掉。 示例1. 根据主机名获取IP地址123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;arpa/inet.h&gt;int main(int argc, char **argv)&#123; if (argc != 2) &#123; printf(&quot;Usag: ./a.out hostname|ip\\n&quot;); exit(1); &#125; struct addrinfo hints; struct addrinfo *res, *cur; int ret; struct sockaddr_in *addr; char ipbuf[16]; int port; memset(&amp;hints, 0, sizeof(struct addrinfo)); hints.ai_family = AF_INET; /* Allow IPv4 */ hints.ai_flags = AI_PASSIVE; /* For wildcard IP address */ hints.ai_protocol = 0; /* Any protocol */ hints.ai_socktype = SOCK_DGRAM; ret = getaddrinfo(argv[1], NULL,&amp;hints,&amp;res); if (ret &lt; 0) &#123; fprintf(stderr, &quot;%s\\n&quot;, gai_strerror(ret)); exit(1); &#125; for (cur = res; cur != NULL; cur = cur-&gt;ai_next) &#123; addr = (struct sockaddr_in *)cur-&gt;ai_addr; printf(&quot;ip: %s\\n&quot;, inet_ntop(AF_INET, &amp;addr-&gt;sin_addr, ipbuf, 16)); printf(&quot;port: %d\\n&quot;, inet_ntop(AF_INET, &amp;addr-&gt;sin_port, (void *)&amp;port, 2)); //printf(&quot;port: %d\\n&quot;, ntohs(addr-&gt;sin_port)); &#125; freeaddrinfo(res); exit(0);&#125; 2. 根据主机名和端口号获取地址信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdarg.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;time.h&gt;#include &lt;ctype.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/wait.h&gt;#include &lt;signal.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/stat.h&gt;#include &lt;netdb.h&gt;#include &lt;sys/socket.h&gt;void main()&#123; struct addrinfo *ailist, *aip; struct addrinfo hint; struct sockaddr_in *sinp; char *hostname = &quot;localhost&quot;; char buf[INET_ADDRSTRLEN]; char *server = &quot;6543&quot;; /* 这是服务端口号 */ const char *addr; int ilRc; hint.ai_family = AF_UNSPEC; /* hint 的限定设置 */ hint.ai_socktype = 0; /* 这里可是设置 socket type . 比如 SOCK——DGRAM */ hint.ai_flags = AI_PASSIVE; /* flags 的标志很多 。常用的有AI_CANONNAME; */ hint.ai_protocol = 0; /* 设置协议 一般为0，默认 */ hint.ai_addrlen = 0; /* 下面不可以设置，为0，或者为NULL */ hint.ai_canonname = NULL; hint.ai_addr = NULL; hint.ai_next = NULL; ilRc = getaddrinfo(hostname, server, &amp;hint, &amp;ailist); if (ilRc &lt; 0) &#123; printf(&quot;str_error = %s\\n&quot;, gai_strerror(errno)); return; &#125; /* 显示获取的信息 */ for (aip = ailist; aip != NULL; aip = aip-&gt;ai_next) &#123; sinp = (struct sockaddr_in *)aip-&gt;ai_addr; addr = inet_ntop(AF_INET, &amp;sinp-&gt;sin_addr, buf, INET_ADDRSTRLEN); printf(&quot; addr = %s, port = %d\\n&quot;, addr?addr:&quot;unknow &quot;, ntohs(sinp-&gt;sin_port)); &#125;&#125; 3. 由内核分配随机端口（再也不担心端口被占了）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;arpa/inet.h&gt;#define MAX_CONN_COUNT 10 #define INVALID_SOCKET (~0)int main(int argc, char **argv)&#123; int motionListenPort = 0; int motion_sock = 0; int err; int maxconn; char familyDesc[32]; struct sockaddr_storage motion_sock_addr; socklen_t alen; struct addrinfo *addrs = NULL, *addr, hints; int ret; int tries = 0; memset(&amp;hints, 0, sizeof(struct addrinfo)); hints.ai_family = AF_INET; /* Allow IPv4 */ hints.ai_flags = AI_PASSIVE; /* For wildcard IP address */ hints.ai_protocol = 0; /* Any protocol */ hints.ai_socktype = SOCK_STREAM; ret = getaddrinfo(NULL, &quot;0&quot;, &amp;hints, &amp;addrs); if (ret &lt; 0) &#123; fprintf(stderr, &quot;%s\\n&quot;, gai_strerror(ret)); exit(1); &#125; for (addr = addrs; addr != NULL; addr = addr-&gt;ai_next) &#123; /* Create the socket. */ if ((motion_sock = socket(addr-&gt;ai_family, SOCK_STREAM, 0)) == INVALID_SOCKET) &#123; fprintf(stderr, &quot;Error:could not create socket for the motion\\n&quot;); continue; &#125; /* Bind it to a kernel assigned port on localhost and get the assigned port via getsockname(). */ if (bind(motion_sock, addr-&gt;ai_addr, addr-&gt;ai_addrlen) &lt; 0) &#123; fprintf(stderr, &quot;Error: could not bind socket for the motion\\n&quot;); close(motion_sock); motion_sock = INVALID_SOCKET; continue; &#125; alen = sizeof(motion_sock_addr); if (getsockname(motion_sock, (struct sockaddr *) &amp;(motion_sock_addr), &amp;alen) &lt; 0) &#123; fprintf(stderr, &quot;could not get address of socket for the motion\\n&quot;); close(motion_sock); motion_sock = INVALID_SOCKET; continue; &#125; /* Resolve the motion listen port. */ switch(motion_sock_addr.ss_family) &#123; case AF_INET: &#123; struct sockaddr_in *motion_addr = (struct sockaddr_in *) &amp;motion_sock_addr; motionListenPort = ntohs(motion_addr-&gt;sin_port); strcpy(familyDesc, &quot;IPv4&quot;); fprintf(stdout, &quot;motionListenPort=%d, familyDesc = %s\\n&quot;, motionListenPort, familyDesc); break; &#125; case AF_INET6: &#123; struct sockaddr_in6 *motion_addr = (struct sockaddr_in6 *) &amp;motion_sock_addr; motionListenPort = ntohs(motion_addr-&gt;sin6_port); strcpy(familyDesc, &quot;IPv6&quot;); fprintf(stdout, &quot;motionListenPort=%d, familyDesc = %s\\n&quot;, motionListenPort, familyDesc); break; &#125; default: &#123; fprintf(stderr, &quot;Error:unrecognized address family \\&quot;%d\\&quot; for the motion\\n&quot;, motion_sock_addr.ss_family); continue; &#125; &#125; /* 监听 */ maxconn = MAX_CONN_COUNT; err = listen(motion_sock, maxconn); if (err &lt; 0) &#123; fprintf(stderr, &quot;could not listen on socket for the motion\\n&quot;); close(motion_sock); motion_sock = INVALID_SOCKET; continue; &#125; &#125; if (motion_sock == INVALID_SOCKET) goto listen_failed; /* XXXXXX socket通信过程 */ freeaddrinfo(addrs); close(motion_sock); return 0;listen_failed: fprintf(stderr, &quot;Error: failed to listen for the motion\\n&quot;); if (addrs) freeaddrinfo(addrs); if (motion_sock != INVALID_SOCKET) close(motion_sock); motion_sock = INVALID_SOCKET; return -1;&#125; 4. 使用ioctl获取指定网卡IP地址123456789101112131415161718192021222324252627282930313233#include &lt;arpa/inet.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#define ETH_NAME &quot;eth0&quot;int main()&#123; int sock; struct sockaddr_in sin; struct ifreq ifr; sock = socket(AF_INET, SOCK_DGRAM, 0); if (sock == -1) &#123; perror(&quot;socket&quot;); return -1; &#125; strncpy(ifr.ifr_name, ETH_NAME, IFNAMSIZ); ifr.ifr_name[IFNAMSIZ - 1] = 0; if (ioctl(sock, SIOCGIFADDR, &amp;ifr) &lt; 0) &#123; perror(&quot;ioctl&quot;); return -1; &#125; memcpy(&amp;sin, &amp;ifr.ifr_addr, sizeof(sin)); fprintf(stdout, &quot;eth0: %s\\n&quot;, inet_ntoa(sin.sin_addr)); return 0;&#125; 5. 使用ping指令，根据hostname获取ip地址本例未用 getaddrinfo，而是采用shell指令方法（不推荐）。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;sys/ioctl.h&gt;#include &lt;netinet/in.h&gt;#include &lt;net/if.h&gt;#include &lt;net/if_arp.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;errno.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;#include &lt;string.h&gt;//使用ping指令，根据hostname获取ip地址int getIpAddrByHostname(char *hostname, char* ip_addr, size_t ip_size)&#123; char command[256]; FILE *f; char *ip_pos; snprintf(command, 256, &quot;ping -c1 %s | head -n 1 | sed &#x27;s/^[^(]*(\\\\([^)]*\\\\).*$/\\\\1/&#x27;&quot;, hostname); fprintf(stdout, &quot;%s\\n&quot;, command); if ((f = popen(command, &quot;r&quot;)) == NULL) &#123; fprintf(stderr, &quot;could not open the command, \\&quot;%s\\&quot;, %s\\n&quot;, command, strerror(errno)); return -1; &#125; fgets(ip_addr, ip_size, f); fclose(f); ip_pos = ip_addr; for (;*ip_pos &amp;&amp; *ip_pos!= &#x27;\\n&#x27;; ip_pos++); *ip_pos = 0; return 0;&#125;int main()&#123; char addr[64] = &#123;0&#125;; getIpAddrByHostname(&quot;localhost&quot;, addr, INET_ADDRSTRLEN); fprintf(stdout, &quot;localhost: %s\\n&quot;, addr); return 0;&#125; 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"APUE","slug":"APUE","permalink":"http://dbkernel.github.io/tags/APUE/"},{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"网络编程","slug":"网络编程","permalink":"http://dbkernel.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"程序人生 | Linux Daemon 程序设计示例","slug":"example-of-linux-daemon-program-design","date":"2014-08-08T09:55:32.000Z","updated":"2021-09-24T13:52:05.947Z","comments":true,"path":"2014/08/08/example-of-linux-daemon-program-design/","link":"","permalink":"http://dbkernel.github.io/2014/08/08/example-of-linux-daemon-program-design/","excerpt":"","text":"本文首发于 2014-08-08 17:55:32 概念daemon 程序，又称为守护进程，通常在系统后台长时间运行，由于没有控制终端而无法与前台交互。daemon程序一般作为系统服务使用，Linux系统中运行着很多这样的守护进程，如 iptables，nfs，ypbind，dhcpd 等。 daemon 程序设计步骤 程序运行后调用fork，并让父进程退出。子进程获得一个新的进程ID，但继承了父进程的进程组ID。 调用setsid创建一个新的session，使自己成为新session和新进程组的leader，并使进程没有控制终端(tty)。 设置文件创建mask为0，避免创建文件时权限的影响。 关闭不需要的打开文件描述符。因为 daemon 程序在后台执行，不需要于终端交互，通常就关闭STDIN、STDOUT和STDERR。其它根据实际情况处理。 daemon 无法输出信息，可以使用SYSLOG或自己的日志系统进行日志处理。（可选） 编写管理 daemon 的SHELL脚本，使用service对 daemon 进行管理和监控。（可选） 示例daemon 程序源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//这里使用自己的日志系统，当然也可以使用SYSLOG。#define LOGBUFSZ 256 /*log buffer size*/#define LOGFILE &quot;/var/log/wsiod.log&quot; /*log filename*/int wsio_logit(char * func, char *msg, ...)&#123; va_list args; char prtbuf[LOGBUFSZ]; int save_errno; struct tm *tm; time_t current_time; int fd_log; save_errno = errno; va_start (args, msg); (void) time (¤t_time); /* Get current time */ tm = localtime (¤t_time); sprintf (prtbuf, &quot;%02d/%02d %02d:%02d:%02d %s &quot;, tm-&gt;tm_mon+1, tm-&gt;tm_mday, tm-&gt;tm_hour, tm-&gt;tm_min, tm-&gt;tm_sec, func); vsprintf (prtbuf+strlen(prtbuf), msg, args); va_end (args); fd_log = open (LOGFILE, O_WRONLY | O_CREAT | O_APPEND, 0664); write (fd_log, prtbuf, strlen(prtbuf)); close (fd_log); errno = save_errno; return 0;&#125;int init_daemon(void)&#123; pid_t pid; int i; /* parent exits , child continues */ if((pid = fork()) &lt; 0) return -1; else if(pid != 0) exit(0); setsid(); /* become session leader */ for(i=0;i&lt;= 2;++i) /* close STDOUT, STDIN, STDERR, */ close(i); umask(0); /* clear file mode creation mask */ return 0;&#125;void sig_term(int signo)&#123; if(signo == SIGTERM) /* catched signal sent by kill(1) command */ &#123; wsio_logit(&quot;&quot;, &quot;wsiod stopped/n&quot;); exit(0); &#125;&#125;/* main program of daemon */int main(void)&#123;if(init_daemon() == -1)&#123;printf(&quot;can&#x27;t fork self/n&quot;);exit(0); &#125; wsio_logit(&quot;&quot;, &quot;wsiod started/n&quot;); signal(SIGTERM, sig_term); /* arrange to catch the signal */ while (1) &#123; // Do what you want here … … &#125; exit(0);&#125; daemon 程序管理脚本daemon 程序可以使用 service 工具进行管理，包括启动、停止、查看状态等，但前题是需要编写一个如下的简单SHELL脚本，比如 /etc/init.d/wsiod ： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/sh## wsiod This shell script takes care of starting and stopping wsiod.## chkconfig: 35 65 35# description: wsiod is web servce I/O server, which is used to access files on remote hosts.# Source function library.. /etc/rc.d/init.d/functions# Source networking configuration.. /etc/sysconfig/network# Check that networking is up.[ $&#123;NETWORKING&#125; = &quot;no&quot; ] &amp;&amp; exit 0RETVAL=0prog=&quot;wsiod&quot;WSIOARGS=&quot;-h $HOSTNAME -p 80 -t STANDALONE -k -c -d&quot;start() &#123; # Start daemons. echo -n $&quot;Starting $prog: &quot; daemon /usr/local/bin/wsiod $&#123;WSIOARGS&#125; RETVAL=$? echo [ $RETVAL -eq 0 ] &amp;&amp; touch /var/lock/subsys/wsiod return $RETVAL&#125;stop() &#123; # Stop daemons. echo -n $&quot;Shutting down $prog: &quot; killproc wsiod RETVAL=$? echo [ $RETVAL -eq 0 ] &amp;&amp; rm -f /var/lock/subsys/wsiod return $RETVAL&#125;# See how we were called.case &quot;$1&quot; in start) start ;; stop) stop ;; restart|reload) stop start RETVAL=$? ;; status) status wsiod RETVAL=$? ;; *) echo $&quot;Usage: $0 &#123;start|stop|restart|status&#125;&quot; exit 1esacexit $RETVAL daemon 程序指令由上述脚本可知，该 daemon 程序支持的指令有 start|stop|restart|reload|status ，以启动 daemon 程序为例，指令为： 1/etc/init.d/wsiod start 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"},{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"shell","slug":"shell","permalink":"http://dbkernel.github.io/tags/shell/"},{"name":"daemon","slug":"daemon","permalink":"http://dbkernel.github.io/tags/daemon/"}]},{"title":"系统运维 | Ubuntu 下安装配置samba 服务的详细过程","slug":"how-to-install-configure-samba-service-in-ubuntu","date":"2014-08-05T02:14:48.000Z","updated":"2021-09-24T04:07:48.629Z","comments":true,"path":"2014/08/05/how-to-install-configure-samba-service-in-ubuntu/","link":"","permalink":"http://dbkernel.github.io/2014/08/05/how-to-install-configure-samba-service-in-ubuntu/","excerpt":"","text":"本文首发于 2014-08-05 10:14:48 1. Samba作用Samba的主要任务就是实现Linux系统和Windows系统之间的资源共享。我们现在是要在Linux下配置Samba，让Windows的用户可以访问你的PC。 当然，也可用于VMWare虚拟机与宿主机之间的资源共享。 2. 安装我是在ubuntu上实现的，所以我只需在配置好ubuntu的更新源之后，在终端中使用一下两句命令，就可以安装Samba的软件包 12sudo apt-get install smabasudo apt-get install smbfs 3. Samba服务的构成Samba的核心是两个守护进程smbd和nmbd 。它们的配置信息都保存在/etc/samba/smb.conf里面。 其中smbd处理Samba软件与Linux协商，nmbd使其他主机能浏览Linux服务器。 4. Samba配置文件配置文件为/etc/samba/smb.conf，如果担心改了之后有问题，可以先备份一下： 1sudo cp /etc/samba/smb.conf /etc/samba/smb_conf_backup 一个完整的Samba配置文件包含两部分： Samba Global Settings 全局参数设置 该部分由[global]段来完成配置，主要是设置整体的规则。其中参数workgroup比较特殊，用于提供NT域名或者工作组名，需要根据实际情况修改： 1workgroup=mygroup Share Definitions 共享定义 有很多段，都用[]标志开始的，需要根据实际情况修改。 语法说明： 每个部分有消息头和参数构成，消息头用[]表示，如[global]就是一个消息头。 参数的结构形式是parameter=value。 注释用 # 表示，这个和shell脚本有点像。 有一些配置前面有 ; ，这个表示这一行的配置可以更改，如需修改，则要去掉;，配置才可能生效。 5. 示例5.1. 设置共享目录假定共享目录为/home/share/samba： 12sudo mkdir -p /home/share/sambasudo chmod 777 /home/share/samba 5.2. 修改配置文件修改 global 段： 12345[global] workgroup = WORKGROUP display charset = UTF-8 unix charset = UTF-8 dos charset = cp936 添加Share段： 123456789101112[Share] comment = Shared Folder with username and password path = /home/share/samba public = yes writable = no valid users = user create mask = 0300 directory mask = 0300 force user = nobody force group = nogroup available = yes browseable = yes 搜索到 security 配置项，修改为： 12security = userusername map = /etc/samba/smbusers 保存并关闭配置文件。 5.3. 添加Samba用户12sudo useradd user #增加了一个叫做user的用户sudo smbpasswd user #修改user的对samba服务的密码，系统会提示输入密码 5.4. 重启服务1sudo /etc/init.d/samba restart 5.5. 使用 在windows系统下使用 方法一：在IE地址栏中输入：\\\\你的IP，然后回车，可能要求你输入用户名和密码（第5.3小节设定的）。 方法二：在网上邻居中新建邻居，在路径中输入: \\\\你的IP\\Share，然后点击下一步完成（可能会要求输入用户名和密码）。 在Linux下访问：在终端中挂载文件系统 1sudo mount -t smbfs -o username=user,password=123456 //218.*.*.*/Share /mnt 其中，-t参数指示了文件系统的类型，username是用户名，password是密码，218.*.*.*是你的IP，Share是在配置文件中已经指明的段名，/mnt是要挂载到的文件夹。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"},{"name":"系统运维","slug":"系统运维","permalink":"http://dbkernel.github.io/tags/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}]},{"title":"程序人生 | 我的《C陷阱与缺陷》读书笔记","slug":"c-traps-and-pitfalls-reading-notes","date":"2014-08-04T09:56:55.000Z","updated":"2021-09-24T03:57:40.372Z","comments":true,"path":"2014/08/04/c-traps-and-pitfalls-reading-notes/","link":"","permalink":"http://dbkernel.github.io/2014/08/04/c-traps-and-pitfalls-reading-notes/","excerpt":"","text":"本文首发于 2014-08-04 17:56:55 第一章 词法“陷阱”1. =不同于==12if(x = y) break; 实际上是将y赋给x，再检查x是否为0。 如果真的是这样预期，那么应该改为： 12if((x = y) != 0) break; 2. &amp;和| 不同于 &amp;&amp; 和 ||3. 词法分析中的“贪心法”编译器将程序分解成符号的方法是：从左到有一个一个字符的读入，如果该字符可能组成一个符号，那么再读入下一个字符，判断已经读入的两个字符组成的字符床是否可能是一个符号的组成部分；如果可能，继续读入下一个字符，重复上述判断，直到读入的字符组成的字符串已不再可能组成一个有意义的符号。例如： 1y = x/*p; 会被解析为：/* 注释符号 4. 整型常量010(八进制数) 不同于 10（十进制）。 5. 字符与字符串首先是单引号与双引号的区别： 用单引号括起来的一个字符表示一个整数（ASCII码），而双引号括起来表示一个指针。 第二章 语法“陷阱”1. 理解函数声明弄懂(*(void(*)())0)(); //首地址为0的函数。 float (*h)(): h是一个指向返回值为浮点型的函数的指针 所以，(float (*)()) 表示一个“指向返回值为浮点型的函数的指针”的类型转换符。 fp(): 是(*fp)( )的简写。 *fp(): 是 *( (*fp) ( ) )的简写。 1( *0 )( ); 虽然上式编译器不认，但可以把0转换为指向“返回值为void的”函数的指针，所以0可变为： ( void(*) ( ) ) 0 ，代入(*0)()，得到： 1(*( void(*) ( ) ) 0) ( ) 该式子用等价于： 12typedef void ( *func ) ( );( *( func ) 0 ) ( ); 类似的，signal.h中对signal函数的声明： 12typedef void (*sighandler_t)(int);sighandler_t signal(int signum, sighandler_t handler); 2. 运算符优先级的问题 优先级 运算符 名称或含义 使用形式 结合方向 说明 1 [] 数组下标 数组名[常量表达式] 左到右 1 () 圆括号 (表达式) 函数名(形参表) 左到右 1 . 成员选择（对象） 对象.成员名 左到右 1 -&gt; 成员选择（指针） 对象指针-&gt;成员名 左到右 2 - 负号运算符 -表达式 右到左 单目运算符 2 (类型) 强制类型转换 (数据类型)表达式 右到左 2 ++ 自增运算符 ++变量名 变量名++ 右到左 单目运算符 2 – 自减运算符 –变量名 变量名– 右到左 单目运算符 2 * 取值运算符 *指针变量 右到左 单目运算符 2 &amp; 取地址运算符 &amp;变量名 右到左 单目运算符 2 ! 逻辑非运算符 !表达式 右到左 单目运算符 2 ~ 按位取反运算符 ~表达式 右到左 单目运算符 2 sizeof 长度运算符 sizeof(表达式) 右到左 3 / 除 表达式 / 表达式 左到右 双目运算符 3 * 乘 表达式*表达式 左到右 双目运算符 3 % 余数（取模） 整型表达式%整型表达式 左到右 双目运算符 4 + 加 表达式+表达式 左到右 双目运算符 4 - 减 表达式-表达式 左到右 双目运算符 5 &lt;&lt; 左移 变量&lt;&lt;表达式 左到右 双目运算符 5 &gt;&gt; 右移 变量&gt;&gt;表达式 左到右 双目运算符 6 &gt; 大于 表达式&gt;表达式 左到右 双目运算符 6 &gt;= 大于等于 表达式&gt;=表达式 左到右 双目运算符 6 &lt; 小于 表达式&lt;表达式 左到右 双目运算符 6 &lt;= 小于等于 表达式&lt;=表达式 左到右 双目运算符 7 == 等于 表达式==表达式 左到右 双目运算符 7 != 不等于 表达式!= 表达式 左到右 双目运算符 8 &amp; 按位与 表达式&amp;表达式 左到右 双目运算符 9 ^ 按位异或 表达式^表达式 左到右 双目运算符 10 | 按位或 表达式|表达式 左到右 双目运算符 11 &amp;&amp; 逻辑与 表达式&amp;&amp;表达式 左到右 双目运算符 12 || 逻辑或 表达式||表达式 左到右 双目运算符 13 ?: 条件运算符 表达式1? 表达式2: 表达式3 右到左 三目运算符 14 = 赋值运算符 变量=表达式 右到左 14 /= 除后赋值 变量/=表达式 右到左 14 *= 乘后赋值 变量*=表达式 右到左 14 %= 取模后赋值 变量%=表达式 右到左 14 += 加后赋值 变量+=表达式 右到左 14 -= 减后赋值 变量-=表达式 右到左 14 &lt;&lt;= 左移后赋值 变量&lt;&lt;=表达式 右到左 14 &gt;&gt;= 右移后赋值 变量&gt;&gt;=表达式 右到左 14 &amp;= 按位与后赋值 变量&amp;=表达式 右到左 14 ^= 按位异或后赋值 变量^=表达式 右到左 14 |= 按位或后赋值 变量|=表达式 右到左 15 , 逗号运算符 表达式,表达式,… 左到右 3. 其他主要是别多写分号，switch别忘了break，别写空else分支。 第三章 语义“陷阱”1. 指针与数组123456789101112131415161718192021222324struct &#123; Int p[4]; Double x;&#125;b[17];int calendar[12][31];int (*p)[31];sizeof(calendar):12*31=372calendar[0] // 指向该一维数组，对应*pcalendar[0][0]......calendar[0][30]calendar[1] // 指向该一维数组，对应*(p+1)calendar[1][0]......calendar[1][30]..................calendar[11] // 指向该一维数组，对应*(p+11)calendar[11][0]......calendar[11][30] 2. 内存分配1free(r); 用malloc显式分配的空间，不会再退出本函数后自动释放掉，而是会等程序员显式释放后才消失。 注意检查，malloc分配的内存可能失败。 C语言中会自动地将作为函数参数的数组声明转换为对应的指针声明，如： 123int strlen(char s[ ])&#123; &#125;等价于int strlen(char *s)&#123; &#125;但在其他情形下不会自动转换，也就是说不等价，如：extern char hello[ ];和extern char *hello;完全不同。 边界计算 自己实现一个memcpy函数： 12345void memcpy(char *dest, const char *source, int k)&#123; while( --k &gt;= 0 ) *dest++ = *source++;&#125; 重点是：操作时一定要知道操作数据的长度。 整数溢出 两个有符号整数相加会发生溢出。 两个无符号整数相加不会发生溢出。 一个有符号和一个无符号整数相加，因为有符号被自动转换成无符号，所以也不会溢出。 第四章 连接编译器一般每次只处理一个文件。编译器的责任是把C源程序翻译成对连接器有意义的形式。 许多系统中的连接器是独立于C语言实现的，因此如果链接时候错误原因是与C语言相关的，连接器无法判断错误原因。但连接器能够理解机器语言和内存布局。 典型的连接器把由汇编器或编译器生成的若干个目标模块，整合成一个被称为载入模块或可执行文件的实体。 连接器通常把目标模块看成是由一组外部对象组成的。每个外部对象代表着机器内存中的某个部分，并通过一个外部名称来识别。因此，程序中的每个函数和每个外部变量，如果没有被声明为static，就都是一个外部对象。static的不会与其它源程序文件中的同名函数或同名变量发生冲突。对于非satatic的函数或变量的名称冲突的解决办法将在后面讨论。 除了外部对象外，目标模块中还可能包括了对其他模块中的外部对象的引用，当连接器读入一个目标模块时，它必须解析出这些引用，并作出标记说明这些外部对象不再是未定义的。 连接器的输入是一组目标模块文件和库文件。输出是一个载入模块。 避免外部变量的函数的冲突和不一致等问题的办法： 每个外部对象只在一个头文件里声明，需要用到该外部对象的所有模块都应该包括这个头文件。 定义该外部对象的模块也应该包括这个头文件。 第五章 库函数没什么好说的，就是apue的一些函数而已。 第六章 预处理器宏定义：主要是理解宏不是函数，而是直接替换。 不能忽视宏定义中的空格：1#define f (x) ( (x)-1 )：因为f后面多了一个空格，所以f(x)代表(x) ( (x)-1 ) 宏并不是函数，所以注意那些括号：12#define abs(x) ( ( (x) &gt;= 0)?(x):-(x) )#define max(a,b) ( (a)&gt;(b)?(a):(b) ) 宏并不是语句：1#define assert(e) if (!e) assert_error(__FILE__, __LINE__) 宏不是类型定义 错误用法：12#define int_8_ int* int_8 a,b; //则a是指针，b是int型 正确用法：应该用typedef1typedef int * int_8_; 第七章 可移植性缺陷主要是： 应对C语言标准的变更； 标识符名称的限制； 整数的大小； 字符是有符号整数还是无符号整数； 移位运算符； 在向右移位时，空出的位是由0填充还是1，还是由符号位的副本填充？如果被移位对象是无符号数，那么由0填充；如果是有符号数，那么是0或符号位的副本。 移位操作的位数允许的取值范围是什么？如果被移位对象的长度是n位，那么移位计数必须大于或等于0，而严格小于n。 移植性需考虑的地方： 机器的字符表不同。 有的机器是one’s complement，有的机器是two’s complement的。基于2的补码的计算机，所允许表示的附属取值范围要大于正数取值范围，所以有时取负值的运算会导致溢出。 各机器对取模运算的定义不同。 第八章 惯用与答案将惯用的c == &#39;\\t&#39;写作&#39;\\t&#39; == c。 一旦写错成=号，编译器就能检查出来。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"}]},{"title":"程序人生 | C语言字节对齐问题详解 - 对齐/字节序/位序/网络序等（下）","slug":"c-language-byte-alignment-problem-in-detail-part-2","date":"2014-07-21T07:35:30.000Z","updated":"2021-09-24T03:56:47.650Z","comments":true,"path":"2014/07/21/c-language-byte-alignment-problem-in-detail-part-2/","link":"","permalink":"http://dbkernel.github.io/2014/07/21/c-language-byte-alignment-problem-in-detail-part-2/","excerpt":"","text":"本文首发于 2014-07-21 15:35:30 6. 附录6.1. 字节序与网络序6.1.1. 字节序字节序，顾名思义就是字节的高低位存放顺序。 对于单字节，大部分处理器以相同的顺序处理比特位，因此单字节的存放和传输方式一般相同。 对于多字节数据，如整型（32位机中一般占4字节），在不同的处理器的存放方式主要有两种（以内存中 0x0A0B0C0D 的存放方式为例）。 6.1.1.1. 大字节序（Big-Endian，又称大端序或大尾序）在计算机中，存储介质以下面方式存储整数 0x0A0B0C0D，则称为大字节序： 数据以8bit为单位：低地址方向 -&gt; 0x0A 0x0B 0x0C 0x0D -&gt; 高地址方向 数据以16bit为单位：低地址方向 -&gt; 0x0A0B 0x0C0D -&gt; 高地址方向 其中，最高有效位(MSB，Most Significant Byte)0x0A存储在最低的内存地址处。下个字节0x0B存在后面的地址处。同时，最高的16bit单元0x0A0B存储在低位。 简而言之，大字节序就是高字节存入低地址，低字节存入高地址。 这里讲个词源典故：“endian”一词来源于乔纳森·斯威夫特的小说《格列佛游记》。小说中，小人国为水煮蛋该从大的一端(Big-End)剥开还是小的一端(Little-End)剥开而争论，争论的双方分别被称为 Big-endians 和 Little-endians 。 1980年，Danny Cohen在其著名的论文”On Holy Wars and a Plea for Peace“中为平息一场关于字节该以什么样的顺序传送的争论而引用了该词。 借用上面的典故，想象一下要把熟鸡蛋旋转着稳立起来，大头（高字节）肯定在下面（低地址）^_^ 6.1.1.2. 小字节序（Little-Endian，又称小端序或小尾序）在计算机中，存储介质以下面方式存储整数 0x0A0B0C0D 则称为小字节序： 数据以8bit为单位：高地址方向 -&gt; 0x0A 0x0B 0x0C 0x0D -&gt; 低地址方向 数据以16bit为单位：高地址方向 -&gt; 0x0A0B 0x0C0D -&gt; 低地址方向 其中，最低有效位(LSB，Least Significant Byte)0x0D存储在最低的内存地址处。后面字节依次存在后面的地址处。同时，最低的16bit单元0x0A0B存储在低位。 可见，小字节序就高字节存入高地址，低字节存入低地址。 C语言中的位域结构也要遵循比特序(类似字节序) 。例如： 1234struct bitfield&#123; unsigned char a: 2; unsigned char b: 6;&#125; 该位域结构占1个字节，假设赋值a=0x01和b=0x02，则大字节机器上该字节为(01)(000010)，小字节机器上该字节为(000010)(01) 。因此在编写可移植代码时，需要加条件编译。 注意，在包含位域的C结构中，若位域A在位域B之前定义，则位域A所占用的内存空间地址低于位域B所占用的内存空间。 另见以下联合体，在小字节机器上若low=0x01，high=0x02，则hex=0x21： 12345678910111213int main(void)&#123; union&#123; unsigned char hex; struct&#123; unsigned char low : 4; unsigned char high : 4; &#125;; &#125;convert; convert.low = 0x01; convert.high = 0x02; printf(&quot;hex = 0x%0x\\n&quot;, convert.hex); return 0;&#125; 6.1.1.3. 注意事项无论是大字节序，还是小字节序，变量的地址都等于变量所占字节中的低地址。例如，下述程序中，小字节序输出 0x0D，大字节序输出 0x0A 。 12int32_t a = 0x0A0B0C0D;printf(&quot;0x%0x\\n&quot;, *((int8_t*)&amp;dwData)); 6.1.2. 网络序网络传输一般采用大字节序，也称为网络字节序或网络序。IP协议中定义大字节序为网络字节序。 对于可移植的代码来说，将接收的网络数据转换成主机的字节序是必须的，一般会有成对的函数用于把网络数据转换成相应的主机字节序或反之（若主机字节序与网络字节序相同，通常将函数定义为空宏）。 伯克利socket API定义了一组转换函数，用于16和32位整数在网络序和主机字节序之间的转换。htonl、htons用于主机序转换到网络序；ntohl、ntohs用于网络序转换到本机序。 注意：在大小字节序转换时，必须考虑待转换数据的长度(如5.1.1节的数据单元)。另外对于单字符或小于单字符的几个bit数据，是不必转换的，因为在机器存储和网络发送的一个字符内的bit位存储顺序是一致的。 6.1.3. 位序用于描述串行设备的传输顺序。一般硬件传输采用小字节序（先传低位），但I2C协议采用大字节序。网络协议中只有数据链路层的底端会涉及到。 6.1.4. 处理器字节序不同处理器体系的字节序如下所示： X86、MOS Technology 6502、Z80、VAX、PDP-11 等处理器为 Little endian； Motorola 6800、Motorola 68000、PowerPC 970、System/370、SPARC(除V9外) 等处理器为 Big endian； ARM、PowerPC (除PowerPC 970外)、DEC Alpha，SPARC V9，MIPS，PA-RISC and IA64 等的字节序是可配置的。 6.1.5. 字节序编程请看下面的语句： 1printf(&quot;%c\\n&quot;, *((short*)&quot;AB&quot;) &gt;&gt; 8); 在大字节序下输出为’A’，小字节序下输出为’B’。 下面的代码可用来判断本地机器字节序： 123456789101112131415161718192021222324252627282930//字节序枚举类型typedef enum&#123; ENDIAN_LITTLE = (INT8U)0X00, ENDIAN_BIG = (INT8U)0X01&#125;E_ENDIAN_TYPE;E_ENDIAN_TYPE GetEndianType(VOID)&#123; INT32U dwData = 0x12345678; // 取数都从低地址开始访问 if(0x78 == *((INT8U*)&amp;dwData)) return ENDIAN_LITTLE; else return ENDIAN_BIG;&#125;//Start of GetEndianTypeTest//#include &lt;endian.h&gt;VOID GetEndianTypeTest(VOID)&#123;#if _BYTE_ORDER == _LITTLE_ENDIAN printf(&quot;[%s]&lt;Test Case&gt; Result: %s, EndianType = %s!\\n&quot;, __FUNCTION__, (ENDIAN_LITTLE != GetEndianType()) ? &quot;ERROR&quot; : &quot;OK&quot;, &quot;Little&quot;);#elif _BYTE_ORDER == _BIG_ENDIAN printf(&quot;[%s]&lt;Test Case&gt; Result: %s, EndianType = %s!\\n&quot;, __FUNCTION__, (ENDIAN_BIG != GetEndianType()) ? &quot;ERROR&quot; : &quot;OK&quot;, &quot;Big&quot;);#endif&#125;//End of GetEndianTypeTest// 在字节序不同的平台间的交换数据时，必须进行转换。比如对于int类型，大字节序写入文件： 12int i = 100;write(fd, &amp;i, sizeof(int)); 小字节序读出后： 1234567891011int i;read(fd, &amp;i, sizeof(int));char buf[sizeof(int)];memcpy(buf, &amp;i, sizeof(int));for(i = 0; i &lt; sizeof(int); i++)&#123; int v = buf[sizeof(int) - i - 1]; buf[sizeof(int) - 1] = buf[i]; buf[i] = v;&#125;memcpy(&amp;i, buf, sizeof(int)); 上面仅仅是个例子。在不同平台间即使不存在字节序的问题，也尽量不要直接传递二进制数据。作为可选的方式就是使用文本来交换数据，这样至少可以避免字节序的问题。 很多的加密算法为了追求速度，都会采取字符串和数字之间的转换，在计算完毕后，必须注意字节序的问题，在某些实现中可以见到使用预编译的方式来完成，这样很不方便，如果使用前面的语句来判断，就可以自动适应。 字节序问题不仅影响异种平台间传递数据，还影响诸如读写一些特殊格式文件之类程序的可移植性。此时使用预编译的方式来完成也是一个好办法。 6.2. 对齐时的填充字节代码如下： 123456789101112struct A&#123; char c; int i; short s;&#125;;int main(void)&#123; struct A a; a.c = 1; a.i = 2; a.s = 3; printf(&quot;sizeof(A)=%d\\n&quot;, sizeof(struct A)); return 0;&#125; 执行后输出为sizeof(A)=12。 6.3. pragma pack语法说明123#pragma pack(n)#pragma pack(push, 1)#pragma pack(pop) 1）#pragma pack(n) 该指令指定结构和联合成员的紧凑对齐。而一个完整的转换单元的结构和联合的紧凑对齐由/Zp选项设置。紧凑对齐用pack编译指示在数据说明层设置。该编译指示在其出现后的第一个结构或者联合声明处生效。该编译指示对定义无效。 当使用#pragma pack (n) 时，n 为1、2、4、8或16。第一个结构成员后的每个结构成员都被存储在更小的成员类型或n字节界限内。如果使用无参量的#pragma pack，结构成员被紧凑为以/Zp指定的值。该缺省/Zp紧凑值为/Zp 8。 2）编译器也支持以下增强型语法： 1#pragma pack( [ [ &#123; push | pop &#125; , ] [identifier, ] ] [ n] ) 若不同的组件使用 pack编译指示 指定不同的紧凑对齐, 这个语法允许你把程序组件组合为一个单独的转换单元。 带push参量的 pack编译指示 的每次出现将当前的紧凑对齐存储到一个内部编译器堆栈中。编译指示的参量表从左到右读取。如果使用push，则当前紧凑值被存储起来；如果给出一个n值，该值将成为新的紧凑值。若指定一个标识符，即选定一个名称，则该标识符将和这个新的的紧凑值联系起来。 带一个pop参量的 pack编译指示 的每次出现都会检索内部编译器堆栈顶的值，并使该值为新的紧凑对齐值。如果使用pop参量且内部编译器堆栈是空的，则紧凑值为命令行给定的值，并将产生一个警告信息。若使用pop且指定一个n值，该值将成为新的紧凑值。 若使用pop且指定一个标识符，所有存储在堆栈中的值将从栈中删除，直到找到一个匹配的标识符。这个与标识符相关的紧凑值也从栈中移出，并且这个仅在标识符入栈之前存在的紧凑值成为新的紧凑值。如果未找到匹配的标识符, 将使用命令行设置的紧凑值，并且将产生一个一级警告。缺省紧凑对齐为8。 pack编译指示 的新的增强功能让你在编写头文件时，确保在遇到该头文件的前后的紧凑值是一样的。 6.4. Intel关于内存对齐的说明以下内容节选自《Intel Architecture 32 Manual》。 字、双字和四字在自然边界上不需要在内存中对齐。（对于字、双字和四字来说，自然边界分别是偶数地址，可以被4整除的地址，和可以被8整除的地址。） 无论如何，为了提高程序的性能，数据结构(尤其是栈)应该尽可能地在自然边界上对齐。原因在于，为了访问未对齐的内存，处理器需要作两次内存访问；然而，对齐的内存访问仅需要一次访问。 一个字或双字操作数跨越了4字节边界，或者一个四字操作数跨越了8字节边界，被认为是未对齐的，从而需要两次总线周期来访问内存。一个字起始地址是奇数但却没有跨越字边界被认为是对齐的，能够在一个总线周期中被访问。 某些操作双四字的指令需要内存操作数在自然边界上对齐。如果操作数没有对齐，这些指令将会产生一个通用保护异常(#GP)。双四字的自然边界是能够被16整除的地址。其他操作双四字的指令允许未对齐的访问(不会产生通用保护异常)，然而，需要额外的内存总线周期来访问内存中未对齐的数据。 6.5. 不同架构处理器的对齐要求RISC指令集处理器（MIPS/ARM）：这种处理器的设计以效率为先，要求所访问的多字节数据（short/int/long）的地址必须是此数据大小的倍数，如short数据地址应为2的倍数，long数据地址应为4的倍数，也就是说是对齐的。 CISC指令集处理器(X86) ：没有上述限制。 对齐处理策略 访问非对齐多字节数据时(pack数据)，编译器会将指令拆成多条(因为非对齐多字节数据可能跨越地址对齐边界)，保证每条指令都从正确的起始地址上获取数据，但也因此效率比较低。 访问对齐数据时则只用一条指令获取数据，因此对齐数据必须确保其起始地址是在对齐边界上。如果不是在对齐的边界，对X86 CPU是安全的，但对MIPS/ARM这种RISC CPU会出现总线访问异常。 为什么X86是安全的呢？ X86 CPU是如何进行数据对齐的？ X86 CPU的EFLAGS寄存器中包含一个特殊的位标志，称为AC(对齐检查的英文缩写)标志。 按照默认设置，当CPU首次加电时，该标志被设置为0。 当该标志是0时，CPU能够自动执行它应该执行的操作，以便成功地访问未对齐的数据值。 然而，如果该标志被设置为1，每当系统试图访问未对齐的数据时，CPU就会发出一个INT 17H中断。 X86的Windows 2000和Windows 98版本从来不改变这个CPU标志位。因此，当应用程序在X86处理器上运行时，你根本看不到应用程序中出现数据未对齐的异常条件。 为什么MIPS/ARM不安全呢？ 因为MIPS/ARM CPU不能自动处理对未对齐数据的访问。当未对齐的数据访问发生时，CPU就会将这一情况通知操作系统。这时，操作系统将会确定它是否应该引发一个数据未对齐异常条件，对vxworks是会触发这个异常的。 6.6. ARM下的对齐处理有部分摘自ARM编译器文档对齐部分。 对齐的使用： __align(num) 用于修改最高级别对象的字节边界。 在汇编中使用LDRD或STRD时就要用到此命令__align(8)进行修饰限制。来保证数据对象是相应对齐。 这个修饰对象的命令最大是8个字节限制，可以让2字节的对象进行4字节对齐，但不能让4字节的对象2字节对齐。 __align是存储类修改，只修饰最高级类型对象，不能用于结构或者函数对象。 __packed 进行一字节对齐。需注意： 不能对packed的对象进行对齐； 所有对象的读写访问都进行非对齐访问； float及包含float的结构联合及未用__packed的对象将不能字节对齐； __packed对局部整型变量无影响。 强制由unpacked对象向packed对象转化时未定义。整型指针可以合法定义为packed，如__packed int* p(__packed int 则没有意义) 对齐或非对齐读写访问可能存在的问题： 12345678910111213141516171819202122232425262728293031323334353637//定义如下结构，b的起始地址不对齐。在栈中访问b可能有问题，因为栈上数据对齐访问__packed struct STRUCT_TEST&#123; char a; int b; char c;&#125;;//将下面的变量定义成全局静态(不在栈上)static char *p;static struct STRUCT_TEST a;void main()&#123; __packed int *q; //定义成__packed来修饰当前q指向为非对齐的数据地址下面的访问则可以 p = (char*)&amp;a; q = (int*)(p + 1); *q = 0x87654321; /* 得到赋值的汇编指令很清楚 ldr r5,0x20001590 ; = #0x12345678 [0xe1a00005] mov r0,r5 [0xeb0000b0] bl __rt_uwrite4 //在此处调用一个写4字节的操作函数 [0xe5c10000] strb r0,[r1,#0] //函数进行4次strb操作然后返回，正确访问数据 [0xe1a02420] mov r2,r0,lsr #8 [0xe5c12001] strb r2,[r1,#1] [0xe1a02820] mov r2,r0,lsr #16 [0xe5c12002] strb r2,[r1,#2] [0xe1a02c20] mov r2,r0,lsr #24 [0xe5c12003] strb r2,[r1,#3] [0xe1a0f00e] mov pc,r14 若q未加__packed修饰则汇编出来指令如下(会导致奇地址处访问失败)： [0xe59f2018] ldr r2,0x20001594 ; = #0x87654321 [0xe5812000] str r2,[r1,#0] */ //这样很清楚地看到非对齐访问如何产生错误，以及如何消除非对齐访问带来的问题 //也可看到非对齐访问和对齐访问的指令差异会导致效率问题&#125; 6.7. 《The C Book》之位域篇While we’re on the subject of structures, we might as well look at bitfields. They can only be declared inside a structure or a union, and allow you to specify some very small objects of a given number of bits in length. Their usefulness is limited and they aren’t seen in many programs, but we’ll deal with them anyway. This example should help to make things clear: 1234567struct&#123; unsigned field1 :4; //field 4 bits wide unsigned :3; //unnamed 3 bit field(allow for padding) signed field2 :1; //one-bit field(can only be 0 or -1 in two&#x27;s complement) unsigned :0; //align next field on a storage unit unsigned field3 :6;&#125;full_of_fields; Each field is accessed and manipulated as if it were an ordinary member of a structure. The keywords signed and unsigned mean what you would expect, except that it is interesting to note that a 1-bit signed field on a two’s complement machine can only take the values 0 or -1. The declarations are permitted to include the const and volatile qualifiers. The main use of bitfields is either to allow tight packing of data or to be able to specify the fields within some externally produced data files. C gives no guarantee of the ordering of fields within machine words, so if you do use them for the latter reason, you program will not only be non-portable, it will be compiler-dependent too. The Standard says that fields are packed into ‘storage units’, which are typically machine words. The packing order, and whether or not a bitfield may cross a storage unit boundary, are implementation defined. To force alignment to a storage unit boundary, a zero width field is used before the one that you want to have aligned. Be careful using them. It can require a surprising amount of run-time code to manipulate these things and you can end up using more space than they save. Bit fields do not have addresses—you can’t have pointers to them or arrays of them. 6.8. C语言字节相关面试题6.8.1. Intel/微软C语言面试题请看下面的问题： 1234567891011#pragma pack(8)struct s1&#123; short a; // 按 min(1,8) 对齐 long b; // 按 min(4,8) 对齐&#125;;struct s2&#123; char c; s1 d; long long e; //VC6.0下可能要用__int64代替双long&#125;;#pragma pack() 问题： sizeof(s2) = ？ s2的s1中的a后面空了几个字节接着是b？ 分析： 成员对齐有一个重要的条件，即每个成员分别按自己的方式对齐。 也就是说上面虽然指定了按8字节对齐，但并不是所有的成员都是以8字节对齐。其对齐的规则是：每个成员按 其类型的对齐参数（通常是这个类型的大小） 和 指定对齐参数（这里是8字节） 中较小的一个对齐，并且结构的长度必须为所用过的所有对齐参数的整数倍，不够就补空字节。 s1中成员a是1字节，默认按1字节对齐，而指定对齐参数为8，两值中取1，即a按1字节对齐；成员b是4个字节，默认按4字节对齐，这时就按4字节对齐，所以sizeof(s1)应该为8； s2中c和s1中a一样，按1字节对齐。而d是个8字节结构体，其默认对齐方式就是所有成员使用的对齐参数中最大的一个，s1的就是4。所以，成员d按4字节对齐。成员e是8个字节，默认按8字节对齐，和指定的一样，所以它对到8字节的边界上。这时，已经使用了12个字节，所以又添加4个字节的空，从第16个字节开始放置成员e。此时长度为24，并可被8（成员e按8字节对齐）整除。这样，一共使用了24个字节。 各个变量在内存中的布局为： 123c***aa**bbbb****dddddddd ——这种“矩阵写法”很方便看出结构体实际大小！ 因此，sizeof(S2)结果为24，a后面空了2个字节接着是b。 这里有三点很重要： 每个成员分别按自己的方式对齐，并能最小化长度； 复杂类型(如结构)的默认对齐方式是其最长的成员的对齐方式，这样在成员是复杂类型时可以最小化长度； 对齐后的长度必须是成员中最大对齐参数的整数倍，这样在处理数组时可保证每一项都边界对齐。 还要注意，“空结构体”(不含数据成员)的大小为1，而不是0。试想如果不占空间的话，一个空结构体变量如何取地址、两个不同的空结构体变量又如何得以区分呢？ 6.8.2 上海网宿科技面试题假设硬件平台是intel x86(little endian)，以下程序输出什么： 12345678910111213141516//假设硬件平台是intel x86(little endian)typedef unsigned int uint32_t;void inet_ntoa(uint32_t in)&#123; char b[18]; register char *p; p = (char *)∈#define UC(b) (((int)b)&amp;0xff) //byte转换为无符号int型 sprintf(b, &quot;%d.%d.%d.%d\\n&quot;, UC(p[0]), UC(p[1]), UC(p[2]), UC(p[3])); printf(b);&#125;int main(void)&#123; inet_ntoa(0x12345678); inet_ntoa(0x87654321); return 0;&#125; 先看如下程序： 12345678int main(void)&#123; int a = 0x12345678; char *p = (char *)&amp;a; char str[20]; sprintf(str,&quot;%d.%d.%d.%d\\n&quot;, p[0], p[1], p[2], p[3]); printf(str); return 0;&#125; 按照小字节序的规则，变量a在计算机中存储方式为： 高地址方向 ————–&gt; 低地址方向0x12 0x34 0x56 0x78p[3] p[2] p[1] p[0] 注意：p并不是指向0x12345678的开头0x12，而是指向0x78。p[0]到p[1]的操作是&amp;p[0]+1，因此p[1]地址比p[0]地址大。输出结果为120.86.52.18。 反过来的话，令int a = 0x87654321，则输出结果为33.67.101.-121。 为什么有负值呢？ 因为系统默认的char是有符号的，本来是0x87也就是135，大于127因此就减去256得到-121。 想要得到正值的话只需将char *p = (char *)&amp;a改为unsigned char *p = (unsigned char *)&amp;a即可。 综上不难得出，网宿面试题的答案为120.86.52.18和33.67.101.135。 说明：本文转载自 https://www.cnblogs.com/clover-toeic/p/3853132.html 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"字节对齐","slug":"字节对齐","permalink":"http://dbkernel.github.io/tags/%E5%AD%97%E8%8A%82%E5%AF%B9%E9%BD%90/"},{"name":"字节序","slug":"字节序","permalink":"http://dbkernel.github.io/tags/%E5%AD%97%E8%8A%82%E5%BA%8F/"},{"name":"网络序","slug":"网络序","permalink":"http://dbkernel.github.io/tags/%E7%BD%91%E7%BB%9C%E5%BA%8F/"}]},{"title":"程序人生 | C语言字节对齐问题详解 - 对齐/字节序/位序/网络序等（上）","slug":"c-language-byte-alignment-problem-in-detail-part-1","date":"2014-07-21T07:32:28.000Z","updated":"2021-09-24T03:55:34.876Z","comments":true,"path":"2014/07/21/c-language-byte-alignment-problem-in-detail-part-1/","link":"","permalink":"http://dbkernel.github.io/2014/07/21/c-language-byte-alignment-problem-in-detail-part-1/","excerpt":"","text":"本文首发于 2014-07-21 15:32:28 1. 引言考虑下面的结构体定义： 123456typedef struct&#123; char c1; short s; char c2; int i;&#125;T_FOO; 假设这个结构体的成员在内存中是紧凑排列的，且c1的起始地址是0，则s的地址就是1，c2的地址是3，i的地址是4。 现在，我们编写一个简单的程序： 123456789int main(void)&#123; T_FOO a; printf(&quot;c1 -&gt; %d, s -&gt; %d, c2 -&gt; %d, i -&gt; %d\\n&quot;, (unsigned int)(void*)&amp;a.c1 - (unsigned int)(void*)&amp;a, (unsigned int)(void*)&amp;a.s - (unsigned int)(void*)&amp;a, (unsigned int)(void*)&amp;a.c2 - (unsigned int)(void*)&amp;a, (unsigned int)(void*)&amp;a.i - (unsigned int)(void*)&amp;a); return 0;&#125; 运行后输出： 1c1 -&gt; 0, s -&gt; 2, c2 -&gt; 4, i -&gt; 8 为什么会这样？这就是字节对齐导致的问题。 本文在参考诸多资料的基础上，详细介绍常见的字节对齐问题。因成文较早，资料来源大多已不可考，敬请谅解。 2. 什么是字节对齐现代计算机中，内存空间按照字节划分，理论上可以从任何起始地址访问任意类型的变量，但实际上在访问特定类型变量时经常在特定的内存地址访问，这就需要各种类型数据按照一定的规则在空间上排列，而不是一个接一个地顺序存放，这就是对齐。 3. 对齐的原因和作用 不同硬件平台对存储空间的处理上存在很大的不同。某些平台对特定类型的数据只能从特定地址开始存取，而不允许其在内存中任意存放。例如 Motorola 68000 处理器不允许16位的字存放在奇地址，否则会触发异常，因此在这种架构下编程必须保证字节对齐。 如果不按照平台要求对数据存放进行对齐，会带来存取效率上的损失。比如32位的 Intel 处理器通过总线访问(包括读和写)内存数据。每个总线周期从偶地址开始访问32位内存数据，内存数据以字节为单位存放。如果一个32位的数据没有存放在4字节整除的内存地址处，那么处理器就需要2个总线周期对其进行访问，显然访问效率下降很多。因此，通过合理的内存对齐可以提高访问效率。 为使CPU能够对数据进行快速访问，数据的起始地址应具有“对齐”特性。比如4字节数据的起始地址应位于4字节边界上，即起始地址能够被4整除。 合理利用字节对齐还可以有效地节省存储空间。但要注意，在32位机中使用1字节或2字节对齐，反而会降低变量访问速度，因此，需要考虑处理器类型。同时，还应考虑编译器的类型，在VC/C++和GNU GCC中都是默认是4字节对齐。 4. 对齐的分类和准则本小节主要基于 Intel X86 架构介绍结构体对齐和栈内存对齐，位域本质上为结构体类型。 对于 Intel X86 平台，每次分配内存应该是从4的整数倍地址开始分配，无论是对结构体变量还是简单类型的变量。 4.1. 结构体对齐在C语言中，结构体是种复合数据类型，其构成元素既可以是基本数据类型（int、long、float等）的变量，也可以是一些复合数据类型（数组、结构体、联合等)的数据单元。编译器为结构体的每个成员按照其自然边界（alignment）分配空间。各成员按照它们被声明的顺序在内存中顺序存储，第一个成员的地址和整个结构的地址相同。 字节对齐的问题主要就是针对结构体。 4.1.1. 简单示例先看个简单的例子（32位，X86处理器，GCC编译器）： 【例1】假设结构体定义如下： 1234567891011struct A&#123; int a; char b; short c;&#125;;struct B&#123; char b; int a; short c;&#125;; 已知32位机器上各数据类型的长度为：char为1字节、short为2字节、int为4字节、long为4字节、float为4字节、double为8字节。那么上面两个结构体大小如何呢？ 结果是：sizeof(strcut A)值为8；sizeof(struct B)的值却是12。 结构体A和B中字段一样，包含一个4字节的int数据，一个1字节char数据和一个2字节short数据，只是顺序不同。按理说A和B大小应该都是7字节，之所以出现上述结果，就是因为编译器要对数据成员在空间上进行对齐。 4.1.2. 对齐准则先来看四个重要的基本概念： 数据类型自身的对齐值：char型数据自身对齐值为1字节，short型数据为2字节，int/float型为4字节，double型为8字节。 结构体或类的自身对齐值：其成员中自身对齐值最大的那个值。 指定对齐值：#pragma pack (value) 指定对齐值 value。 数据成员、结构体和类的有效对齐值：自身对齐值和指定对齐值中较小者，即有效对齐值=min&#123;自身对齐值，当前指定的pack值&#125;。 基于上面这些原则，就可以方便地讨论具体数据结构的成员和其自身的对齐方式。 其中，有效对齐值 N 是最终用来决定数据存放地址方式的值。有效对齐值 N 表示“对齐在N上”，即该数据的存放起始地址 % N = 0。而数据结构中的数据变量都是按定义的先后顺序存放。第一个数据变量的起始地址就是数据结构的起始地址。结构体的成员变量要对齐存放，结构体本身也要根据自身的有效对齐值圆整（即结构体成员变量占用总长度为结构体有效对齐值的整数倍）。 以此分析3.1.1节中的结构体B： 假设B从地址空间0x0000开始存放，且指定对齐值默认为4(4字节对齐)。成员变量b的自身对齐值是1，比默认指定对齐值4小，所以其有效对齐值为1，其存放地址0x0000符合0x0000%1=0。 成员变量a自身对齐值为4，所以有效对齐值也为4，只能存放在起始地址为0x0004~0x0007四个连续的字节空间中，符合0x0004%4=0且紧靠第一个变量。 变量c自身对齐值为2，所以有效对齐值也是2，可存放在0x0008~0x0009两个字节空间中，符合0x0008%2=0。 所以从0x0000~0x0009存放的都是B内容。 再看数据结构B的自身对齐值为其变量中最大对齐值（这里是b），也就是4，所以结构体的有效对齐值也是4。根据结构体圆整的要求，0x0000~0x0009=10字节，(10＋2)％4＝0。 所以0x0000A~0x000B也为结构体B所占用。故B从0x0000到0x000B，共有12个字节，sizeof(struct B)=12。 之所以编译器在后面补充2个字节，是为了实现结构数组的存取效率。试想如果定义一个结构B的数组，那么第一个结构起始地址是0没有问题，但是第二个结构呢？ 按照数组的定义，数组中所有元素都紧挨着。如果我们不把结构体大小补充为4的整数倍，那么下一个结构的起始地址将是0x0000A，这显然不能满足结构的地址对齐。因此要把结构体补充成有效对齐大小的整数倍。 其实对于char/short/int/float/double等已有类型的自身对齐值也是基于数组考虑的，只是因为这些类型的长度已知，所以他们的自身对齐值也就已知。 上面的概念非常便于理解，不过个人还是更喜欢下面的对齐准则。 结构体字节对齐的细节和具体编译器实现相关，但一般而言满足三个准则： 结构体变量的首地址能够被其最宽基本类型成员的大小所整除； 结构体每个成员相对结构体首地址的偏移量(offset)都是成员大小的整数倍，如有需要编译器会在成员之间加上填充字节(internal adding)； 结构体的总大小为结构体最宽基本类型成员大小的整数倍，如有需要编译器会在最末一个成员之后加上填充字节{trailing padding}。 对于以上规则的说明如下： 第一条：编译器在给结构体开辟空间时，首先找到结构体中最宽的基本数据类型，然后寻找内存地址能被该基本数据类型所整除的位置，作为结构体的首地址。将这个最宽的基本数据类型的大小作为上面介绍的对齐模数。 第二条：为结构体的一个成员开辟空间之前，编译器首先检查预开辟空间的首地址相对于结构体首地址的偏移是否是本成员大小的整数倍，若是，则存放本成员，反之，则在本成员和上一个成员之间填充一定的字节，以达到整数倍的要求，也就是将预开辟空间的首地址后移几个字节。 第三条：结构体总大小是包括填充字节，最后一个成员满足上面两条以外，还必须满足第三条，否则就必须在最后填充几个字节以达到本条要求。 【例2】假设4字节对齐，以下程序的输出结果是多少？ 1234567891011121314151617/* OFFSET宏定义可取得指定结构体某成员在结构体内部的偏移 */#define OFFSET(st, field) (size_t)&amp;(((st*)0)-&gt;field)typedef struct&#123; char a; short b; char c; int d; char e[3];&#125;T_Test;int main(void)&#123; printf(&quot;Size = %d\\n a-%d, b-%d, c-%d, d-%d\\n e[0]-%d, e[1]-%d, e[2]-%d\\n&quot;, sizeof(T_Test), OFFSET(T_Test, a), OFFSET(T_Test, b), OFFSET(T_Test, c), OFFSET(T_Test, d), OFFSET(T_Test, e[0]), OFFSET(T_Test, e[1]),OFFSET(T_Test, e[2])); return 0;&#125; 执行后输出如下： 123Size = 16a-0, b-2, c-4, d-8e[0]-12, e[1]-13, e[2]-14 下面来具体分析： 首先char a占用1个字节，没问题。 short b本身占用2个字节，根据上面准则2，需要在b和a之间填充1个字节。 char c占用1个字节，没问题。 int d本身占用4个字节，根据准则2，需要在d和c之间填充3个字节。 char e[3]；本身占用3个字节，根据原则3，需要在其后补充1个字节。 因此，sizeof(T_Test) = 1 + 1 + 2 + 1 + 3 + 4 + 3 + 1 = 16字节。 4.1.3. 对齐的隐患4.1.3.1. 数据类型转换代码中关于对齐的隐患，很多是隐式的。例如，在强制类型转换的时候： 12345678int main(void)&#123; unsigned int i = 0x12345678; unsigned char *p = (unsigned char *)&amp;i; *p = 0x00; unsigned short *p1 = (unsigned short *)(p+1); *p1 = 0x0000; return 0;&#125; 最后两句代码，从奇数边界去访问 unsigned short 型变量，显然不符合对齐的规定。在X86上，类似的操作只会影响效率；但在MIPS或者SPARC上可能导致error，因为它们要求必须字节对齐。 又如对于3.1.1节的结构体 struct B，定义如下函数： 123void Func(struct B *p)&#123; //Code&#125; 在函数体内如果直接访问 p-&gt;a，则很可能会异常。因为MIPS认为a是int，其地址应该是4的倍数，但p-&gt;a的地址很可能不是4的倍数。 如果p的地址不在对齐边界上就可能出问题，比如p来自一个跨CPU的数据包(多种数据类型的数据被按顺序放置在一个数据包中传输)，或p是经过指针移位算出来的。因此要特别注意跨CPU数据的接口函数对接口输入数据的处理，以及指针移位再强制转换为结构指针进行访问时的安全性。 解决方式如下： 定义一个此结构的局部变量，用memmove方式将数据拷贝进来。12345void Func(struct B *p)&#123; struct B tData; memmove(&amp;tData, p, sizeof(struct B)); //此后可安全访问tData.a，因为编译器已将tData分配在正确的起始地址上&#125; 注意：如果能确定p的起始地址没问题，则不需要这么处理；如果不能确定（比如跨CPU输入数据、或指针移位运算出来的数据），则需要这样处理。 用#pragma pack (1)将 STRUCT_T 定义为1字节对齐方式。 4.1.3.2. 处理器间数据通信处理器间通过消息（对于C/C++而言就是结构体）进行通信时，需要注意字节对齐以及字节序的问题。 大多数编译器提供一些内存选项供用户使用。这样用户可以根据处理器的情况选择不同的字节对齐方式。例如：C/C++编译器提供的#pragma pack(n) n=1，2，4等，让编译器在生成目标文件时，使内存数据按照指定的方式排布在1，2，4等字节整除的内存地址处。 然而在不同编译平台或处理器上，字节对齐会造成消息结构长度的变化。编译器为了使字节对齐可能会对消息结构体进行填充，不同编译平台可能填充为不同的形式，大大增加处理器间数据通信的风险。 下面以32位处理器为例，提出一种内存对齐方法以解决上述问题。 对于本地使用的数据结构，为提高内存访问效率，采用4字节对齐方式；同时为了减少内存的开销，合理安排结构体成员的位置，减少4字节对齐导致的成员之间的空隙，降低内存开销。 对于处理器之间的数据结构，需要保证消息长度不会因不同编译平台或处理器而导致消息结构体长度发生变化，使用1字节对齐方式对消息结构进行紧缩；为保证处理器之间的消息数据结构的内存访问效率，采用字节填充的方式自己对消息中成员进行4字节对齐。 数据结构的成员位置要兼顾成员之间的关系、数据访问效率和空间利用率。顺序安排原则是：4字节的放在最前面，2字节的紧接最后一个4字节成员，1字节紧接最后一个2字节成员，填充字节放在最后。 举例如下： 1234567typedef struct tag_T_MSG&#123; long ParaA; long ParaB; short ParaC； char ParaD; char Pad; //填充字节&#125;T_MSG; 4.1.3.3. 排查对齐问题如果出现对齐或者赋值问题，可查看： 编译器的字节序大小端设置； 处理器架构本身是否支持非对齐访问； 如果支持，则看是否设置对齐； 如果没有，则看访问时是否需要加某些特殊的修饰来标志其特殊访问操作。 4.1.4. 更改对齐方式主要是更改C编译器的缺省字节对齐方式。 在缺省情况下，C编译器为每一个变量或是数据单元按其自然对界条件分配空间。一般地，可以通过下面的方法来改变缺省的对界条件： 使用伪指令#pragma pack(n)：C编译器将按照n个字节对齐； 使用伪指令#pragma pack()：取消自定义字节对齐方式。 另外，还有如下的一种方式（GCC特有语法）： __attribute__((aligned (n)))：让所作用的结构成员对齐在n字节自然边界上。如果结构体中有成员的长度大于n，则按照最大成员的长度来对齐。 __attribute__((packed))：取消结构在编译过程中的优化对齐，按照实际占用字节数进行对齐。 注意： __attribute__机制是GCC的一大特色，可以设置函数属性(Function Attribute)、变量属性(Variable Attribute)和类型属性(Type Attribute)。 在编码时，可用#pragma pack动态修改对齐值。具体语法说明见附录5.3节。 自定义对齐值后要用#pragma pack()来还原，否则会对后面的结构造成影响。 【例3】分析如下结构体C： 1234567#pragma pack(2) //指定按2字节对齐struct C&#123; char b; int a; short c;&#125;;#pragma pack() //取消指定对齐，恢复缺省对齐 变量b自身对齐值为1，指定对齐值为2，所以有效对齐值为1，假设C从0x0000开始，则b存放在0x0000，符合0x0000%1=0； 变量a自身对齐值为4，指定对齐值为2，所以有效对齐值为2，顺序存放在0x0002~0x0005四个连续字节中，符合0x0002%2=0。 变量c的自身对齐值为2，所以有效对齐值为2，顺序存放在0x0006~0x0007中，符合0x0006%2=0。 所以从0x0000到0x00007共8字节存放的是C的变量。 C的自身对齐值为4，所以其有效对齐值为2。又8%2=0，C只占用0x0000~0x0007的八个字节。所以sizeof(struct C)=8。 注意：结构体对齐到的字节数并非完全取决于当前指定的pack值，例如： 1234567#pragma pack(8)struct D&#123; char b; short a; char c;&#125;;#pragma pack() 虽然#pragma pack(8)，但依然按照2字节对齐，所以 sizeof(struct D) 的值为6。所以，对齐到的字节数=min｛当前指定的pack值，最大成员大小｝。 另外，GNU GCC编译器中按1字节对齐可写为以下形式： 123456#define GNUC_PACKED __attribute__((packed))struct C&#123; char b; int a; short c;&#125;GNUC_PACKED; 此时 sizeof(struct C) 的值为7。 4.2. 栈内存对齐在VC/C++中，栈的对齐方式不受结构体成员对齐选项的影响，总是保持对齐在4字节边界上。 【例4】分析栈内存对齐方式： 1234567891011121314151617181920212223#pragma pack(push, 1) //后面可改为1, 2, 4, 8struct StrtE&#123; char m1; long m2;&#125;;#pragma pack(pop)int main(void)&#123; char a; short b; int c; double d[2]; struct StrtE s; printf(&quot;a address: %p\\n&quot;, &amp;a); printf(&quot;b address: %p\\n&quot;, &amp;b); printf(&quot;c address: %p\\n&quot;, &amp;c); printf(&quot;d[0] address: %p\\n&quot;, &amp;(d[0])); printf(&quot;d[1] address: %p\\n&quot;, &amp;(d[1])); printf(&quot;s address: %p\\n&quot;, &amp;s); printf(&quot;s.m2 address: %p\\n&quot;, &amp;(s.m2)); return 0;&#125; 结果如下： 1234567a address: 0xbfc4cfffb address: 0xbfc4cffcc address: 0xbfc4cff8d[0] address: 0xbfc4cfe8d[1] address: 0xbfc4cff0s address: 0xbfc4cfe3s.m2 address: 0xbfc4cfe4 可以看出都是对齐到4字节，并且前面的char和short并没有被凑在一起（成4字节），这和结构体内的处理是不同的。 至于为什么输出的地址值是变小的，这是因为该平台下的栈是倒着“生长”的。 4.3. 位域对齐4.3.1. 位域定义有些信息在存储时，并不需要占用一个完整的字节，而只需占几个或一个二进制位。例如在存放一个开关量时，只有0和1两种状态，用一位二进位即可。为了节省存储空间和处理简便，C语言提供了一种数据结构，称为位域或位段。 位域是一种特殊的结构成员或联合成员（即只能用在结构或联合中），用于指定该成员在内存存储时所占用的位数，从而在机器内更紧凑地表示数据。每个位域有一个域名，允许在程序中按域名操作对应的位，这样就可用一个字节的二进制位域来表示几个不同的对象。 位域定义与结构定义类似，其形式为： 12struct 位域结构名 &#123; 位域列表 &#125;; 其中位域列表的形式为： 1类型说明符位域名：位域长度 位域的使用和结构成员的使用相同，其一般形式为： 1位域变量名.位域名 位域允许用各种格式输出。 位域在本质上就是一种结构类型，不过其成员是按二进位分配的。位域变量的说明与结构变量说明的方式相同，可先定义后说明、同时定义说明或直接说明。 位域的使用主要为下面两种情况： 当机器可用内存空间较少而使用位域可大量节省内存时。例如：把结构作为大数组的元素时。 当需要把一结构体或联合映射成某预定的组织结构时。例如：需要访问字节内的特定位时。 4.3.2. 对齐准则位域成员不能单独被取sizeof值。下面主要讨论含有位域的结构体的sizeof。 C99规定 int、unsigned int 和 bool 可以作为位域类型，但编译器几乎都对此作了扩展，允许其它类型的存在。位域作为嵌入式系统中非常常见的一种编程工具，优点在于压缩程序的存储空间。 其对齐规则大致为： 如果相邻位域字段的类型相同，且其位宽之和小于类型的sizeof大小，则后面的字段将紧邻前一个字段存储，直到不能容纳为止； 如果相邻位域字段的类型相同，但其位宽之和大于类型的sizeof大小，则后面的字段将从新的存储单元开始，其偏移量为其类型大小的整数倍； 如果相邻的位域字段的类型不同，则各编译器的具体实现有差异，VC6采取不压缩方式，Dev-C++和GCC采取压缩方式； 如果位域字段之间穿插着非位域字段，则不进行压缩； 整个结构体的总大小为最宽基本类型成员大小的整数倍，而位域则按照其最宽类型字节数对齐。 【例5】 12345struct BitField&#123; char element1 : 1; char element2 : 4; char element3 : 5;&#125;; 位域类型为char，第1个字节仅能容纳下element1和element2，所以element1和element2被压缩到第1个字节中，而element3只能从下一个字节开始。因此 sizeof(BitField) 的结果为2。 【例6】 12345struct BitField1&#123; char element1 : 1; short element2 : 5; char element3 : 7;&#125;; 由于相邻位域类型不同，在VC6中其sizeof为6，在Dev-C++中为2。 【例7】 12345struct BitField2&#123; char element1 : 3; char element2 ; char element3 : 5;&#125;; 非位域字段穿插在其中，不会产生压缩，在VC6和Dev-C++中得到的大小均为3。 【例8】 12345678struct StructBitField&#123; int element1 : 1; int element2 : 5; int element3 : 29; int element4 : 6; char element5 :2; char stelement; //在含位域的结构或联合中也可同时说明普通成员&#125;; 位域中最宽类型int的字节数为4，因此结构体按4字节对齐，在VC6中其sizeof为16。 4.3.3. 注意事项关于位域操作有几点需要注意： 1）位域的地址不能访问，因此不允许将&amp;运算符用于位域。不能使用指向位域的指针也不能使用位域的数组（数组是种特殊指针）。例如，scanf函数无法直接向位域中存储数据： 12345int main(void)&#123; struct BitField1 tBit; scanf(&quot;%d&quot;, &amp;tBit.element2); //error: cannot take address of bit-field &#x27;element2&#x27; return 0;&#125; 可用scanf函数将输入读入到一个普通的整型变量中，然后再赋值给tBit.element2。 2）位域不能作为函数返回的结果。 3）位域以定义的类型为单位，且位域的长度不能够超过所定义类型的长度。例如：定义 int a:33 是不允许的。 4）位域可以不指定位域名，但不能访问无名的位域。 位域可以无位域名，只用作填充或调整位置，占位大小取决于该类型。例如，char :0 表示整个位域向后推一个字节，即该无名位域后的下一个位域从下一个字节开始存放，同理 short :0 和 int :0 分别表示整个位域向后推两个和四个字节。 当空位域的长度为具体数值N时(如 int :2)，该变量仅用来占位N位。 【例9】 12345struct BitField3&#123; char element1 : 3; char :6; char element3 : 5;&#125;; 结构体大小为3。因为element1占3位，后面要保留6位而char为8位，所以保留的6位只能放到第2个字节。同样element3只能放到第3字节。 12345struct BitField4&#123; char element1 : 3; char :0; char element3 : 5;&#125;; 长度为0的位域告诉编译器将下一个位域放在一个存储单元的起始位置。如上，编译器会给成员element1分配3位，接着跳过余下的4位到下一个存储单元，然后给成员element3分配5位。所以，上面的结构体大小为2 。 5）位域的表示范围： 位域的赋值不能超过其可以表示的范围。 位域的类型决定该编码能表示的值的结果。 对于第二点，若位域为unsigned类型，则直接转化为正数；若非unsigned类型，则先判断最高位是否为1，若为1，则表示补码，则对其除符号位外的所有位取反再加一得到最后的结果数据（原码）。例如： 12unsigned int p:3 = 111; //p表示7int p:3 = 111; //p 表示-1，对除符号位之外的所有位取反再加一 6）带位域的结构在内存中各个位域的存储方式取决于编译器，既可从左到右也可从右到左存储。 【例10】在VC6下执行下面的代码： 1234567891011121314int main(void)&#123; union&#123; int i; struct&#123; char a : 1; char b : 1; char c : 2; &#125;bits; &#125;num; printf(&quot;Input an integer for i(0~15): &quot;); scanf(&quot;%d&quot;, &amp;num.i); printf(&quot;i = %d, cba = %d %d %d\\n&quot;, num.i, num.bits.c, num.bits.b, num.bits.a); return 0; 输入i值为11，则输出为i = 11, cba = -2 -1 -1。 Intel x86 处理器按小字节序存储数据，所以bits中的位域在内存中放置顺序为ccba。当num.i置为11时，bits的最低有效位(即位域a)的值为1，a、b、c按低地址到高地址分别存储为10、1、1(二进制)。 但为什么最后的打印结果是a=-1而不是1？ 因为位域a定义的类型signed char是有符号数，所以尽管a只有1位，仍要进行符号扩展。1做为补码存在，对应原码-1。 如果将a、b、c的类型定义为unsigned char，即可得到cba = 2 1 1。1011即为11的二进制数。 注：C语言中，不同的成员使用共同的存储区域的数据构造类型称为联合(或共用体)。联合占用空间的大小取决于类型长度最大的成员。联合在定义、说明和使用形式上与结构体相似。 7）位域的实现会因编译器的不同而不同，使用位域会影响程序可移植性。因此如无必要，最好不要使用位域。 8）尽管使用位域可以节省内存空间，但却增加了处理时间。当访问各个位域成员时，需要把位域从它所在的字中分解出来或反过来把一值压缩存到位域所在的字位中。 5. 总结让我们回到引言部分的问题。 缺省情况下，C/C++编译器默认将结构、栈中的成员数据进行内存对齐。因此，引言程序输出就变成”c1 -&gt; 0, s -&gt; 2, c2 -&gt; 4, i -&gt; 8”。 编译器将未对齐的成员向后移，将每一个都成员对齐到自然边界上，从而也导致整个结构的尺寸变大。尽管会牺牲一点空间(成员之间有空洞)，但提高了性能。 也正是这个原因，引言例子中sizeof(T_ FOO)为12，而不是8。 总结说来，就是：在结构体中，综合考虑变量本身和指定的对齐值；在栈上，不考虑变量本身的大小，统一对齐到4字节。 说明： 本文转载自 https://www.cnblogs.com/clover-toeic/p/3853132.html 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"字节对齐","slug":"字节对齐","permalink":"http://dbkernel.github.io/tags/%E5%AD%97%E8%8A%82%E5%AF%B9%E9%BD%90/"},{"name":"字节序","slug":"字节序","permalink":"http://dbkernel.github.io/tags/%E5%AD%97%E8%8A%82%E5%BA%8F/"},{"name":"网络序","slug":"网络序","permalink":"http://dbkernel.github.io/tags/%E7%BD%91%E7%BB%9C%E5%BA%8F/"}]},{"title":"程序人生 | Makefile 常用模板 - 静态链接库/动态链接库/可执行文件","slug":"makefile-common-templates","date":"2014-07-10T03:51:10.000Z","updated":"2021-09-22T15:24:33.493Z","comments":true,"path":"2014/07/10/makefile-common-templates/","link":"","permalink":"http://dbkernel.github.io/2014/07/10/makefile-common-templates/","excerpt":"","text":"本文首发于 2014-07-10 11:51:10 前言本文把 makefile 分成了三份：生成可执行文件的 makefile，生成静态链接库的 makefile，生成动态链接库的 makefile。 这些 makefile 都很简单，一般都是一看就会用，用法也很容易，只需要把它们拷贝到你的代码的同一目录下，然后就可以用 make 来生成目标文件了。 下面是三个makefile的源代码： 生成可执行文件的 makefile1234567891011121314151617181920212223242526272829303132333435363738394041424344##############################################################################source file#源文件，自动找所有.c和.cpp文件，并将目标定义为同名.o文件SOURCE := $(wildcard *.c) $(wildcard *.cpp)OBJS := $(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(SOURCE)))#target you can change test to what you want#目标文件名，输入任意你想要的执行文件名TARGET := test#compile and lib parameter#编译参数CC := gccLIBS :=LDFLAGS :=DEFINES :=INCLUDE := -I.CFLAGS := -g -Wall -O3 $(DEFINES) $(INCLUDE)CXXFLAGS:= $(CFLAGS) -DHAVE_CONFIG_H#i think you should do anything here#下面的基本上不需要做任何改动了.PHONY : everything objs clean veryclean rebuildeverything : $(TARGET)all : $(TARGET)objs : $(OBJS)rebuild: veryclean everythingclean : rm -fr *.so rm -fr *.overyclean : clean rm -fr $(TARGET)$(TARGET) : $(OBJS) $(CC) $(CXXFLAGS) -o $@ $(OBJS) $(LDFLAGS) $(LIBS) 生成静态链接库的 makefile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748###############################################################################target you can change test to what you want#共享库文件名，lib*.aTARGET := libtest.a#compile and lib parameter#编译参数CC := gccAR = arRANLIB = ranlibLIBS :=LDFLAGS :=DEFINES :=INCLUDE := -I.CFLAGS := -g -Wall -O3 $(DEFINES) $(INCLUDE)CXXFLAGS:= $(CFLAGS) -DHAVE_CONFIG_H#i think you should do anything here#下面的基本上不需要做任何改动了#source file#源文件，自动找所有.c和.cpp文件，并将目标定义为同名.o文件SOURCE := $(wildcard *.c) $(wildcard *.cpp)OBJS := $(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(SOURCE))).PHONY : everything objs clean veryclean rebuildeverything : $(TARGET)all : $(TARGET)objs : $(OBJS)rebuild: veryclean everythingclean : rm -fr *.overyclean : clean rm -fr $(TARGET)$(TARGET) : $(OBJS) $(AR) cru $(TARGET) $(OBJS) $(RANLIB) $(TARGET) 生成动态链接库的 makefile12345678910111213141516171819202122232425262728293031323334353637383940414243444546###############################################################################target you can change test to what you want#共享库文件名，lib*.soTARGET := libtest.so#compile and lib parameter#编译参数CC := gccLIBS :=LDFLAGS :=DEFINES :=INCLUDE := -I.CFLAGS := -g -Wall -O3 $(DEFINES) $(INCLUDE)CXXFLAGS:= $(CFLAGS) -DHAVE_CONFIG_HSHARE := -fPIC -shared -o#i think you should do anything here#下面的基本上不需要做任何改动了#source file#源文件，自动找所有.c和.cpp文件，并将目标定义为同名.o文件SOURCE := $(wildcard *.c) $(wildcard *.cpp)OBJS := $(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(SOURCE))).PHONY : everything objs clean veryclean rebuildeverything : $(TARGET)all : $(TARGET)objs : $(OBJS)rebuild: veryclean everythingclean : rm -fr *.overyclean : clean rm -fr $(TARGET)$(TARGET) : $(OBJS) $(CC) $(CXXFLAGS) $(SHARE) $@ $(OBJS) $(LDFLAGS) $(LIBS) 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"编译调试","slug":"编译调试","permalink":"http://dbkernel.github.io/categories/%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95/"}],"tags":[{"name":"Makefile","slug":"Makefile","permalink":"http://dbkernel.github.io/tags/Makefile/"}]},{"title":"程序人生 | UNIX环境高级编程技巧之 du 指令实现","slug":"advanced-programming-in-the-unix-environment-du","date":"2014-07-10T02:00:41.000Z","updated":"2021-09-22T15:24:33.493Z","comments":true,"path":"2014/07/10/advanced-programming-in-the-unix-environment-du/","link":"","permalink":"http://dbkernel.github.io/2014/07/10/advanced-programming-in-the-unix-environment-du/","excerpt":"","text":"本文首发于 2014-07-10 10:00:41 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;glob.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#define PATHSIZE 1024static int path_noloop(const char *path)&#123; char *pos; pos = strrchr(path,&#x27;/&#x27;);//定位最右边的&#x27;/&#x27;的位置 if(strcmp(pos+1,&quot;.&quot;) == 0 || (strcmp(pos+1,&quot;..&quot;) == 0)) return 0; return 1;&#125;static int64_t mydu(const char *path)&#123; int i; glob_t globres; int64_t sum; static struct stat statres; static char nextpath[PATHSIZE]; if(lstat(path, &amp;statres) &lt; 0) &#123; perror(&quot;lstat()&quot;); return 0;//exit(1); &#125; if(!S_ISDIR(statres.st_mode)) return statres.st_blocks; strncpy(nextpath, path,PATHSIZE); strncat(nextpath, &quot;/*&quot; , PATHSIZE); glob(nextpath,GLOB_NOSORT, NULL, &amp;globres); strncpy(nextpath, path,PATHSIZE); strncat(nextpath, &quot;/.*&quot; , PATHSIZE); glob(nextpath,GLOB_NOSORT|GLOB_APPEND, NULL, &amp;globres); sum = statres.st_blocks; for(i = 0 ;i &lt; globres.gl_pathc ; i++) &#123; if(path_noloop(globres.gl_pathv[i])) sum += mydu(globres.gl_pathv[i]); &#125; return sum;&#125;int main(int argc,char **argv)&#123; if(argc &lt; 2) &#123; fprintf(stderr,&quot;Usage...\\n&quot;); exit(1); &#125; printf(&quot;%lld 512B blocks\\n&quot;, (long long int)mydu(argv[1])); return 0;&#125; 编译1$ gcc -g -Wall testdu.c -o testdu 运行 testdf执行效果：12$ ./testdu /usr/bin1766184 512B blocks 原生df执行效果：12$ du -sh /usr/bin859M /usr/bin 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"APUE","slug":"APUE","permalink":"http://dbkernel.github.io/tags/APUE/"},{"name":"UNIX","slug":"UNIX","permalink":"http://dbkernel.github.io/tags/UNIX/"},{"name":"du","slug":"du","permalink":"http://dbkernel.github.io/tags/du/"},{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"}]},{"title":"程序人生 | UNIX环境高级编程技巧之 df 指令实现","slug":"advanced-programming-in-the-unix-environment-df","date":"2014-07-10T01:48:48.000Z","updated":"2021-09-22T15:24:33.178Z","comments":true,"path":"2014/07/10/advanced-programming-in-the-unix-environment-df/","link":"","permalink":"http://dbkernel.github.io/2014/07/10/advanced-programming-in-the-unix-environment-df/","excerpt":"","text":"本文首发于 2014-07-10 09:48:48 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;stdio.h&gt;#include &lt;mntent.h&gt;#include &lt;string.h&gt;#include &lt;sys/vfs.h&gt;static const unsigned long long G = 1024*1024*1024ull;static const unsigned long long M = 1024*1024;static const unsigned long long K = 1024;static char str[20];char* kscale(unsigned long b, unsigned long bs)&#123; unsigned long long size = b * (unsigned long long)bs; if (size &gt; G) &#123; sprintf(str, &quot;%0.2f GB&quot;, size/(G*1.0)); return str; &#125; else if (size &gt; M) &#123; sprintf(str, &quot;%0.2f MB&quot;, size/(1.0*M)); return str; &#125; else if (size &gt; K) &#123; sprintf(str, &quot;%0.2f K&quot;, size/(1.0*K)); return str; &#125; else &#123; sprintf(str, &quot;%0.2f B&quot;, size*1.0); return str; &#125;&#125;int main(int argc, char *argv[])&#123; FILE* mount_table; struct mntent *mount_entry; struct statfs s; unsigned long blocks_used; unsigned blocks_percent_used; const char *disp_units_hdr = NULL; mount_table = NULL; mount_table = setmntent(&quot;/etc/mtab&quot;, &quot;r&quot;); if (!mount_table) &#123; fprintf(stderr, &quot;set mount entry error\\n&quot;); return -1; &#125; disp_units_hdr = &quot; Size&quot;; printf(&quot;Filesystem %-15sUsed Available %s Mounted on\\n&quot;, disp_units_hdr, &quot;Use%&quot;); while (1) &#123; const char *device; const char *mount_point; if (mount_table) &#123; mount_entry = getmntent(mount_table); if (!mount_entry) &#123; endmntent(mount_table); break; &#125; &#125; else continue; device = mount_entry-&gt;mnt_fsname; mount_point = mount_entry-&gt;mnt_dir; //fprintf(stderr, &quot;mount info: device=%s mountpoint=%s\\n&quot;, device, mount_point); if (statfs(mount_point, &amp;s) != 0) &#123; fprintf(stderr, &quot;statfs failed!\\n&quot;); continue; &#125; if ((s.f_blocks &gt; 0) || !mount_table ) &#123; blocks_used = s.f_blocks - s.f_bfree; blocks_percent_used = 0; if (blocks_used + s.f_bavail) &#123; blocks_percent_used = (blocks_used * 100ULL + (blocks_used + s.f_bavail)/2 ) / (blocks_used + s.f_bavail); &#125; /* GNU coreutils 6.10 skips certain mounts, try to be compatible. */ if (strcmp(device, &quot;rootfs&quot;) == 0) continue; if (printf(&quot;\\n%-20s&quot; + 1, device) &gt; 20) printf(&quot;\\n%-20s&quot;, &quot;&quot;); char s1[20]; char s2[20]; char s3[20]; strcpy(s1, kscale(s.f_blocks, s.f_bsize)); strcpy(s2, kscale(s.f_blocks - s.f_bfree, s.f_bsize)); strcpy(s3, kscale(s.f_bavail, s.f_bsize)); printf(&quot; %9s %9s %9s %3u%% %s\\n&quot;, s1, s2, s3, blocks_percent_used, mount_point); &#125; &#125; return 0;&#125; 编译1$ gcc -g -Wall testdf.c -o testdf 运行 testdf执行效果：1234567891011121314151617$ ./testdfFilesystem Size Used Available Use% Mounted onudev 3.87 GB 0.00 B 3.87 GB 0% /devtmpfs 796.17 MB 980.00 K 795.21 MB 0% /run/dev/vda1 96.75 GB 40.54 GB 56.19 GB 42% /tmpfs 3.89 GB 0.00 B 3.89 GB 0% /dev/shmtmpfs 5.00 MB 0.00 B 5.00 MB 0% /run/locktmpfs 3.89 GB 0.00 B 3.89 GB 0% /sys/fs/cgroup/dev/vda15 104.35 MB 3.86 MB 100.50 MB 4% /boot/efi/dev/loop1 55.50 MB 55.50 MB 0.00 B 100% /snap/core18/2074/dev/loop2 70.62 MB 70.62 MB 0.00 B 100% /snap/lxd/16922/dev/loop4 70.38 MB 70.38 MB 0.00 B 100% /snap/lxd/21029/dev/loop5 32.38 MB 32.38 MB 0.00 B 100% /snap/snapd/12704tmpfs 796.17 MB 980.00 K 795.21 MB 0% /run/snapd/nstmpfs 796.17 MB 0.00 B 796.17 MB 0% /run/user/1000/dev/loop6 55.50 MB 55.50 MB 0.00 B 100% /snap/core18/2128/dev/loop0 32.38 MB 32.38 MB 0.00 B 100% /snap/snapd/12883 原生df执行效果：12345678910111213141516$ df -hFilesystem Size Used Avail Use% Mounted onudev 3.9G 0 3.9G 0% /devtmpfs 797M 980K 796M 1% /run/dev/vda1 97G 41G 57G 42% /tmpfs 3.9G 0 3.9G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/vda15 105M 3.9M 101M 4% /boot/efi/dev/loop1 56M 56M 0 100% /snap/core18/2074/dev/loop2 71M 71M 0 100% /snap/lxd/16922/dev/loop4 71M 71M 0 100% /snap/lxd/21029/dev/loop5 33M 33M 0 100% /snap/snapd/12704tmpfs 797M 0 797M 0% /run/user/1000/dev/loop6 56M 56M 0 100% /snap/core18/2128/dev/loop0 33M 33M 0 100% /snap/snapd/12883 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"APUE","slug":"APUE","permalink":"http://dbkernel.github.io/tags/APUE/"},{"name":"UNIX","slug":"UNIX","permalink":"http://dbkernel.github.io/tags/UNIX/"},{"name":"df","slug":"df","permalink":"http://dbkernel.github.io/tags/df/"},{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"}]},{"title":"特性介绍 | Linux 内存管理机制解析","slug":"linux-memory-management-parsing","date":"2014-03-12T13:27:30.000Z","updated":"2021-09-24T15:43:59.870Z","comments":true,"path":"2014/03/12/linux-memory-management-parsing/","link":"","permalink":"http://dbkernel.github.io/2014/03/12/linux-memory-management-parsing/","excerpt":"","text":"本文首发于 2014-03-12 21:27:30 Linux 内存地址映射图 后文中 图：XXX 指的就是上图中对应区域。 地址映射（图：左中）inux 内核使用页式内存管理，应用程序给出的内存地址是虚拟地址，它需要经过若干级页表一级一级的变换，才变成真正的物理地址。 想一下，地址映射还是一件很恐怖的事情。当访问一个由虚拟地址表示的内存空间时，需要先经过若干次的内存访问，得到每一级页表中用于转换的页表项（页表是存放在内存里面的），才能完成映射。也就是说，要实现一次内存访问，实际上内存被访问了N+1次（N=页表级数），并且还需要做N次加法运算。 所以，地址映射必须要有硬件支持，mmu（内存管理单元）就是这个硬件。并且需要有cache来保存页表，这个 cache 就是 TLB（Translation lookaside buffer）。 尽管如此，地址映射还是有着不小的开销。假设 cache 的访存速度是内存的10倍，命中率是40%，页表有三级，那么平均一次虚拟地址访问大概就消耗了两次物理内存访问的时间。于是，一些嵌入式硬件上可能会放弃使用 mmu，这样的硬件能够运行 VxWorks（一个很高效的嵌入式实时操作系统）、linux（linux 也有禁用 mmu 的编译选项）等系统。 但是使用 mmu 的优势也是很大的，最主要的是出于安全性考虑。各个进程都是相互独立的虚拟地址空间，互不干扰。而放弃地址映射之后，所有程序将运行在同一个地址空间。于是，在没有mmu的机器上，一个进程越界访存，可能引起其他进程莫名其妙的错误，甚至导致内核崩溃。 在地址映射这个问题上，内核只提供页表，实际的转换是由硬件去完成的。那么内核如何生成这些页表呢？这就有两方面的内容：虚拟地址空间的管理和物理内存的管理。（实际上只有用户态的地址映射才需要管理，内核态的地址映射是写死的。） 虚拟地址管理（图：左下）每个进程对应一个 task 结构，它指向一个 mm 结构，这就是该进程的内存管理器。（对于线程来说，每个线程也都有一个 task 结构，但是它们都指向同一个 mm，所以同一进程中的多个线程的地址空间是共享的。） mm-&gt;pgd 指向容纳页表的内存，每个进程有自已的 mm，每个 mm 有自己的页表。于是，进程调度时，页表被切换（一般会有一个CPU寄存器来保存页表的地址，比如X86下的CR3，页表切换就是改变该寄存器的值）。所以，各个进程的地址空间互不影响（因为页表都不一样了，当然无法访问到别人的地址空间上。但是共享内存除外，这是故意让不同的页表能够访问到相同的物理地址上）。 用户程序对内存的操作（分配、回收、映射、等）都是对 mm 的操作，具体来说是对 mm 上的 vma（虚拟内存空间） 的操作。这些 vma 代表着进程空间的各个区域，比如堆、栈、代码区、数据区、各种映射区 等。 用户程序对内存的操作并不会直接影响到页表，更不会直接影响到物理内存的分配。比如 malloc 成功，仅仅是改变了某个 vma，页表不会变，物理内存的分配也不会变。 假设用户分配了内存，然后访问这块内存。由于页表里面并没有记录相关的映射，CPU产生一次缺页异常。内核捕捉异常，检查产生异常的地址是不是存在于一个合法的vma中，如果不是，则给进程一个”段错误”，让其崩溃；如果是，则分配一个物理页，并为之建立映射。 物理内存管理（图：右上）那么物理内存是如何分配的呢？ 首先，linux 支持 NUMA (Non Uniform Memory Access)。物理内存管理的第一个层次就是介质的管理，pg_data_t结构就描述了介质。一般而言，我们的内存管理介质只有内存，并且它是均匀的，所以可以简单地认为系统中只有一个 pg_data_t 对象。 每一种介质下面有若干个zone，一般是三个：DMA、NORMAL和HIGH。 DMA：因为有些硬件系统的DMA总线比系统总线窄，所以只有一部分地址空间能够用作 DMA，这部分地址被管理在 DMA 区域（这属于是高级货了）； HIGH：高端内存。在32位系统中，地址空间是4G，其中内核规定 3~4G 的范围是内核空间，0~3G 是用户空间（每个用户进程都有这么大的虚拟空间）（图：中下）。前面提到过内核的地址映射是写死的，就是指这3~4G的对应的页表是写死的，它映射到了物理地址的0~1G上。（实际上没有映射1G，只映射了896M。剩下的空间留下来映射大于1G的物理地址，而这一部分显然不是写死的）。所以，大于896M的物理地址是没有写死的页表来对应的，内核不能直接访问它们（必须要建立映射），称它们为高端内存（当然，如果机器内存不足896M，就不存在高端内存。如果是64位机器，也不存在高端内存，因为地址空间很大很大，属于内核的空间也不止1G了）； NORMAL：不属于 DMA 或 HIGH 的内存就叫 NORMAL 。 在 zone 之上的 zone_list 代表了分配策略，即内存分配时的 zone 优先级。一种内存分配往往不是只能在一个zone里进行分配的，比如分配一个页给内核使用时，最优先是从 NORMAL 里面分配，不行的话就分配 DMA 里面的好了（ HIGH 就不行，因为还没建立映射），这就是一种分配策略。 每个内存介质维护了一个 mem_map，为介质中的每一个物理页面建立了一个 page 结构与之对应，以便管理物理内存。 每个zone记录着它在 mem_map 上的起始位置。并且通过 free_area 串连着这个 zone 上空闲的 page。物理内存的分配就是从这里来的，从 free_area 上把 page 摘下，就算是分配了。（内核的内存分配与用户进程不同，用户使用内存会被内核监督，使用不当就&quot;段错误&quot;；而内核则无人监督，只能靠自觉，不是自己从 free_area 摘下的 page 就不要乱用。） 建立地址映射内核需要物理内存时，很多情况是整页分配的，这在上面的 mem_map 中摘一个 page 下来就好了。比如前面说到的内核捕捉缺页异常，然后需要分配一个 page 以建立映射。 说到这里，会有一个疑问：内核在分配 page、建立地址映射的过程中，使用的是虚拟地址还是物理地址呢？ 首先，内核代码所访问的地址都是虚拟地址，因为CPU指令接收的就是虚拟地址（地址映射对于CPU指令是透明的）。但是，建立地址映射时，内核在页表里面填写的内容却是物理地址，因为地址映射的目标就是要得到物理地址。 那么，内核怎么得到这个物理地址呢？其实，上面也提到了，mem_map 中的 page 就是根据物理内存来建立的，每一个 page 就对应了一个物理页。 于是我们可以说，虚拟地址的映射是靠这里 page 结构来完成的，是它们给出了最终的物理地址。然而，page 结构显然是通过虚拟地址来管理的（前面已经说过，CPU指令接收的就是虚拟地址）。那么，page 结构实现了别人的虚拟地址映射，谁又来实现 page 结构自己的虚拟地址映射呢？没人能够实现。 这就引出了前面提到的一个问题，内核空间的页表项是写死的。在内核初始化时，内核的地址空间就已经把地址映射写死了。page 结构显然存在于内核空间，所以它的地址映射问题已经通过“写死”解决了。 由于内核空间的页表项是写死的，又引出另一个问题，NORMAL（或DMA）区域的内存可能被同时映射到内核空间和用户空间。被映射到内核空间是显然的，因为这个映射已经写死了。而这些页面也可能被映射到用户空间的，在前面提到的缺页异常的场景里面就有这样的可能。映射到用户空间的页面应该优先从 HIGH 区域获取，因为这些内存被内核访问起来很不方便，拿给用户空间再合适不过了。但是 HIGH 区域可能会耗尽，或者可能因为设备上物理内存不足导致系统里面根本就没有 HIGH 区域，所以，将 NORMAL 区域映射给用户空间是必然存在的。 但是 NORMAL 区域的内存被同时映射到内核空间和用户空间并没有问题，因为如果某个页面正在被内核使用，对应的 page 应该已经从 free_area 被摘下，于是缺页异常处理代码中不会再将该页映射到用户空间。反过来也一样，被映射到用户空间的 page 自然已经从 free_area 被摘下，内核不会再去使用这个页面。 内核空间管理（图：右下）除了对内存整页的使用，有些时候，内核也需要像用户程序使用 malloc 一样，分配一块任意大小的空间。这个功能是由 slab 系统来实现的。 slab 相当于为内核中常用的一些结构体对象建立了对象池，比如对应 task 结构的池、对应 mm 结构的池、等等。 而 slab 也维护有通用的对象池，比如”32字节大小”的对象池、”64字节大小”的对象池、等等。内核中常用的 kmalloc 函数（类似于用户态的malloc）就是在这些通用的对象池中实现分配的。 slab除了对象实际使用的内存空间外，还有其对应的控制结构。有两种组织方式：如果对象较大，则控制结构使用专门的页面来保存；如果对象较小，控制结构与对象空间使用相同的页面。 除了slab，linux 2.6 还引入了mempool（内存池）。其意图是：某些对象我们不希望它会因为内存不足而分配失败，于是我们预先分配若干个，放在 mempool 中存起来。正常情况下，分配对象时是不会去动 mempool 里面的资源的，照常通过 slab 去分配。当系统内存紧缺，已经无法通过 slab 分配内存时，才会使用 mempool 中的内容。 页面换入换出（图：左上 &amp; 图：右上）页面换入换出又是一个很复杂的系统。内存页面被换出到磁盘，与磁盘文件被映射到内存，是很相似的两个过程（内存页被换出到磁盘的动机，就是今后还要从磁盘将其载回内存）。所以 swap 复用了文件子系统的一些机制。 页面换入换出是一件很费CPU和IO的事情，但是由于内存昂贵这一历史原因，我们只好拿磁盘来扩展内存。但是现在内存越来越便宜了，我们可以轻松安装数G的内存，然后将 swap 系统关闭。于是 swap 的实现实在让人难有探索的欲望，在这里就不赘述了。 用户空间内存管理malloc是libc的库函数，用户程序一般通过它（或类似函数）来分配内存空间。 libc对内存的分配有两种途径：一是调整堆的大小，二是mmap一个新的虚拟内存区域（堆也是一个vma）。 在内核中，堆是一个一端固定、一端可伸缩的vma（图：左中）。可伸缩的一端通过系统调用 brk 来调整。libc 管理着堆的空间，用户调用 malloc 分配内存时，libc 尽量从现有的堆中去分配。如果堆空间不够，则通过 brk 增大堆空间。 当用户将已分配的空间 free 时，libc 可能会通过 brk 减小堆空间。但是堆空间增大容易减小却难，考虑这样一种情况，用户空间连续分配了10块内存，前9块已经free。这时，未free的第10块哪怕只有1字节大，libc也不能够去减小堆的大小。因为堆只有一端可伸缩，并且中间不能掏空。而第10块内存就死死地占据着堆可伸缩的那一端，堆的大小没法减小，相关资源也没法归还内核。 当用户 malloc 一块很大的内存时，libc 会通过 mmap 系统调用映射一个新的vma。因为对于堆的大小调整和空间管理还是比较麻烦的，重新建一个 vma 会更方便（上面提到的free的问题也是原因之一）。 那么为什么不总是在 malloc 的时候去 mmap 一个新的 vma 呢？ 第一，对于小空间的分配与回收，被 libc 管理的堆空间已经能够满足需要，不必每次都去进行系统调用。 并且 vma 是以 page 为单位的，最小就是分配一个页； 第二，太多的vma会降低系统性能。缺页异常、vma 的新建与销毁、堆空间的大小调整、等等情况下，都需要对 vma 进行操作，需要在当前进程的所有 vma 中找到需要被操作的那个（或那些）vma。vma 数目太多，必然导致性能下降。（在进程的 vma 较少时，内核采用链表来管理 vma；vma 较多时，改用红黑树来管理。） 用户的栈与堆一样，栈也是一个vma（图：左中），这个vma是一端固定、一端可伸（注意，不能缩）的。这个vma比较特殊，没有类似 brk 的系统调用让这个 vma 伸展，它是自动伸展的。 当用户访问的虚拟地址越过这个 vma 时，内核会在处理缺页异常的时候将自动将这个 vma增大。内核会检查当时的栈寄存器（如：ESP），访问的虚拟地址不能超过 ESP加n（n为CPU压栈指令一次性压栈的最大字节数）。也就是说，内核是以ESP为基准来检查访问是否越界。 但是，ESP的值是可以由用户态程序自由读写的，用户程序如果调整ESP，将栈划得很大很大怎么办呢？ 内核中有一套关于进程限制的配置，其中就有栈大小的配置，栈只能这么大，再大就出错。 对于一个进程来说，栈一般是可以被伸展得比较大（如：8MB）。然而对于线程呢？首先线程的栈是怎么回事？前面说过，线程的 mm 是共享其父进程的。虽然栈是mm中的一个vma，但是线程不能与其父进程共用这个vma（两个运行实体显然不用共用一个栈）。于是，在线程创建时，线程库通过mmap新建了一个vma，以此作为线程的栈（大于一般为：2M）。 可见，线程的栈在某种意义上并不是真正栈，它是一个固定的区域，并且容量很有限。 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"},{"name":"内存管理","slug":"内存管理","permalink":"http://dbkernel.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"}]},{"title":"程序人生 | UNIX 环境高级编程技巧之 getopt & getopt_long 使用示例","slug":"advanced-programming-in-the-unix-environment-getopt","date":"2014-01-10T11:48:48.000Z","updated":"2021-09-24T03:50:44.761Z","comments":true,"path":"2014/01/10/advanced-programming-in-the-unix-environment-getopt/","link":"","permalink":"http://dbkernel.github.io/2014/01/10/advanced-programming-in-the-unix-environment-getopt/","excerpt":"","text":"本文首发于 2014-01-10 19:48:48 1. getopt该函数用来解析命令行参数。 1.1. 函数定义12int getopt(int argc, char * const argv[], const char *optstring);#include &lt;unistd.h&gt; 前两个参数设为main函数的两个参数。 optstring设为由该命令要处理的各个选项组成的字符串。选项后面带有冒号’：’时，该选项是一个带参数的选项。 举例：make -f filename -n-f 是一个带参数的选项，-n 是一个没有参数的选项。 可以像下面这样调用 函数getopt 来解析上面的例子。 1c = getopt(argc, argv, &quot;f:n&quot;); 此函数的返回值即为当前找到的命令选项，全部选项都找到时的返回值为-1。 通常一个命令有多个选项，为了取得所有选项，需要循环调用此函数，直到返回值为-1。要使用此函数，还有几个全局变量必须要了解： 123456789extern char *optarg;extern int optind, opterr, optopt; /*optarg: 当前选项带参数时，optarg指向该参数。optind: argv的索引。通常选项参数取得完毕时，通过此变量可以取得非选项参数（argv[optind]）optopt: 一个选项在argv中有，但在optstring中不存在时，或者一个带参数的选项没有参数时， getopt()返回&#x27;?&#x27;，同时将optopt设为该选项。opterr: 将此变量设置为0，可以抑制getopt()输出错误信息。*/ 1.2. 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;int main(int argc, char *argv[ ])&#123; int c; int flg = 0; char filename[256]; char testdata[256]; if (argc &lt; 2) &#123; printf(&quot;usage:%s [-f filename] [-n] testdata\\n&quot;, argv[0]); return -1; &#125; opterr = 0; while ((c = getopt(argc, argv, &quot;f:n&quot;)) != -1) &#123; switch (c) &#123; case &#x27;f&#x27;: strncpy(filename, optarg, sizeof(filename)-1); break; case &#x27;n&#x27;: flg = 1; break; case &#x27;?&#x27;: default: printf(&quot;usage:%s [-f filename] [-n] testdata\\n&quot;, argv[0]); return -1; &#125; &#125; if (argv[optind] == NULL) &#123; printf(&quot;usage:%s [-f filename] [-n] testdata\\n&quot;, argv[0]); return -1; &#125; else &#123; strncpy(testdata, argv[optind], sizeof(testdata)-1); &#125; printf(&quot;fliename:%s flg:%d testdata:%s\\n&quot;, filename, flg, testdata); return 0;&#125; 2. getopt_long这是支持长命令选项的函数，长选项以’–’开头。 2.1. 函数定义1234int getopt_long(int argc, char * const argv[], const char *optstring, const struct option *longopts, int *longindex);#include &lt;getopt.h&gt; 前三个参数与函数getopt的参数是一样的。 只支持长选项时，参数optstring设置为NULL或者空字符串””。 第四个参数是一个构造体struct option的数组。此构造体定义在头文件getopt.h中。此数组的最后一个须将成员都置为0。 123456struct option &#123;const char *name;int has_arg;int *flag;int val;&#125;; 构造体各个成员的解释如下： name : 长选项的名字。 has_arg : no_argument或0表示此选项不带参数，required_argument或1表示此选项带参数，optional_argument或2表示是一个可选选项。 flag : 设置为NULL时，getopt_long()返回val,设置为NULL以外时，&gt;getopt_long()返回0，且将*flag设为val。 val : 返回值或者*flag的设定值。有些命令既支持长选项也支持短选项，可以通过设定此值为短选项实现。 第五个参数是一个输出参数，函数getopt_long()返回时，longindex的值是struct option数组的索引。 关于返回值有以下几种情况： 识别为短选项时，返回值为该短选项。 识别为长选项时，如果flag是NULL的情况下，返回val,如果flag非NULL的情况下，返回0。 所有选项解析结束时返回-1。 存在不能识别的选项或者带参数选项的参数不存在时返回’?’ 。 2.2. 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;stdio.h&gt; /* for printf */#include &lt;stdlib.h&gt; /* for exit */#include &lt;getopt.h&gt;int main(int argc, char **argv)&#123; int c; int digit_optind = 0; int flag = 0; while (1) &#123; int this_option_optind = optind ? optind : 1; int option_index = 0; static struct option long_options[] = &#123; &#123;&quot;add&quot;, required_argument, 0, 0 &#125;, &#123;&quot;append&quot;, no_argument, 0, 0 &#125;, &#123;&quot;delete&quot;, required_argument, 0, 0 &#125;, &#123;&quot;verbose&quot;, no_argument, 0, 0 &#125;, &#123;&quot;create&quot;, required_argument, 0, &#x27;c&#x27;&#125;, &#123;&quot;file&quot;, required_argument, 0, &#x27;f&#x27;&#125;, &#123;0, 0, 0, 0 &#125; &#125;; c = getopt_long_only(argc, argv, &quot;abc:d:f:012&quot;, long_options, &amp;option_index); if (c == -1) break; switch (c) &#123; case 0: printf(&quot;option %s&quot;, long_options[option_index].name); if (optarg) printf(&quot; with arg %s&quot;, optarg); printf(&quot;\\n&quot;); break; case &#x27;0&#x27;: case &#x27;1&#x27;: case &#x27;2&#x27;: if (digit_optind != 0 &amp;&amp; digit_optind != this_option_optind) printf(&quot;digits occur in two different argv-elements.\\n&quot;); digit_optind = this_option_optind; printf(&quot;option %c\\n&quot;, c); break; case &#x27;a&#x27;: printf(&quot;option a\\n&quot;); break; case &#x27;b&#x27;: printf(&quot;option b\\n&quot;); break; case &#x27;c&#x27;: printf(&quot;option c with value &#x27;%s&#x27;\\n&quot;, optarg); break; case &#x27;d&#x27;: printf(&quot;option d with value &#x27;%s&#x27;\\n&quot;, optarg); break; case &#x27;f&#x27;: printf(&quot;option f with value &#x27;%s&#x27;\\n&quot;, optarg); break; case &#x27;?&#x27;: break; default: printf(&quot;?? getopt returned character code 0%o ??\\n&quot;, c); &#125; &#125; if (optind &lt; argc) &#123; printf(&quot;non-option ARGV-elements: &quot;); while (optind &lt; argc) printf(&quot;%s &quot;, argv[optind++]); printf(&quot;\\n&quot;); &#125; exit(EXIT_SUCCESS);&#125; 欢迎关注我的微信公众号【数据库内核】：分享主流开源数据库和存储引擎相关技术。 标题 网址 GitHub https://dbkernel.github.io 知乎 https://www.zhihu.com/people/dbkernel/posts 思否（SegmentFault） https://segmentfault.com/u/dbkernel 掘金 https://juejin.im/user/5e9d3ed251882538083fed1f/posts 开源中国（oschina） https://my.oschina.net/dbkernel 博客园（cnblogs） https://www.cnblogs.com/dbkernel","categories":[{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"APUE","slug":"APUE","permalink":"http://dbkernel.github.io/tags/APUE/"},{"name":"UNIX","slug":"UNIX","permalink":"http://dbkernel.github.io/tags/UNIX/"},{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"getopt","slug":"getopt","permalink":"http://dbkernel.github.io/tags/getopt/"}]}],"categories":[{"name":"通用","slug":"通用","permalink":"http://dbkernel.github.io/categories/%E9%80%9A%E7%94%A8/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/categories/ClickHouse/"},{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/categories/MySQL/"},{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/categories/Linux/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/categories/GreenPlum/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/categories/PostgreSQL/"},{"name":"Postgres-X2","slug":"Postgres-X2","permalink":"http://dbkernel.github.io/categories/Postgres-X2/"},{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/categories/C%E8%AF%AD%E8%A8%80/"},{"name":"编译调试","slug":"编译调试","permalink":"http://dbkernel.github.io/categories/%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95/"}],"tags":[{"name":"开源协议","slug":"开源协议","permalink":"http://dbkernel.github.io/tags/%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE/"},{"name":"开源许可证","slug":"开源许可证","permalink":"http://dbkernel.github.io/tags/%E5%BC%80%E6%BA%90%E8%AE%B8%E5%8F%AF%E8%AF%81/"},{"name":"LICENCE","slug":"LICENCE","permalink":"http://dbkernel.github.io/tags/LICENCE/"},{"name":"github","slug":"github","permalink":"http://dbkernel.github.io/tags/github/"},{"name":"ClickHouse和他的朋友们","slug":"ClickHouse和他的朋友们","permalink":"http://dbkernel.github.io/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"http://dbkernel.github.io/tags/ClickHouse/"},{"name":"源码分析","slug":"源码分析","permalink":"http://dbkernel.github.io/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Materialized View","slug":"Materialized-View","permalink":"http://dbkernel.github.io/tags/Materialized-View/"},{"name":"MySQL","slug":"MySQL","permalink":"http://dbkernel.github.io/tags/MySQL/"},{"name":"问题定位","slug":"问题定位","permalink":"http://dbkernel.github.io/tags/%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"name":"Percona","slug":"Percona","permalink":"http://dbkernel.github.io/tags/Percona/"},{"name":"Xtrabackup","slug":"Xtrabackup","permalink":"http://dbkernel.github.io/tags/Xtrabackup/"},{"name":"RadonDB","slug":"RadonDB","permalink":"http://dbkernel.github.io/tags/RadonDB/"},{"name":"Xenon","slug":"Xenon","permalink":"http://dbkernel.github.io/tags/Xenon/"},{"name":"WAL","slug":"WAL","permalink":"http://dbkernel.github.io/tags/WAL/"},{"name":"MergeTree","slug":"MergeTree","permalink":"http://dbkernel.github.io/tags/MergeTree/"},{"name":"Parser","slug":"Parser","permalink":"http://dbkernel.github.io/tags/Parser/"},{"name":"B-Tree","slug":"B-Tree","permalink":"http://dbkernel.github.io/tags/B-Tree/"},{"name":"LSM-Tree","slug":"LSM-Tree","permalink":"http://dbkernel.github.io/tags/LSM-Tree/"},{"name":"DAG Scheduler","slug":"DAG-Scheduler","permalink":"http://dbkernel.github.io/tags/DAG-Scheduler/"},{"name":"pipeline","slug":"pipeline","permalink":"http://dbkernel.github.io/tags/pipeline/"},{"name":"processor","slug":"processor","permalink":"http://dbkernel.github.io/tags/processor/"},{"name":"Select","slug":"Select","permalink":"http://dbkernel.github.io/tags/Select/"},{"name":"Count","slug":"Count","permalink":"http://dbkernel.github.io/tags/Count/"},{"name":"auto_increment","slug":"auto-increment","permalink":"http://dbkernel.github.io/tags/auto-increment/"},{"name":"MEMORY引擎","slug":"MEMORY引擎","permalink":"http://dbkernel.github.io/tags/MEMORY%E5%BC%95%E6%93%8E/"},{"name":"HEAP引擎","slug":"HEAP引擎","permalink":"http://dbkernel.github.io/tags/HEAP%E5%BC%95%E6%93%8E/"},{"name":"本地事务","slug":"本地事务","permalink":"http://dbkernel.github.io/tags/%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1/"},{"name":"Linux","slug":"Linux","permalink":"http://dbkernel.github.io/tags/Linux/"},{"name":"crontab","slug":"crontab","permalink":"http://dbkernel.github.io/tags/crontab/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"http://dbkernel.github.io/tags/PostgreSQL/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"http://dbkernel.github.io/tags/GreenPlum/"},{"name":"并行查询","slug":"并行查询","permalink":"http://dbkernel.github.io/tags/%E5%B9%B6%E8%A1%8C%E6%9F%A5%E8%AF%A2/"},{"name":"优化器","slug":"优化器","permalink":"http://dbkernel.github.io/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"name":"gcov","slug":"gcov","permalink":"http://dbkernel.github.io/tags/gcov/"},{"name":"lcov","slug":"lcov","permalink":"http://dbkernel.github.io/tags/lcov/"},{"name":"测试","slug":"测试","permalink":"http://dbkernel.github.io/tags/%E6%B5%8B%E8%AF%95/"},{"name":"回归测试","slug":"回归测试","permalink":"http://dbkernel.github.io/tags/%E5%9B%9E%E5%BD%92%E6%B5%8B%E8%AF%95/"},{"name":"Postgres-X2","slug":"Postgres-X2","permalink":"http://dbkernel.github.io/tags/Postgres-X2/"},{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"http://dbkernel.github.io/tags/Postgres-XC/"},{"name":"主从同步","slug":"主从同步","permalink":"http://dbkernel.github.io/tags/%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"name":"pgbench","slug":"pgbench","permalink":"http://dbkernel.github.io/tags/pgbench/"},{"name":"C语言","slug":"C语言","permalink":"http://dbkernel.github.io/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"libpq","slug":"libpq","permalink":"http://dbkernel.github.io/tags/libpq/"},{"name":"pg_depend","slug":"pg-depend","permalink":"http://dbkernel.github.io/tags/pg-depend/"},{"name":"pg_constraint","slug":"pg-constraint","permalink":"http://dbkernel.github.io/tags/pg-constraint/"},{"name":"gcc","slug":"gcc","permalink":"http://dbkernel.github.io/tags/gcc/"},{"name":"编译器","slug":"编译器","permalink":"http://dbkernel.github.io/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"},{"name":"内存分配","slug":"内存分配","permalink":"http://dbkernel.github.io/tags/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"name":"APUE","slug":"APUE","permalink":"http://dbkernel.github.io/tags/APUE/"},{"name":"网络编程","slug":"网络编程","permalink":"http://dbkernel.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"shell","slug":"shell","permalink":"http://dbkernel.github.io/tags/shell/"},{"name":"daemon","slug":"daemon","permalink":"http://dbkernel.github.io/tags/daemon/"},{"name":"系统运维","slug":"系统运维","permalink":"http://dbkernel.github.io/tags/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"},{"name":"字节对齐","slug":"字节对齐","permalink":"http://dbkernel.github.io/tags/%E5%AD%97%E8%8A%82%E5%AF%B9%E9%BD%90/"},{"name":"字节序","slug":"字节序","permalink":"http://dbkernel.github.io/tags/%E5%AD%97%E8%8A%82%E5%BA%8F/"},{"name":"网络序","slug":"网络序","permalink":"http://dbkernel.github.io/tags/%E7%BD%91%E7%BB%9C%E5%BA%8F/"},{"name":"Makefile","slug":"Makefile","permalink":"http://dbkernel.github.io/tags/Makefile/"},{"name":"UNIX","slug":"UNIX","permalink":"http://dbkernel.github.io/tags/UNIX/"},{"name":"du","slug":"du","permalink":"http://dbkernel.github.io/tags/du/"},{"name":"df","slug":"df","permalink":"http://dbkernel.github.io/tags/df/"},{"name":"内存管理","slug":"内存管理","permalink":"http://dbkernel.github.io/tags/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"name":"getopt","slug":"getopt","permalink":"http://dbkernel.github.io/tags/getopt/"}]}